{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f17c21c2",
   "metadata": {},
   "source": [
    "# 02_comparacion_resultados — Baselines A/D\n",
    "\n",
    "**Objetivo:** consolidar y analizar resultados de los tres baselines (reglas, TF‑IDF, transformer).  \n",
    "**Exportables:** `data/02_baselines_comparacion.csv` con métricas clave (macro F1/Prec/Rec, n).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6af82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths / Globals (auto-detect) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, unicodedata, os\n",
    "\n",
    "# Rutas y entorno\n",
    "BASE_PATH = Path.cwd()\n",
    "if BASE_PATH.name == \"notebooks\":\n",
    "    BASE_PATH = BASE_PATH.parent\n",
    "\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "FORK_PATH = BASE_PATH / \"Spanish_Psych_Phenotyping_PY\"\n",
    "\n",
    "# Reuse existing globals if present (from your 02_baselines.ipynb)\n",
    "DATA_PATH = Path(DATA_PATH) if 'DATA_PATH' in globals() else Path('data')\n",
    "FORK_PATH = Path(FORK_PATH) if 'FORK_PATH' in globals() else Path('Spanish_Psych_Phenotyping_PY')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17553049",
   "metadata": {},
   "source": [
    "## 1) Cargar métricas y unificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec4cd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rule_based</td>\n",
       "      <td>0.529224</td>\n",
       "      <td>0.561122</td>\n",
       "      <td>0.537079</td>\n",
       "      <td>630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.913216</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.909961</td>\n",
       "      <td>630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beto</td>\n",
       "      <td>0.855857</td>\n",
       "      <td>0.869106</td>\n",
       "      <td>0.845339</td>\n",
       "      <td>630.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     baseline  macro_f1  macro_precision  macro_recall      n\n",
       "0  rule_based  0.529224         0.561122      0.537079  630.0\n",
       "1       tfidf  0.913216         0.916667      0.909961  630.0\n",
       "2        beto  0.855857         0.869106      0.845339  630.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(DATA_PATH) if 'DATA_PATH' in globals() else Path('data')\n",
    "\n",
    "paths = {\n",
    "    \"rule_based\": {\n",
    "        \"eval\": DATA_PATH/'rule_based_eval.csv',\n",
    "        \"report\": DATA_PATH/'rule_based_classification_report.csv',\n",
    "        \"pred\": DATA_PATH/'rule_based_predictions.csv',\n",
    "    },\n",
    "    \"tfidf\": {\n",
    "        \"eval\": DATA_PATH/'tfidf_eval.csv',\n",
    "        \"report\": DATA_PATH/'tfidf_classification_report.csv',\n",
    "        \"pred\": DATA_PATH/'tfidf_predictions.csv',\n",
    "    },\n",
    "    \"beto\": {\n",
    "        \"eval\": DATA_PATH/'beto_eval.csv',\n",
    "        \"report\": DATA_PATH/'beto_classification_report.csv',\n",
    "        \"pred\": DATA_PATH/'beto_predictions.csv',\n",
    "    }\n",
    "}\n",
    "\n",
    "def pick(series, *names, default=None):\n",
    "    \"\"\"Devuelve el primer nombre presente en la serie.\"\"\"\n",
    "    for n in names:\n",
    "        if n in series:\n",
    "            return series[n]\n",
    "    return default\n",
    "\n",
    "rows = []\n",
    "for name, ps in paths.items():\n",
    "    if ps[\"eval\"].exists():\n",
    "        ev = pd.read_csv(ps[\"eval\"]).iloc[0].to_dict()\n",
    "        # Aceptar con o sin prefijo 'eval_'\n",
    "        macro_f1       = pick(ev, \"macro_f1\", \"eval_macro_f1\")\n",
    "        macro_prec     = pick(ev, \"macro_precision\", \"eval_macro_precision\")\n",
    "        macro_rec      = pick(ev, \"macro_recall\", \"eval_macro_recall\")\n",
    "        n_val          = pick(ev, \"n\")  # puede no existir en beto_eval\n",
    "        if pd.isna(n_val) or n_val is None:\n",
    "            # fallback: contar filas de predicciones si existe\n",
    "            if ps.get(\"pred\") and Path(ps[\"pred\"]).exists():\n",
    "                n_val = len(pd.read_csv(ps[\"pred\"]))\n",
    "        rows.append({\"baseline\": name,\n",
    "                     \"macro_f1\": macro_f1,\n",
    "                     \"macro_precision\": macro_prec,\n",
    "                     \"macro_recall\": macro_rec,\n",
    "                     \"n\": n_val})\n",
    "    else:\n",
    "        rows.append({\"baseline\": name, \"macro_f1\": None, \"macro_precision\": None, \"macro_recall\": None, \"n\": None})\n",
    "\n",
    "comp = pd.DataFrame(rows, columns=[\"baseline\",\"macro_f1\",\"macro_precision\",\"macro_recall\",\"n\"])\n",
    "out_csv = DATA_PATH/'02_baselines_comparacion.csv'\n",
    "comp.to_csv(out_csv, index=False, encoding='utf-8')\n",
    "comp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b454e",
   "metadata": {},
   "source": [
    "## 2) Comentarios y guía de interpretación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c54ee0",
   "metadata": {},
   "source": [
    "- **Rule-based**: traza de patrones: si macro-F1 es muy bajo y precisión alta pero recall bajo, suele indicar **cobertura insuficiente** de reglas ante typos/variantes locales. Útil para depurar fenómenos lingüísticos (ej.: negaciones, abreviaturas).\n",
    "- **TF‑IDF (char)**: debe ser más **robusto a ruido**; si supera a reglas, confirma que los errores ortográficos afectan a las reglas.\n",
    "- **Transformer**: si el dataset es suficiente y el preprocesamiento conservador, debería ser **competitivo** o superior; si no mejora, revisar **tamaño de datos**, **desbalance** o **ruido de etiquetas**.\n",
    "- **Próximos pasos**: calibrar *thresholds*, *class weights*, *data cleaning* incremental; y documentar ejemplos de **falsos positivos/negativos** desde los CSV de predicciones para la discusión en tu presentación.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
