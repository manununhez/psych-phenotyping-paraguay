{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf968e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SETUP COLAB (Carga desde Drive)\n",
    "# ==========================================\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üöÄ En Colab: Montando Drive...\")\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # --- CONFIGURACI√ìN ---\n",
    "    # Ruta a tu carpeta en Drive\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/UCOM/proyecto_final/nlp_mental_health/antigravity\"\n",
    "    # ---------------------\n",
    "\n",
    "    # Instalar librer√≠as\n",
    "    !pip install -q transformers datasets accelerate scikit-learn seaborn matplotlib unidecode\n",
    "\n",
    "    # Preparar entorno local en Colab\n",
    "    !mkdir -p data/splits\n",
    "    !mkdir -p notebooks\n",
    "\n",
    "    # Copiar datos y utils desde Drive al entorno local de Colab (m√°s r√°pido que leer de Drive)\n",
    "    print(\"‚è≥ Copiando archivos...\")\n",
    "    !cp -r \"$DRIVE_BASE/data/splits/\"* data/splits/\n",
    "    !cp \"$DRIVE_BASE/notebooks/utils_shared.py\" notebooks/\n",
    "\n",
    "    # Agregar carpeta notebooks al path para poder importar utils_shared\n",
    "    sys.path.append('/content/notebooks')\n",
    "\n",
    "    print(\"‚úÖ Setup listo. Archivos copiados.\")\n",
    "\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª En Local: Setup omitido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1af325",
   "metadata": {},
   "source": [
    "# 04_baseline_transformers ‚Äî BETO & RoBERTa\n",
    "\n",
    "**Objetivo:** Implementar baselines con Transformers pre-entrenados en espa√±ol.\n",
    "\n",
    "**Modelos:**\n",
    "1. **BETO** (dccuchile/bert-base-spanish-wwm-cased): General.\n",
    "2. **RoBERTa Biom√©dico** (PlanTL-GOB-ES/roberta-base-biomedical-es): Biom√©dico.\n",
    "3. **RoBERTa Cl√≠nico** (PlanTL-GOB-ES/roberta-base-biomedical-clinical-es): Cl√≠nico.\n",
    "\n",
    "**Estrategia:**\n",
    "- **Train:** `train_denoised.csv` (Se√±al pura, 814 casos).\n",
    "- **Dev:** `dev_full.csv` (Realista, 641 casos).\n",
    "- **Max Length:** 512 tokens (Cr√≠tico: ~32% de textos exceden 256).\n",
    "\n",
    "**Optimizado para:**\n",
    "- ‚úÖ Apple Silicon (M2/M3) con MPS\n",
    "- ‚úÖ NVIDIA GPU con CUDA\n",
    "- ‚úÖ CPU (fallback)\n",
    "\n",
    "**Exportables:**\n",
    "- `data/{model_name}_eval.csv`\n",
    "- `data/{model_name}_classification_report.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9926cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Setup: Imports y configuraci√≥n de paths\n",
    "# ===============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# Importar utilidades compartidas\n",
    "try:\n",
    "    from utils_shared import setup_paths, load_splits, calculate_metrics, get_cv_splitter\n",
    "    paths = setup_paths()\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    print(\"[OK] Usando utils_shared.py\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] No se encontr√≥ utils_shared.py. Verifica que est√°s en el directorio correcto.\")\n",
    "    raise\n",
    "\n",
    "# Configuraci√≥n de dispositivo (GPU/MPS/CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_mps_device = False\n",
    "    print(f\"üöÄ Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    use_mps_device = True\n",
    "    print(\"üöÄ Usando Apple Silicon (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_mps_device = False\n",
    "    print(\"‚ö†Ô∏è  Usando CPU (lento)\")\n",
    "\n",
    "# Hiperpar√°metros\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuraci√≥n:\")\n",
    "print(f\"   Max Length: {MAX_LENGTH}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27dc21",
   "metadata": {},
   "source": [
    "## 1) Carga de Datos y Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a50745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datasets\n",
    "try:\n",
    "    # Train: Usar train_denoised (se√±al cl√≠nica) desde SPLITS_PATH\n",
    "    df_train = pd.read_csv(SPLITS_PATH / 'train_denoised.csv')\n",
    "\n",
    "    # Dev: Construir desde splits (dataset completo)\n",
    "    df_base, _, dev_idx, _ = load_splits(SPLITS_PATH)\n",
    "    df_dev = df_base.set_index('row_id').loc[dev_idx].reset_index()\n",
    "\n",
    "    print(f\"‚úÖ Train (Denoised): {len(df_train)} casos\")\n",
    "    print(f\"‚úÖ Dev (Full): {len(df_dev)} casos\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] No se encontraron los datasets. Ejecuta 03_rule_based_denoising.ipynb primero.\")\n",
    "    raise\n",
    "\n",
    "# Mapeo de etiquetas\n",
    "label2id = {'depresion': 0, 'ansiedad': 1}\n",
    "id2label = {0: 'depresion', 1: 'ansiedad'}\n",
    "\n",
    "df_train['label'] = df_train['etiqueta'].map(label2id)\n",
    "df_dev['label'] = df_dev['etiqueta'].map(label2id)\n",
    "\n",
    "print(f\"\\nüìä Distribuci√≥n Train: {dict(df_train['etiqueta'].value_counts())}\")\n",
    "print(f\"üìä Distribuci√≥n Dev: {dict(df_dev['etiqueta'].value_counts())}\")\n",
    "\n",
    "# Limpieza conservadora (Transformers manejan bien el texto crudo)\n",
    "RE_MULTI = re.compile(r'(.)\\1{2,}')\n",
    "\n",
    "def clean_text_trf(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = RE_MULTI.sub(r'\\1\\1', s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "print(\"\\nüßπ Limpiando textos...\")\n",
    "df_train['texto_trf'] = df_train['texto'].map(clean_text_trf)\n",
    "df_dev['texto_trf'] = df_dev['texto'].map(clean_text_trf)\n",
    "\n",
    "# Convertir a HuggingFace Datasets\n",
    "ds_train = Dataset.from_pandas(df_train[['texto_trf', 'label']].rename(columns={'texto_trf': 'texto'}))\n",
    "ds_dev = Dataset.from_pandas(df_dev[['texto_trf', 'label']].rename(columns={'texto_trf': 'texto'}))\n",
    "\n",
    "print(\"‚úÖ Datasets preparados\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc44e3e",
   "metadata": {},
   "source": [
    "## 2) Entrenamiento y Evaluaci√≥n (Loop Modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15338ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"beto\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "    \"roberta_biomedical\": \"PlanTL-GOB-ES/roberta-base-biomedical-es\",\n",
    "    \"roberta_clinical\": \"PlanTL-GOB-ES/roberta-base-biomedical-clinical-es\"\n",
    "}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calcula m√©tricas usando utils_shared\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Convertir IDs a etiquetas string\n",
    "    labels_str = [id2label[l] for l in labels]\n",
    "    preds_str = [id2label[p] for p in predictions]\n",
    "\n",
    "    # Usar funci√≥n compartida\n",
    "    metrics = calculate_metrics(labels_str, preds_str)\n",
    "\n",
    "    # Trainer espera flat dict con nombres espec√≠ficos\n",
    "    return {\n",
    "        'f1': metrics['f1_macro'],\n",
    "        'precision': metrics['precision_macro'],\n",
    "        'recall': metrics['recall_macro'],\n",
    "        'accuracy': metrics['accuracy']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Loop de Entrenamiento\n",
    "# ===============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üöÄ INICIANDO ENTRENAMIENTO DE {len(MODELS)} MODELOS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, (model_name, model_id) in enumerate(MODELS.items(), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ [{i}/{len(MODELS)}] MODELO: {model_name}\")\n",
    "    print(f\"üîó HuggingFace ID: {model_id}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Tokenizer\n",
    "    print(\"‚è≥ Cargando tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"texto\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "\n",
    "    print(\"‚è≥ Tokenizando datasets...\")\n",
    "    tokenized_train = ds_train.map(tokenize_function, batched=True)\n",
    "    tokenized_dev = ds_dev.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Model\n",
    "    print(\"‚è≥ Cargando modelo...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=2,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Mover modelo al dispositivo\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training Args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(DATA_PATH / \"checkpoints\" / model_name),\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_dir=str(DATA_PATH / \"logs\" / model_name),\n",
    "        logging_steps=10,\n",
    "        seed=42,\n",
    "        use_mps_device=use_mps_device,  # Activar MPS si est√° disponible\n",
    "        report_to=\"none\"  # Desactivar wandb/tensorboard\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_dev,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(f\"\\nüèãÔ∏è  Entrenando {model_name}...\")\n",
    "    print(f\"   Epochs: {EPOCHS} | Batch Size: {BATCH_SIZE} | Max Length: {MAX_LENGTH}\")\n",
    "    print(f\"   Dispositivo: {device}\\n\")\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate Final\n",
    "    print(f\"\\nüìä Evaluando {model_name} en Dev Set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ RESULTADOS {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  F1 Macro:     {eval_results['eval_f1']:.4f}\")\n",
    "    print(f\"  Precision:    {eval_results['eval_precision']:.4f}\")\n",
    "    print(f\"  Recall:       {eval_results['eval_recall']:.4f}\")\n",
    "    print(f\"  Accuracy:     {eval_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Exportar m√©tricas\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'modelo': model_name,\n",
    "        'f1_macro': eval_results['eval_f1'],\n",
    "        'precision_macro': eval_results['eval_precision'],\n",
    "        'recall_macro': eval_results['eval_recall'],\n",
    "        'accuracy': eval_results['eval_accuracy'],\n",
    "        'n_train': len(ds_train),\n",
    "        'n_dev': len(ds_dev),\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'device': str(device)\n",
    "    }])\n",
    "\n",
    "    output_path = DATA_PATH / f'{model_name}_eval.csv'\n",
    "    metrics_df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Guardado: {output_path}\")\n",
    "\n",
    "    # Limpiar memoria\n",
    "    print(f\"üßπ Liberando memoria...\\n\")\n",
    "    del model, trainer, tokenized_train, tokenized_dev\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üéâ ENTRENAMIENTO COMPLETADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚úÖ {len(MODELS)} modelos entrenados y evaluados\")\n",
    "print(f\"üìÅ Resultados guardados en: {DATA_PATH}\")\n",
    "print(f\"\\nüí° Pr√≥ximo paso: Ejecuta 05_comparacion_resultados.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9cff2",
   "metadata": {},
   "source": [
    "## 3) Cross-Validation (5-Fold)\n",
    "\n",
    "**Advertencia:** Esto puede tomar considerablemente m√°s tiempo que el entrenamiento simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_shared import get_cv_splitter\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Combinar Train + Dev\n",
    "df_full = pd.concat([df_train, df_dev]).reset_index(drop=True)\n",
    "\n",
    "# DEBUG: Verificar etiquetas\n",
    "print(f\"Etiquetas √∫nicas en df_full: {df_full['label'].unique()}\")\n",
    "print(f\"Tipos de datos en label: {df_full['label'].dtype}\")\n",
    "if df_full['label'].isnull().any():\n",
    "    print(\"‚ö†Ô∏è ADVERTENCIA: Hay valores nulos en la columna label!\")\n",
    "    df_full = df_full.dropna(subset=['label'])\n",
    "    print(\"‚úÖ Filas con label nulo eliminadas.\")\n",
    "\n",
    "# Asegurar que sean enteros\n",
    "df_full['label'] = df_full['label'].astype(int)\n",
    "\n",
    "groups_full = df_full['patient_id']  # Usar patient_id directamente\n",
    "\n",
    "cv = get_cv_splitter(n_splits=5)\n",
    "cv_results = []\n",
    "\n",
    "print(\"Iniciando Cross-Validation (BETO)...\")\n",
    "\n",
    "# Load BETO tokenizer once for CV\n",
    "model_id_cv = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer_cv = AutoTokenizer.from_pretrained(model_id_cv)\n",
    "\n",
    "# Funci√≥n auxiliar para tokenizar\n",
    "def tokenize_function_cv(examples):\n",
    "    return tokenizer_cv(\n",
    "        examples[\"texto\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH # Use global MAX_LENGTH\n",
    "    )\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(df_full, df_full['etiqueta'], groups_full)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "\n",
    "    # Split datos\n",
    "    train_fold = df_full.iloc[train_idx]\n",
    "    val_fold = df_full.iloc[val_idx]\n",
    "\n",
    "    # Crear datasets HF (Usando texto limpio 'texto_trf' renombrado a 'texto')\n",
    "    ds_train_fold = Dataset.from_pandas(train_fold[['texto_trf', 'label']].rename(columns={'texto_trf': 'texto'}))\n",
    "    ds_val_fold = Dataset.from_pandas(val_fold[['texto_trf', 'label']].rename(columns={'texto_trf': 'texto'}))\n",
    "\n",
    "    # Tokenizar fold-specific datasets\n",
    "    tokenized_train_fold = ds_train_fold.map(tokenize_function_cv, batched=True)\n",
    "    tokenized_val_fold = ds_val_fold.map(tokenize_function_cv, batched=True)\n",
    "\n",
    "    # Usamos BETO expl√≠citamente como indica el print anterior\n",
    "    model_cv = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id_cv,\n",
    "        num_labels=2,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    training_args_cv = TrainingArguments(\n",
    "        no_cuda=True, # DEBUG: Force CPU\n",
    "        output_dir=f\"./results_cv/fold_{fold}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=BATCH_SIZE, # Use global BATCH_SIZE\n",
    "        per_device_eval_batch_size=BATCH_SIZE, # Use global BATCH_SIZE\n",
    "        num_train_epochs=EPOCHS, # Use global EPOCHS\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_dir=f'./logs_cv/fold_{fold}',\n",
    "        logging_steps=10,\n",
    "        seed=42,\n",
    "        report_to=\"none\", # Disable wandb logging\n",
    "        use_mps_device=use_mps_device, # Activate MPS if available\n",
    "    )\n",
    "\n",
    "    trainer_cv = Trainer(\n",
    "        model=model_cv,\n",
    "        args=training_args_cv,\n",
    "        train_dataset=tokenized_train_fold,\n",
    "        eval_dataset=tokenized_val_fold,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer_cv # Pass the correct tokenizer for the Trainer\n",
    "    )\n",
    "\n",
    "    trainer_cv.train()\n",
    "\n",
    "    # Evaluar\n",
    "    eval_result = trainer_cv.evaluate()\n",
    "\n",
    "    cv_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'model': 'BETO',\n",
    "        'f1_macro': eval_result['eval_f1'],\n",
    "        'precision_macro': eval_result['eval_precision'],\n",
    "        'recall_macro': eval_result['eval_recall']\n",
    "    })\n",
    "    print(f\"Fold {fold+1}: F1={eval_result['eval_f1']:.4f}\")\n",
    "\n",
    "    # Clean up memory after each fold\n",
    "    print(f\"üßπ Liberando memoria para Fold {fold+1}...\")\n",
    "    del model_cv, trainer_cv, tokenized_train_fold, tokenized_val_fold\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "df_cv = pd.DataFrame(cv_results)\n",
    "print(\"\\nPromedio CV:\")\n",
    "print(df_cv.mean(numeric_only=True))\n",
    "\n",
    "out_path = DATA_PATH / 'beto_cv_results.csv'\n",
    "df_cv.to_csv(out_path, index=False)\n",
    "print(f\"\\u2713 Exportado: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627967db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IN_COLAB' in locals() and IN_COLAB:\n",
    "    print(\"‚è≥ Guardando resultados en Drive...\")\n",
    "    # Asegurar que la carpeta de destino exista\n",
    "    !mkdir -p \"$DRIVE_BASE/data\"\n",
    "    \n",
    "    # Copiar resultados\n",
    "    !cp data/*.csv \"$DRIVE_BASE/data/\"\n",
    "    \n",
    "    print(\"‚úÖ Resultados guardados en: \", DRIVE_BASE + \"/data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
