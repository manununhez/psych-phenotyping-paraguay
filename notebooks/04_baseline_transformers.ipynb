{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1af325",
   "metadata": {},
   "source": [
    "# 04_baseline_transformers ‚Äî BETO & RoBERTa\n",
    "\n",
    "**Objetivo:** Implementar baselines con Transformers pre-entrenados en espa√±ol.\n",
    "\n",
    "**Modelos:**\n",
    "1. **BETO** (dccuchile/bert-base-spanish-wwm-cased): General.\n",
    "2. **RoBERTa Biom√©dico** (PlanTL-GOB-ES/roberta-base-biomedical-es): Biom√©dico.\n",
    "3. **RoBERTa Cl√≠nico** (PlanTL-GOB-ES/roberta-base-biomedical-clinical-es): Cl√≠nico.\n",
    "\n",
    "**Estrategia:**\n",
    "- **Train:** `train_denoised.csv` (Se√±al pura, 814 casos).\n",
    "- **Dev:** `dev_full.csv` (Realista, 641 casos).\n",
    "- **Max Length:** 512 tokens (Cr√≠tico: ~32% de textos exceden 256).\n",
    "\n",
    "**Optimizado para:**\n",
    "- ‚úÖ Apple Silicon (M2/M3) con MPS\n",
    "- ‚úÖ NVIDIA GPU con CUDA\n",
    "- ‚úÖ CPU (fallback)\n",
    "\n",
    "**Exportables:**\n",
    "- `data/{model_name}_eval.csv`\n",
    "- `data/{model_name}_classification_report.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9926cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelnunez/Projects/psych-phenotyping-paraguay/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Usando utils_shared.py\n",
      "üöÄ Usando Apple Silicon (MPS)\n",
      "\n",
      "‚öôÔ∏è  Configuraci√≥n:\n",
      "   Max Length: 512\n",
      "   Batch Size: 16\n",
      "   Epochs: 3\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Imports y configuraci√≥n de paths\n",
    "# ===============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importar utilidades compartidas\n",
    "try:\n",
    "    from utils_shared import setup_paths, load_splits, calculate_metrics, get_cv_splitter\n",
    "    paths = setup_paths()\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    print(\"[OK] Usando utils_shared.py\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] No se encontr√≥ utils_shared.py. Verifica que est√°s en el directorio correcto.\")\n",
    "    raise\n",
    "\n",
    "# Configuraci√≥n de dispositivo (GPU/MPS/CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_mps_device = False\n",
    "    print(f\"üöÄ Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    use_mps_device = True\n",
    "    print(\"üöÄ Usando Apple Silicon (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_mps_device = False\n",
    "    print(\"‚ö†Ô∏è  Usando CPU (lento)\")\n",
    "\n",
    "# Hiperpar√°metros\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuraci√≥n:\")\n",
    "print(f\"   Max Length: {MAX_LENGTH}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27dc21",
   "metadata": {},
   "source": [
    "## 1) Carga de Datos y Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a50745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train (Denoised): 993 casos\n",
      "‚úÖ Dev (Full): 627 casos\n",
      "\n",
      "üìä Distribuci√≥n Train: {'depresion': np.int64(708), 'ansiedad': np.int64(285)}\n",
      "üìä Distribuci√≥n Dev: {'depresion': np.int64(342), 'ansiedad': np.int64(285)}\n",
      "\n",
      "üßπ Limpiando textos...\n",
      "‚úÖ Datasets preparados\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets\n",
    "try:\n",
    "    # Train: Usar train_denoised (se√±al cl√≠nica) desde SPLITS_PATH\n",
    "    df_train = pd.read_csv(SPLITS_PATH / 'train_denoised.csv')\n",
    "    \n",
    "    # Dev: Construir desde splits (dataset completo)\n",
    "    df_base, _, dev_idx, _ = load_splits(SPLITS_PATH)\n",
    "    df_dev = df_base.set_index('row_id').loc[dev_idx].reset_index()\n",
    "    \n",
    "    print(f\"‚úÖ Train (Denoised): {len(df_train)} casos\")\n",
    "    print(f\"‚úÖ Dev (Full): {len(df_dev)} casos\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] No se encontraron los datasets. Ejecuta 03_rule_based_denoising.ipynb primero.\")\n",
    "    raise\n",
    "\n",
    "# Mapeo de etiquetas\n",
    "label2id = {'depresion': 0, 'ansiedad': 1}\n",
    "id2label = {0: 'depresion', 1: 'ansiedad'}\n",
    "\n",
    "df_train['label'] = df_train['etiqueta'].map(label2id)\n",
    "df_dev['label'] = df_dev['etiqueta'].map(label2id)\n",
    "\n",
    "print(f\"\\nüìä Distribuci√≥n Train: {dict(df_train['etiqueta'].value_counts())}\")\n",
    "print(f\"üìä Distribuci√≥n Dev: {dict(df_dev['etiqueta'].value_counts())}\")\n",
    "\n",
    "# Limpieza conservadora (Transformers manejan bien el texto crudo)\n",
    "RE_MULTI = re.compile(r'(.)\\1{2,}')\n",
    "\n",
    "def clean_text_trf(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = RE_MULTI.sub(r'\\1\\1', s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "print(\"\\nüßπ Limpiando textos...\")\n",
    "df_train['texto_trf'] = df_train['texto'].map(clean_text_trf)\n",
    "df_dev['texto_trf'] = df_dev['texto'].map(clean_text_trf)\n",
    "\n",
    "# Convertir a HuggingFace Datasets\n",
    "ds_train = Dataset.from_pandas(df_train[['texto_trf', 'label']].rename(columns={'texto_trf': 'texto'}))\n",
    "ds_dev = Dataset.from_pandas(df_dev[['texto_trf', 'label']].rename(columns={'texto_trf': 'texto'}))\n",
    "\n",
    "print(\"‚úÖ Datasets preparados\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc44e3e",
   "metadata": {},
   "source": [
    "## 2) Entrenamiento y Evaluaci√≥n (Loop Modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train_loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ INICIANDO ENTRENAMIENTO DE 3 MODELOS\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì¶ [1/3] MODELO: beto\n",
      "üîó HuggingFace ID: dccuchile/bert-base-spanish-wwm-cased\n",
      "============================================================\n",
      "\n",
      "‚è≥ Cargando tokenizer...\n",
      "‚è≥ Tokenizando datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 993/993 [00:00<00:00, 2312.23 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 627/627 [00:00<00:00, 4122.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cargando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     58\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m     59\u001b[39m     model_id, \n\u001b[32m     60\u001b[39m     num_labels=\u001b[32m2\u001b[39m, \n\u001b[32m     61\u001b[39m     id2label=id2label, \n\u001b[32m     62\u001b[39m     label2id=label2id\n\u001b[32m     63\u001b[39m )\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Mover modelo al dispositivo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Training Args\u001b[39;00m\n\u001b[32m     69\u001b[39m training_args = TrainingArguments(\n\u001b[32m     70\u001b[39m     output_dir=\u001b[38;5;28mstr\u001b[39m(DATA_PATH / \u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m / model_name),\n\u001b[32m     71\u001b[39m     learning_rate=\u001b[32m2e-5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Desactivar wandb/tensorboard\u001b[39;00m\n\u001b[32m     85\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/psych-phenotyping-paraguay/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4343\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4340\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4341\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/psych-phenotyping-paraguay/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/psych-phenotyping-paraguay/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/psych-phenotyping-paraguay/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (4 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/psych-phenotyping-paraguay/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/psych-phenotyping-paraguay/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/psych-phenotyping-paraguay/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "MODELS = {\n",
    "    \"beto\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "    \"roberta_biomedical\": \"PlanTL-GOB-ES/roberta-base-biomedical-es\",\n",
    "    \"roberta_clinical\": \"PlanTL-GOB-ES/roberta-base-biomedical-clinical-es\"\n",
    "}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calcula m√©tricas usando utils_shared\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Convertir IDs a etiquetas string\n",
    "    labels_str = [id2label[l] for l in labels]\n",
    "    preds_str = [id2label[p] for p in predictions]\n",
    "    \n",
    "    # Usar funci√≥n compartida\n",
    "    metrics = calculate_metrics(labels_str, preds_str)\n",
    "    \n",
    "    # Trainer espera flat dict con nombres espec√≠ficos\n",
    "    return {\n",
    "        'f1': metrics['f1_macro'],\n",
    "        'precision': metrics['precision_macro'],\n",
    "        'recall': metrics['recall_macro'],\n",
    "        'accuracy': accuracy_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "# ===============================================================\n",
    "# Loop de Entrenamiento\n",
    "# ===============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üöÄ INICIANDO ENTRENAMIENTO DE {len(MODELS)} MODELOS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, (model_name, model_id) in enumerate(MODELS.items(), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ [{i}/{len(MODELS)}] MODELO: {model_name}\")\n",
    "    print(f\"üîó HuggingFace ID: {model_id}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Tokenizer\n",
    "    print(\"‚è≥ Cargando tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"texto\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "    \n",
    "    print(\"‚è≥ Tokenizando datasets...\")\n",
    "    tokenized_train = ds_train.map(tokenize_function, batched=True)\n",
    "    tokenized_dev = ds_dev.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Model\n",
    "    print(\"‚è≥ Cargando modelo...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id, \n",
    "        num_labels=2, \n",
    "        id2label=id2label, \n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    # Mover modelo al dispositivo\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training Args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(DATA_PATH / \"checkpoints\" / model_name),\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_dir=str(DATA_PATH / \"logs\" / model_name),\n",
    "        logging_steps=10,\n",
    "        seed=42,\n",
    "        use_mps_device=use_mps_device,  # Activar MPS si est√° disponible\n",
    "        report_to=\"none\"  # Desactivar wandb/tensorboard\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_dev,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nüèãÔ∏è  Entrenando {model_name}...\")\n",
    "    print(f\"   Epochs: {EPOCHS} | Batch Size: {BATCH_SIZE} | Max Length: {MAX_LENGTH}\")\n",
    "    print(f\"   Dispositivo: {device}\\n\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate Final\n",
    "    print(f\"\\nüìä Evaluando {model_name} en Dev Set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ RESULTADOS {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  F1 Macro:     {eval_results['eval_f1']:.4f}\")\n",
    "    print(f\"  Precision:    {eval_results['eval_precision']:.4f}\")\n",
    "    print(f\"  Recall:       {eval_results['eval_recall']:.4f}\")\n",
    "    print(f\"  Accuracy:     {eval_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Exportar m√©tricas\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'modelo': model_name,\n",
    "        'f1_macro': eval_results['eval_f1'],\n",
    "        'precision_macro': eval_results['eval_precision'],\n",
    "        'recall_macro': eval_results['eval_recall'],\n",
    "        'accuracy': eval_results['eval_accuracy'],\n",
    "        'n_train': len(ds_train),\n",
    "        'n_dev': len(ds_dev),\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'device': str(device)\n",
    "    }])\n",
    "    \n",
    "    output_path = DATA_PATH / f'{model_name}_eval.csv'\n",
    "    metrics_df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Guardado: {output_path}\")\n",
    "    \n",
    "    # Limpiar memoria\n",
    "    print(f\"üßπ Liberando memoria...\\n\")\n",
    "    del model, trainer, tokenized_train, tokenized_dev\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üéâ ENTRENAMIENTO COMPLETADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚úÖ {len(MODELS)} modelos entrenados y evaluados\")\n",
    "print(f\"üìÅ Resultados guardados en: {DATA_PATH}\")\n",
    "print(f\"\\nüí° Pr√≥ximo paso: Ejecuta 05_comparacion_resultados.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9cff2",
   "metadata": {},
   "source": [
    "## 3) Cross-Validation (5-Fold)\n",
    "\n",
    "**Advertencia:** Esto puede tomar considerablemente m√°s tiempo que el entrenamiento simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_shared import get_cv_splitter\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Combinar Train + Dev\n",
    "df_full = pd.concat([df_train, df_dev]).reset_index(drop=True)\n",
    "groups_full = df_full['patient_id']  # Usar patient_id directamente\n",
    "\n",
    "cv = get_cv_splitter(n_splits=5)\n",
    "cv_results = []\n",
    "\n",
    "print(\"Iniciando Cross-Validation (BETO)...\")\n",
    "\n",
    "# Funci√≥n auxiliar para tokenizar\n",
    "def tokenize_function_cv(examples):\n",
    "    return tokenizer(examples[\"texto\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(df_full, df_full['etiqueta'], groups_full)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "    \n",
    "    # Split datos\n",
    "    train_fold = df_full.iloc[train_idx]\n",
    "    val_fold = df_full.iloc[val_idx]\n",
    "    \n",
    "    # Crear datasets HF\n",
    "    ds_train_fold = Dataset.from_pandas(train_fold)\n",
    "    ds_val_fold = Dataset.from_pandas(val_fold)\n",
    "    \n",
    "    # Tokenizar\n",
    "    tokenized_train = ds_train_fold.map(tokenize_function_cv, batched=True)\n",
    "    tokenized_val = ds_val_fold.map(tokenize_function_cv, batched=True)\n",
    "    \n",
    "    # Re-inicializar modelo (CR√çTICO para no leakage)\n",
    "    model_cv = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    \n",
    "    training_args_cv = TrainingArguments(\n",
    "        output_dir=f\"./results_cv/fold_{fold}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_dir=f'./logs_cv/fold_{fold}',\n",
    "        logging_steps=10,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    trainer_cv = Trainer(\n",
    "        model=model_cv,\n",
    "        args=training_args_cv,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer_cv.train()\n",
    "    \n",
    "    # Evaluar\n",
    "    eval_result = trainer_cv.evaluate()\n",
    "    \n",
    "    cv_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'model': 'BETO',\n",
    "        'f1_macro': eval_result['eval_f1'],\n",
    "        'precision_macro': eval_result['eval_precision'],\n",
    "        'recall_macro': eval_result['eval_recall']\n",
    "    })\n",
    "    print(f\"Fold {fold+1}: F1={eval_result['eval_f1']:.4f}\")\n",
    "\n",
    "df_cv = pd.DataFrame(cv_results)\n",
    "print(\"\\nPromedio CV:\")\n",
    "print(df_cv.mean(numeric_only=True))\n",
    "\n",
    "out_path = DATA_PATH / 'beto_cv_results.csv'\n",
    "df_cv.to_csv(out_path, index=False)\n",
    "print(f\"‚úì Exportado: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
