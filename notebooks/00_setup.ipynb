{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Entorno: Local (Jupyter/VSCode)\n",
            "  Ejecutando en entorno local\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 00_setup.ipynb - Configuración Inicial del Proyecto\n",
        "# ===============================================================\n",
        "#\n",
        "# **Propósito**: \n",
        "# - Validar estructura del proyecto\n",
        "# - Verificar dependencias instaladas\n",
        "# - Configurar paths usando utils_shared.py\n",
        "# - Preparar entorno para ejecutar notebooks 01_ y 02_\n",
        "#\n",
        "# **Cuándo ejecutar**:\n",
        "# - Primera vez que clonas el repositorio\n",
        "# - Después de cambios en la estructura de carpetas\n",
        "# - Para verificar que todo está correctamente instalado\n",
        "#\n",
        "# **Qué NO hace**:\n",
        "# - No procesa datos (eso es 01_eda_understanding.ipynb)\n",
        "# - No crea splits (eso es 02_create_splits.ipynb)\n",
        "# - No entrena modelos (eso es 02_baseline_*.ipynb)\n",
        "#\n",
        "# ===============================================================\n",
        "\n",
        "import os, sys, pathlib, yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 1. Detección de entorno (Local vs Google Colab)\n",
        "# ---------------------------------------------------------------\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"[INFO] Entorno: Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"[INFO] Entorno: Local (Jupyter/VSCode)\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"  En Colab puedes montar Google Drive para persistir datos\")\n",
        "else:\n",
        "    print(\"  Ejecutando en entorno local\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Cargando paths desde utils_shared.py\n",
            "  Usando configuración centralizada\n",
            "\n",
            "[INFO] Paths configurados:\n",
            "  BASE_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay\n",
            "  DATA_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
            "  FORK_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/Spanish_Psych_Phenotyping_PY\n",
            "  SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n",
            "  FIGS_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/figs\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 2. Configuración de Paths usando utils_shared.py\n",
        "# ===============================================================\n",
        "#\n",
        "# Intentamos usar utils_shared.py para centralizar configuración.\n",
        "# Si no está disponible, usamos configuración manual.\n",
        "#\n",
        "# ===============================================================\n",
        "\n",
        "try:\n",
        "    from utils_shared import setup_paths\n",
        "    print(\"[INFO] Cargando paths desde utils_shared.py\")\n",
        "    \n",
        "    paths = setup_paths()\n",
        "    BASE_PATH = paths['BASE_PATH']\n",
        "    DATA_PATH = paths['DATA_PATH']\n",
        "    FORK_PATH = paths['FORK_PATH']\n",
        "    SPLITS_PATH = paths['SPLITS_PATH']\n",
        "    FIGS_PATH = paths['FIGS_PATH']\n",
        "    \n",
        "    print(\"  Usando configuración centralizada\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"[WARNING] utils_shared.py no encontrado, usando configuración manual\")\n",
        "    \n",
        "    BASE_PATH = pathlib.Path.cwd()\n",
        "    if BASE_PATH.name == \"notebooks\":\n",
        "        BASE_PATH = BASE_PATH.parent\n",
        "    \n",
        "    DATA_PATH = BASE_PATH / \"data\"\n",
        "    FORK_PATH = BASE_PATH / \"Spanish_Psych_Phenotyping_PY\"\n",
        "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
        "    FIGS_PATH = DATA_PATH / \"figs\"\n",
        "    \n",
        "    # Crear directorios si no existen\n",
        "    DATA_PATH.mkdir(exist_ok=True)\n",
        "    SPLITS_PATH.mkdir(exist_ok=True)\n",
        "    FIGS_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "# En Colab → permitir montaje de Google Drive\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        DRIVE_DATA_PATH = pathlib.Path(\"/content/drive/MyDrive/psych-data\")\n",
        "        \n",
        "        if DRIVE_DATA_PATH.exists():\n",
        "            DATA_PATH = DRIVE_DATA_PATH\n",
        "            SPLITS_PATH = DATA_PATH / \"splits\"\n",
        "            FIGS_PATH = DATA_PATH / \"figs\"\n",
        "            print(\"[INFO] Usando datos desde Google Drive\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] No se pudo montar Google Drive: {e}\")\n",
        "\n",
        "print(f\"\\n[INFO] Paths configurados:\")\n",
        "print(f\"  BASE_PATH:   {BASE_PATH}\")\n",
        "print(f\"  DATA_PATH:   {DATA_PATH}\")\n",
        "print(f\"  FORK_PATH:   {FORK_PATH}\")\n",
        "print(f\"  SPLITS_PATH: {SPLITS_PATH}\")\n",
        "print(f\"  FIGS_PATH:   {FIGS_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Verificando estructura del proyecto...\n",
            "[OK] data/                          /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
            "[OK] data/splits/                   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n",
            "[OK] data/figs/                     /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/figs\n",
            "[OK] notebooks/                     /Users/manuelnunez/Projects/psych-phenotyping-paraguay/notebooks\n",
            "[OK] configs/                       /Users/manuelnunez/Projects/psych-phenotyping-paraguay/configs\n",
            "[OK] Spanish_Psych_Phenotyping_PY/  /Users/manuelnunez/Projects/psych-phenotyping-paraguay/Spanish_Psych_Phenotyping_PY\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Verificación de estructura del proyecto\n",
        "print(\"\\n[INFO] Verificando estructura del proyecto...\")\n",
        "\n",
        "auto_create_dirs = {\n",
        "    'data/': DATA_PATH,\n",
        "    'data/splits/': SPLITS_PATH,\n",
        "    'data/figs/': FIGS_PATH,\n",
        "    'notebooks/': BASE_PATH / \"notebooks\",\n",
        "    'configs/': BASE_PATH / \"configs\"\n",
        "}\n",
        "\n",
        "required_dirs = {\n",
        "    'Spanish_Psych_Phenotyping_PY/': FORK_PATH\n",
        "}\n",
        "\n",
        "for name, path in auto_create_dirs.items():\n",
        "    if path.exists():\n",
        "        print(f\"[OK] {name:30} {path}\")\n",
        "    else:\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"[NEW] {name:30} {path}\")\n",
        "\n",
        "for name, path in required_dirs.items():\n",
        "    if path.exists():\n",
        "        print(f\"[OK] {name:30} {path}\")\n",
        "    else:\n",
        "        print(f\"[ERROR] {name:30} {path} (NO ENCONTRADO)\")\n",
        "        print(f\"        El fork es necesario para baseline rule-based\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Verificando dependencias de Python...\n",
            "\n",
            "Core Data Science:\n",
            "  [OK] pandas\n",
            "  [OK] numpy\n",
            "  [OK] scipy\n",
            "\n",
            "Machine Learning:\n",
            "  [OK] sklearn\n",
            "  [OK] transformers\n",
            "  [OK] torch\n",
            "\n",
            "Visualización:\n",
            "  [OK] matplotlib\n",
            "  [OK] seaborn\n",
            "  [OK] plotly\n",
            "\n",
            "NLP/Texto:\n",
            "  [OK] nltk\n",
            "  [OK] spacy\n",
            "\n",
            "Utilities:\n",
            "  [OK] yaml\n",
            "  [OK] tqdm\n",
            "\n",
            "[INFO] Todas las dependencias instaladas correctamente\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Verificación de dependencias\n",
        "print(\"\\n[INFO] Verificando dependencias de Python...\")\n",
        "\n",
        "dependencies = {\n",
        "    'Core Data Science': ['pandas', 'numpy', 'scipy'],\n",
        "    'Machine Learning': ['sklearn', 'transformers', 'torch'],\n",
        "    'Visualización': ['matplotlib', 'seaborn', 'plotly'],\n",
        "    'NLP/Texto': ['nltk', 'spacy'],\n",
        "    'Utilities': ['yaml', 'tqdm']\n",
        "}\n",
        "\n",
        "all_ok = True\n",
        "\n",
        "for category, libs in dependencies.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for lib in libs:\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"  [OK] {lib}\")\n",
        "        except ImportError:\n",
        "            print(f\"  [ERROR] {lib} (NO INSTALADO)\")\n",
        "            all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\n[INFO] Todas las dependencias instaladas correctamente\")\n",
        "else:\n",
        "    print(\"\\n[WARNING] Faltan dependencias. Instalar con: pip install -r requirements.txt\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Verificando archivos de datos...\n",
            "[OK] ips_raw.csv                   \n",
            "     Columnas: ['Archivo', 'Prontuario', 'Nombre Paciente', 'Sexo', 'Fecha Nacimiento', 'N° Consulta', 'Id', 'Fecha Consulta', 'Motivo Consulta', 'Tipo']\n",
            "\n",
            "[WARNING] ips_clean.csv                  (NO ENCONTRADO)\n",
            "         Generado por 01_eda_understanding.ipynb\n",
            "         Usado por: 02_create_splits.ipynb\n",
            "\n",
            "[WARNING] splits/dataset_base.csv        (NO ENCONTRADO)\n",
            "         Generado por 02_create_splits.ipynb\n",
            "         Usado por: Todos los 02_baseline_*.ipynb\n",
            "\n",
            "[WARNING] splits/train_indices.csv       (NO ENCONTRADO)\n",
            "         Generado por 02_create_splits.ipynb\n",
            "         Usado por: Todos los 02_baseline_*.ipynb\n",
            "\n",
            "[WARNING] splits/val_indices.csv         (NO ENCONTRADO)\n",
            "         Generado por 02_create_splits.ipynb\n",
            "         Usado por: Todos los 02_baseline_*.ipynb\n",
            "\n",
            "============================================================\n",
            "[INFO] Todos los archivos críticos disponibles\n"
          ]
        }
      ],
      "source": [
        "# Verificación de archivos de datos\n",
        "print(\"\\n[INFO] Verificando archivos de datos...\")\n",
        "\n",
        "data_files = {\n",
        "    'ips_raw.csv': {\n",
        "        'path': DATA_PATH / 'ips_raw.csv',\n",
        "        'required': True,\n",
        "        'source': 'Archivo original del dataset',\n",
        "        'used_by': '01_eda_understanding.ipynb'\n",
        "    },\n",
        "    'ips_clean.csv': {\n",
        "        'path': DATA_PATH / 'ips_clean.csv',\n",
        "        'required': False,\n",
        "        'source': 'Generado por 01_eda_understanding.ipynb',\n",
        "        'used_by': '02_create_splits.ipynb'\n",
        "    },\n",
        "    'splits/dataset_base.csv': {\n",
        "        'path': SPLITS_PATH / 'dataset_base.csv',\n",
        "        'required': False,\n",
        "        'source': 'Generado por 02_create_splits.ipynb',\n",
        "        'used_by': 'Todos los 02_baseline_*.ipynb'\n",
        "    },\n",
        "    'splits/train_indices.csv': {\n",
        "        'path': SPLITS_PATH / 'train_indices.csv',\n",
        "        'required': False,\n",
        "        'source': 'Generado por 02_create_splits.ipynb',\n",
        "        'used_by': 'Todos los 02_baseline_*.ipynb'\n",
        "    },\n",
        "    'splits/val_indices.csv': {\n",
        "        'path': SPLITS_PATH / 'val_indices.csv',\n",
        "        'required': False,\n",
        "        'source': 'Generado por 02_create_splits.ipynb',\n",
        "        'used_by': 'Todos los 02_baseline_*.ipynb'\n",
        "    }\n",
        "}\n",
        "\n",
        "missing_critical = []\n",
        "\n",
        "for name, info in data_files.items():\n",
        "    exists = info['path'].exists()\n",
        "    required = info['required']\n",
        "    \n",
        "    if exists:\n",
        "        print(f\"[OK] {name:30}\")\n",
        "        if info['path'].suffix == '.csv':\n",
        "            try:\n",
        "                df_info = pd.read_csv(info['path'], nrows=1)\n",
        "                print(f\"     Columnas: {list(df_info.columns)}\")\n",
        "            except:\n",
        "                pass\n",
        "    else:\n",
        "        status = \"[ERROR]\" if required else \"[WARNING]\"\n",
        "        print(f\"{status} {name:30} (NO ENCONTRADO)\")\n",
        "        print(f\"         {info['source']}\")\n",
        "        print(f\"         Usado por: {info['used_by']}\")\n",
        "        \n",
        "        if required:\n",
        "            missing_critical.append(name)\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"=\"*60)\n",
        "if missing_critical:\n",
        "    print(f\"[ERROR] Faltan {len(missing_critical)} archivo(s) crítico(s): {', '.join(missing_critical)}\")\n",
        "else:\n",
        "    print(\"[INFO] Todos los archivos críticos disponibles\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
