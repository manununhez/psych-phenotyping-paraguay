{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Entorno: Local (Jupyter/VSCode)\n",
            "   üí° Ejecutando en entorno local\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 00_setup.ipynb - Configuraci√≥n Inicial del Proyecto\n",
        "# ===============================================================\n",
        "#\n",
        "# **Prop√≥sito**: \n",
        "# - Validar estructura del proyecto\n",
        "# - Verificar dependencias instaladas\n",
        "# - Configurar paths usando utils_shared.py\n",
        "# - Preparar entorno para ejecutar notebooks 01_ y 02_\n",
        "#\n",
        "# **Cu√°ndo ejecutar**:\n",
        "# - Primera vez que clonas el repositorio\n",
        "# - Despu√©s de cambios en la estructura de carpetas\n",
        "# - Para verificar que todo est√° correctamente instalado\n",
        "#\n",
        "# **Qu√© NO hace**:\n",
        "# - No procesa datos (eso es 01_eda_understanding.ipynb)\n",
        "# - No crea splits (eso es 02_create_splits.ipynb)\n",
        "# - No entrena modelos (eso es 02_baseline_*.ipynb)\n",
        "#\n",
        "# ===============================================================\n",
        "\n",
        "import os, sys, pathlib, yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 1. Detecci√≥n de entorno (Local vs Google Colab)\n",
        "# ---------------------------------------------------------------\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"üì¶ Entorno: Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"üì¶ Entorno: Local (Jupyter/VSCode)\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"   üí° En Colab puedes montar Google Drive para persistir datos\")\n",
        "else:\n",
        "    print(\"   üí° Ejecutando en entorno local\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cargando paths desde utils_shared.py\n",
            "   Usando configuraci√≥n centralizada\n",
            "\n",
            "üìÅ Paths configurados:\n",
            "   BASE_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay\n",
            "   DATA_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
            "   FORK_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/Spanish_Psych_Phenotyping_PY\n",
            "   SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n",
            "   FIGS_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/figs\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 2. Configuraci√≥n de Paths usando utils_shared.py\n",
        "# ===============================================================\n",
        "#\n",
        "# Intentamos usar utils_shared.py para centralizar configuraci√≥n.\n",
        "# Si no est√° disponible, usamos configuraci√≥n manual.\n",
        "#\n",
        "# ===============================================================\n",
        "\n",
        "try:\n",
        "    from utils_shared import setup_paths\n",
        "    print(\"‚úÖ Cargando paths desde utils_shared.py\")\n",
        "    \n",
        "    paths = setup_paths()\n",
        "    BASE_PATH = paths['BASE_PATH']\n",
        "    DATA_PATH = paths['DATA_PATH']\n",
        "    FORK_PATH = paths['FORK_PATH']\n",
        "    SPLITS_PATH = paths['SPLITS_PATH']\n",
        "    FIGS_PATH = paths['FIGS_PATH']\n",
        "    \n",
        "    print(\"   Usando configuraci√≥n centralizada\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è utils_shared.py no encontrado, usando configuraci√≥n manual\")\n",
        "    \n",
        "    BASE_PATH = pathlib.Path.cwd()\n",
        "    if BASE_PATH.name == \"notebooks\":\n",
        "        BASE_PATH = BASE_PATH.parent\n",
        "    \n",
        "    DATA_PATH = BASE_PATH / \"data\"\n",
        "    FORK_PATH = BASE_PATH / \"Spanish_Psych_Phenotyping_PY\"\n",
        "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
        "    FIGS_PATH = DATA_PATH / \"figs\"\n",
        "    \n",
        "    # Crear directorios si no existen\n",
        "    DATA_PATH.mkdir(exist_ok=True)\n",
        "    SPLITS_PATH.mkdir(exist_ok=True)\n",
        "    FIGS_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "# En Colab ‚Üí permitir montaje de Google Drive\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        DRIVE_DATA_PATH = pathlib.Path(\"/content/drive/MyDrive/psych-data\")\n",
        "        \n",
        "        if DRIVE_DATA_PATH.exists():\n",
        "            DATA_PATH = DRIVE_DATA_PATH\n",
        "            SPLITS_PATH = DATA_PATH / \"splits\"\n",
        "            FIGS_PATH = DATA_PATH / \"figs\"\n",
        "            print(\"üìÅ Usando datos desde Google Drive\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è No se pudo montar Google Drive: {e}\")\n",
        "\n",
        "print(f\"\\nüìÅ Paths configurados:\")\n",
        "print(f\"   BASE_PATH:   {BASE_PATH}\")\n",
        "print(f\"   DATA_PATH:   {DATA_PATH}\")\n",
        "print(f\"   FORK_PATH:   {FORK_PATH}\")\n",
        "print(f\"   SPLITS_PATH: {SPLITS_PATH}\")\n",
        "print(f\"   FIGS_PATH:   {FIGS_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Verificando estructura del proyecto...\n",
            "\n",
            "‚úÖ data/                          /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
            "‚úÖ data/splits/                   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n",
            "‚úÖ data/figs/                     /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/figs\n",
            "‚úÖ notebooks/                     /Users/manuelnunez/Projects/psych-phenotyping-paraguay/notebooks\n",
            "‚úÖ configs/                       /Users/manuelnunez/Projects/psych-phenotyping-paraguay/configs\n",
            "‚úÖ Spanish_Psych_Phenotyping_PY/  /Users/manuelnunez/Projects/psych-phenotyping-paraguay/Spanish_Psych_Phenotyping_PY\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 3. Verificaci√≥n de Estructura del Proyecto\n",
        "# ===============================================================\n",
        "#\n",
        "# Verifica que existen las carpetas clave del proyecto.\n",
        "# Si faltan, las crea autom√°ticamente (excepto FORK_PATH).\n",
        "#\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\nüîç Verificando estructura del proyecto...\\n\")\n",
        "\n",
        "# Directorios que DEBEN existir (se crean si faltan)\n",
        "auto_create_dirs = {\n",
        "    'data/': DATA_PATH,\n",
        "    'data/splits/': SPLITS_PATH,\n",
        "    'data/figs/': FIGS_PATH,\n",
        "    'notebooks/': BASE_PATH / \"notebooks\",\n",
        "    'configs/': BASE_PATH / \"configs\"\n",
        "}\n",
        "\n",
        "# Directorios que NO se crean autom√°ticamente (requieren acci√≥n del usuario)\n",
        "required_dirs = {\n",
        "    'Spanish_Psych_Phenotyping_PY/': FORK_PATH\n",
        "}\n",
        "\n",
        "# Crear directorios faltantes\n",
        "for name, path in auto_create_dirs.items():\n",
        "    if path.exists():\n",
        "        print(f\"‚úÖ {name:30} {path}\")\n",
        "    else:\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"üÜï {name:30} {path} (creado)\")\n",
        "\n",
        "# Validar directorios que requieren acci√≥n manual\n",
        "for name, path in required_dirs.items():\n",
        "    if path.exists():\n",
        "        print(f\"‚úÖ {name:30} {path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {name:30} {path} (NO ENCONTRADO)\")\n",
        "        print(f\"   ‚ö†Ô∏è El fork es necesario para baseline rule-based\")\n",
        "        print(f\"   üí° Clona desde: https://github.com/[owner]/Spanish_Psych_Phenotyping_PY\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Verificando dependencias de Python...\n",
            "\n",
            "üì¶ Core Data Science:\n",
            "   ‚úÖ pandas\n",
            "   ‚úÖ numpy\n",
            "   ‚úÖ scipy\n",
            "\n",
            "üì¶ Machine Learning:\n",
            "   ‚úÖ sklearn\n",
            "   ‚úÖ transformers\n",
            "   ‚úÖ torch\n",
            "\n",
            "üì¶ Visualizaci√≥n:\n",
            "   ‚úÖ matplotlib\n",
            "   ‚úÖ seaborn\n",
            "   ‚úÖ plotly\n",
            "\n",
            "üì¶ NLP/Texto:\n",
            "   ‚úÖ transformers\n",
            "   ‚úÖ torch\n",
            "\n",
            "üì¶ Visualizaci√≥n:\n",
            "   ‚úÖ matplotlib\n",
            "   ‚úÖ seaborn\n",
            "   ‚úÖ plotly\n",
            "\n",
            "üì¶ NLP/Texto:\n",
            "   ‚úÖ nltk\n",
            "   ‚úÖ nltk\n",
            "   ‚úÖ spacy\n",
            "\n",
            "üì¶ Utilities:\n",
            "   ‚úÖ yaml\n",
            "   ‚úÖ tqdm\n",
            "\n",
            "‚úÖ Todas las dependencias est√°n instaladas correctamente\n",
            "============================================================\n",
            "   ‚úÖ spacy\n",
            "\n",
            "üì¶ Utilities:\n",
            "   ‚úÖ yaml\n",
            "   ‚úÖ tqdm\n",
            "\n",
            "‚úÖ Todas las dependencias est√°n instaladas correctamente\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 4. Verificaci√≥n de Dependencias de Python\n",
        "# ===============================================================\n",
        "#\n",
        "# Verifica que las librer√≠as necesarias est√°n instaladas.\n",
        "# Agrupa por categor√≠a para diagn√≥stico m√°s claro.\n",
        "#\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\nüîç Verificando dependencias de Python...\\n\")\n",
        "\n",
        "dependencies = {\n",
        "    'Core Data Science': ['pandas', 'numpy', 'scipy'],\n",
        "    'Machine Learning': ['sklearn', 'transformers', 'torch'],\n",
        "    'Visualizaci√≥n': ['matplotlib', 'seaborn', 'plotly'],\n",
        "    'NLP/Texto': ['nltk', 'spacy'],\n",
        "    'Utilities': ['yaml', 'tqdm']\n",
        "}\n",
        "\n",
        "all_ok = True\n",
        "\n",
        "for category, libs in dependencies.items():\n",
        "    print(f\"üì¶ {category}:\")\n",
        "    for lib in libs:\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"   ‚úÖ {lib}\")\n",
        "        except ImportError:\n",
        "            print(f\"   ‚ùå {lib} (NO INSTALADO)\")\n",
        "            all_ok = False\n",
        "    print()\n",
        "\n",
        "if all_ok:\n",
        "    print(\"‚úÖ Todas las dependencias est√°n instaladas correctamente\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Faltan dependencias. Instalar con:\")\n",
        "    print(\"   pip install -r requirements.txt\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Verificando archivos de datos...\n",
            "\n",
            "‚úÖ ips_raw.csv                   \n",
            "   üìÑ /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/ips_raw.csv\n",
            "   üìã Columnas: ['Archivo', 'Prontuario', 'Nombre Paciente', 'Sexo', 'Fecha Nacimiento', 'N¬∞ Consulta', 'Id', 'Fecha Consulta', 'Motivo Consulta', 'Tipo']\n",
            "\n",
            "‚úÖ ips_clean.csv                 \n",
            "   üìÑ /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/ips_clean.csv\n",
            "   üìã Columnas: ['id_paciente', 'fecha', 'etiqueta', 'texto']\n",
            "\n",
            "‚úÖ splits/dataset_base.csv       \n",
            "   üìÑ /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits/dataset_base.csv\n",
            "   üìã Columnas: ['row_id', 'texto', 'etiqueta']\n",
            "\n",
            "‚úÖ splits/train_indices.csv      \n",
            "   üìÑ /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits/train_indices.csv\n",
            "   üìã Columnas: ['row_id']\n",
            "\n",
            "‚úÖ splits/val_indices.csv        \n",
            "   üìÑ /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits/val_indices.csv\n",
            "   üìã Columnas: ['row_id']\n",
            "\n",
            "============================================================\n",
            "‚úÖ Archivos cr√≠ticos disponibles\n",
            "üí° Puedes ejecutar 01_eda_understanding.ipynb para comenzar\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 5. Verificaci√≥n de Archivos de Datos\n",
        "# ===============================================================\n",
        "#\n",
        "# Verifica la disponibilidad de archivos clave en el flujo de trabajo:\n",
        "# - ips_raw.csv (entrada original)\n",
        "# - ips_clean.csv (salida de 01_eda)\n",
        "# - splits/ (salida de 02_create_splits)\n",
        "#\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\nüîç Verificando archivos de datos...\\n\")\n",
        "\n",
        "# Archivos clave y su prop√≥sito\n",
        "data_files = {\n",
        "    'ips_raw.csv': {\n",
        "        'path': DATA_PATH / 'ips_raw.csv',\n",
        "        'required': True,\n",
        "        'source': 'Archivo original del dataset',\n",
        "        'used_by': '01_eda_understanding.ipynb'\n",
        "    },\n",
        "    'ips_clean.csv': {\n",
        "        'path': DATA_PATH / 'ips_clean.csv',\n",
        "        'required': False,\n",
        "        'source': 'Generado por 01_eda_understanding.ipynb',\n",
        "        'used_by': '02_create_splits.ipynb'\n",
        "    },\n",
        "    'splits/dataset_base.csv': {\n",
        "        'path': SPLITS_PATH / 'dataset_base.csv',\n",
        "        'required': False,\n",
        "        'source': 'Generado por 02_create_splits.ipynb',\n",
        "        'used_by': 'Todos los 02_baseline_*.ipynb'\n",
        "    },\n",
        "    'splits/train_indices.csv': {\n",
        "        'path': SPLITS_PATH / 'train_indices.csv',\n",
        "        'required': False,\n",
        "        'source': 'Generado por 02_create_splits.ipynb',\n",
        "        'used_by': 'Todos los 02_baseline_*.ipynb'\n",
        "    },\n",
        "    'splits/val_indices.csv': {\n",
        "        'path': SPLITS_PATH / 'val_indices.csv',\n",
        "        'required': False,\n",
        "        'source': 'Generado por 02_create_splits.ipynb',\n",
        "        'used_by': 'Todos los 02_baseline_*.ipynb'\n",
        "    }\n",
        "}\n",
        "\n",
        "missing_critical = []\n",
        "\n",
        "for name, info in data_files.items():\n",
        "    exists = info['path'].exists()\n",
        "    required = info['required']\n",
        "    \n",
        "    if exists:\n",
        "        print(f\"‚úÖ {name:30}\")\n",
        "        print(f\"   üìÑ {info['path']}\")\n",
        "        \n",
        "        # Mostrar info adicional si es CSV\n",
        "        if info['path'].suffix == '.csv':\n",
        "            try:\n",
        "                df_info = pd.read_csv(info['path'], nrows=1)\n",
        "                print(f\"   üìã Columnas: {list(df_info.columns)}\")\n",
        "            except:\n",
        "                pass\n",
        "    else:\n",
        "        status = \"‚ùå CR√çTICO\" if required else \"‚ö†Ô∏è Pendiente\"\n",
        "        print(f\"{status} {name:30} (NO ENCONTRADO)\")\n",
        "        print(f\"   üí° {info['source']}\")\n",
        "        print(f\"   üîó Usado por: {info['used_by']}\")\n",
        "        \n",
        "        if required:\n",
        "            missing_critical.append(name)\n",
        "    \n",
        "    print()\n",
        "\n",
        "# Resumen final\n",
        "print(\"=\"*60)\n",
        "if missing_critical:\n",
        "    print(f\"‚ùå Faltan {len(missing_critical)} archivo(s) cr√≠tico(s):\")\n",
        "    for f in missing_critical:\n",
        "        print(f\"   - {f}\")\n",
        "    print(\"\\n‚ö†Ô∏è No podr√°s ejecutar los notebooks hasta tener estos archivos\")\n",
        "else:\n",
        "    print(\"‚úÖ Archivos cr√≠ticos disponibles\")\n",
        "    print(\"üí° Puedes ejecutar 01_eda_understanding.ipynb para comenzar\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
