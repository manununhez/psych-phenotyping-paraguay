{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1af325",
   "metadata": {},
   "source": [
    "# 02_baseline_transformer_beto â€” Binario A/D\n",
    "\n",
    "**Objetivo:** baseline con **transformer en espaÃ±ol** (*roberta-bne* o equivalente) para contrastar con TFâ€‘IDF y reglas.  \n",
    "**JustificaciÃ³n:** modelos preentrenados capturan semÃ¡ntica y contexto; con preprocesamiento conservador suelen superar a mÃ©todos clÃ¡sicos cuando hay suficiente seÃ±al.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb438c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ INPUT_FILE: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/ips_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# === Paths / Globals (auto-detect) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, unicodedata, os\n",
    "\n",
    "# Rutas y entorno\n",
    "BASE_PATH = Path.cwd()\n",
    "if BASE_PATH.name == \"notebooks\":\n",
    "    BASE_PATH = BASE_PATH.parent\n",
    "\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "FORK_PATH = BASE_PATH / \"Spanish_Psych_Phenotyping_PY\"\n",
    "\n",
    "# Reuse existing globals if present (from your 02_baselines.ipynb)\n",
    "DATA_PATH = Path(DATA_PATH) if 'DATA_PATH' in globals() else Path('data')\n",
    "FORK_PATH = Path(FORK_PATH) if 'FORK_PATH' in globals() else Path('Spanish_Psych_Phenotyping_PY')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "INPUT_FILE   = DATA_PATH/'ips_raw.csv'\n",
    "if not INPUT_FILE.exists():\n",
    "    raise FileNotFoundError(\"No se encontrÃ³ ni ips_clean.csv ni ips_raw.csv en \" + str(DATA_PATH))\n",
    "\n",
    "print(\"ðŸ“¥ INPUT_FILE:\", INPUT_FILE)\n",
    "\n",
    "# --- Columnas reales de tu dataset ---\n",
    "TEXT_COL = \"texto\"\n",
    "LABEL_COL = \"etiqueta\"\n",
    "\n",
    "# Column preferences (honor globals if defined)\n",
    "TEXT_COL  = TEXT_COL  if 'TEXT_COL'  in globals() else None\n",
    "LABEL_COL = LABEL_COL if 'LABEL_COL' in globals() else None\n",
    "\n",
    "def _guess_text_col(df):\n",
    "    if TEXT_COL and TEXT_COL in df.columns: \n",
    "        return TEXT_COL\n",
    "    for c in ['texto','Motivo Consulta','original_motivo_consulta','text']:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'O': return c\n",
    "    raise ValueError(\"No se encontrÃ³ columna de texto.\")\n",
    "\n",
    "def _guess_label_col(df):\n",
    "    if LABEL_COL and LABEL_COL in df.columns: \n",
    "        return LABEL_COL\n",
    "    for c in ['etiqueta','Tipo','label','target','y','clase']:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def _norm_label_bin(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    return {'depresivo':'depresion'}.get(s, s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27dc21",
   "metadata": {},
   "source": [
    "## 1) Carga y preprocesamiento **conservador** (preserva tildes/casing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a50745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Splits cargados: Train=2518 | Val=630\n",
      "Train: 2518 | Val: 630\n",
      "Train distribuciÃ³n:\n",
      "label\n",
      "0    1778\n",
      "1     740\n",
      "Name: count, dtype: int64\n",
      "Val distribuciÃ³n:\n",
      "label\n",
      "0    445\n",
      "1    185\n",
      "Name: count, dtype: int64\n",
      "Train: 2518 | Val: 630\n",
      "Train distribuciÃ³n:\n",
      "label\n",
      "0    1778\n",
      "1     740\n",
      "Name: count, dtype: int64\n",
      "Val distribuciÃ³n:\n",
      "label\n",
      "0    445\n",
      "1    185\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, re, unicodedata\n",
    "\n",
    "# === CARGAR SPLITS UNIFICADOS ===\n",
    "SPLITS_PATH = DATA_PATH / \"splits\"\n",
    "dataset_base = pd.read_csv(SPLITS_PATH / 'dataset_base.csv')\n",
    "train_indices = pd.read_csv(SPLITS_PATH / 'train_indices.csv')['row_id'].values\n",
    "val_indices = pd.read_csv(SPLITS_PATH / 'val_indices.csv')['row_id'].values\n",
    "\n",
    "print(f\"âœ… Splits cargados: Train={len(train_indices)} | Val={len(val_indices)}\")\n",
    "\n",
    "text_col  = _guess_text_col(dataset_base)\n",
    "label_col = _guess_label_col(dataset_base)\n",
    "\n",
    "# Transformer: conservador (mantener tildes/casing; sÃ³lo alargamientos/espacios)\n",
    "RE_MULTI = re.compile(r'(.)\\1{2,}')\n",
    "def clean_text_trf(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = RE_MULTI.sub(r'\\1\\1', s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "dataset_base['texto_trf'] = dataset_base[text_col].map(clean_text_trf)\n",
    "label2id = {'depresion':0, 'ansiedad':1}\n",
    "dataset_base['label'] = dataset_base[label_col].map(label2id)\n",
    "\n",
    "# Separar train y val usando Ã­ndices guardados\n",
    "train_df = dataset_base[dataset_base['row_id'].isin(train_indices)][['texto_trf','label']].copy()\n",
    "val_df = dataset_base[dataset_base['row_id'].isin(val_indices)][['texto_trf','label']].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)}\")\n",
    "print(f\"Train distribuciÃ³n:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"Val distribuciÃ³n:\\n{val_df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc44e3e",
   "metadata": {},
   "source": [
    "## 2) TokenizaciÃ³n y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6298d69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987fef2fae6f4109af42fa3ca6fb0cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fcc9ead7094246bd058a84128826a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "MODEL = \"dccuchile/bert-base-spanish-wwm-cased\"  # EspaÃ±ol cased\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tok(batch[\"texto_trf\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True)).map(preprocess, batched=True, remove_columns=[\"texto_trf\"])\n",
    "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True)).map(preprocess, batched=True, remove_columns=[\"texto_trf\"])\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede33d46",
   "metadata": {},
   "source": [
    "## 3) Entrenamiento y evaluaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330e270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/25/fy01l91x3gj63g090ghxj7000000gn/T/ipykernel_3980/2543227052.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/var/folders/25/fy01l91x3gj63g090ghxj7000000gn/T/ipykernel_3980/2543227052.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 08:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.367800</td>\n",
       "      <td>0.311753</td>\n",
       "      <td>0.807470</td>\n",
       "      <td>0.865743</td>\n",
       "      <td>0.780443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.292184</td>\n",
       "      <td>0.852454</td>\n",
       "      <td>0.853017</td>\n",
       "      <td>0.851898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>0.315057</td>\n",
       "      <td>0.855857</td>\n",
       "      <td>0.869106</td>\n",
       "      <td>0.845339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eval guardada: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_eval.csv\n"
     ]
    }
   ],
   "source": [
    "import evaluate, numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "id2label = {0:'depresion', 1:'ansiedad'}\n",
    "label2id = {'depresion':0, 'ansiedad':1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "metric_f1   = evaluate.load(\"f1\")\n",
    "metric_prec = evaluate.load(\"precision\")\n",
    "metric_rec  = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"macro_f1\":   metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"macro_precision\": metric_prec.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"macro_recall\":    metric_rec.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"runs/beto_ad\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_res = trainer.evaluate()\n",
    "\n",
    "import pandas as pd\n",
    "(pd.DataFrame([eval_res]).to_csv(DATA_PATH/'beto_eval.csv', index=False, encoding='utf-8'))\n",
    "print(\"âœ… Eval guardada:\", DATA_PATH/'beto_eval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ae2c9",
   "metadata": {},
   "source": [
    "## 4) Reporte detallado y predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6178f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exportados:\n",
      " - Predicciones: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_predictions.csv\n",
      " - Reporte: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_classification_report.csv\n",
      " - Eval: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_eval.csv\n",
      " - Matriz: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_confusion_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred_logits = trainer.predict(val_ds).predictions\n",
    "pred_ids = pred_logits.argmax(axis=-1)\n",
    "y_true = val_df[\"label\"].to_numpy()\n",
    "\n",
    "# Exportables\n",
    "beto_pred_csv   = DATA_PATH/'beto_predictions.csv'\n",
    "beto_report_csv = DATA_PATH/'beto_classification_report.csv'\n",
    "beto_eval_csv   = DATA_PATH/'beto_eval.csv'  # ya creado arriba\n",
    "beto_cm_csv     = DATA_PATH/'beto_confusion_matrix.csv'\n",
    "\n",
    "pd.DataFrame(classification_report(y_true, pred_ids, target_names=['depresion','ansiedad'], output_dict=True, zero_division=0))  .transpose().to_csv(beto_report_csv, index=True, encoding='utf-8')\n",
    "\n",
    "cm = confusion_matrix(y_true, pred_ids, labels=[0,1])\n",
    "pd.DataFrame(cm, index=['true_depresion','true_ansiedad'], columns=['pred_depresion','pred_ansiedad']).to_csv(beto_cm_csv)\n",
    "\n",
    "# Con textos (Ãºtil para anÃ¡lisis de errores)\n",
    "val_out = val_df.copy()\n",
    "val_out[\"y_true\"] = val_out[\"label\"].map({0:\"depresion\",1:\"ansiedad\"})\n",
    "val_out[\"y_pred\"] = [ {0:\"depresion\",1:\"ansiedad\"}[i] for i in pred_ids ]\n",
    "val_out.drop(columns=[\"label\"]).to_csv(beto_pred_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… Exportados:\")\n",
    "print(\" - Predicciones:\", beto_pred_csv)\n",
    "print(\" - Reporte:\", beto_report_csv)\n",
    "print(\" - Eval:\", beto_eval_csv)\n",
    "print(\" - Matriz:\", beto_cm_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
