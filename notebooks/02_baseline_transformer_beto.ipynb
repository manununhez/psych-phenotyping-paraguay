{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1af325",
   "metadata": {},
   "source": [
    "# 02_baseline_transformer_beto — Binario A/D\n",
    "\n",
    "**Objetivo:** baseline con **transformer en español** (*roberta-bne* o equivalente) para contrastar con TF‑IDF y reglas.  \n",
    "**Justificación:** modelos preentrenados capturan semántica y contexto; con preprocesamiento conservador suelen superar a métodos clásicos cuando hay suficiente señal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb438c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Utilizando utils_shared.py\n",
      "[INFO] Paths configurados:\n",
      "  BASE_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay\n",
      "  DATA_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
      "  SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Paths, Imports, y Utilidades Compartidas\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, unicodedata, os\n",
    "\n",
    "# Intentar importar utilidades compartidas\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, str(Path.cwd()))\n",
    "    from utils_shared import setup_paths, guess_text_col, guess_label_col, normalize_label\n",
    "    print(\"[INFO] Utilizando utils_shared.py\")\n",
    "    \n",
    "    # Setup de paths centralizado\n",
    "    paths = setup_paths()\n",
    "    BASE_PATH = paths['BASE_PATH']\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    \n",
    "    # Usar funciones centralizadas\n",
    "    _guess_text_col = guess_text_col\n",
    "    _guess_label_col = guess_label_col\n",
    "    _norm_label_bin = normalize_label\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"[WARNING] utils_shared.py no encontrado, usando funciones locales\")\n",
    "    \n",
    "    # Setup manual de paths\n",
    "    BASE_PATH = Path.cwd()\n",
    "    if BASE_PATH.name == \"notebooks\":\n",
    "        BASE_PATH = BASE_PATH.parent\n",
    "    \n",
    "    DATA_PATH = BASE_PATH / \"data\"\n",
    "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
    "    \n",
    "    DATA_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Funciones helper locales\n",
    "    def _guess_text_col(df):\n",
    "        for c in [\"texto\", \"text\", \"comment\", \"comentario\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[0]\n",
    "    \n",
    "    def _guess_label_col(df):\n",
    "        for c in [\"etiqueta\", \"label\", \"category\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[1] if len(df.columns) > 1 else df.columns[-1]\n",
    "    \n",
    "    def _norm_label_bin(s):\n",
    "        if pd.isna(s): \n",
    "            return \"\"\n",
    "        s = str(s).strip().lower()\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "        return {'depresivo': 'depresion'}.get(s, s)\n",
    "\n",
    "# Validar existencia de splits\n",
    "if not SPLITS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[ERROR] Splits no encontrados en {SPLITS_PATH}\\n\"\n",
    "        f\"        Debes ejecutar primero: 02_create_splits.ipynb\"\n",
    "    )\n",
    "\n",
    "print(f\"[INFO] Paths configurados:\")\n",
    "print(f\"  BASE_PATH:   {BASE_PATH}\")\n",
    "print(f\"  DATA_PATH:   {DATA_PATH}\")\n",
    "print(f\"  SPLITS_PATH: {SPLITS_PATH}\")\n",
    "\n",
    "# Columnas esperadas en dataset_base.csv\n",
    "TEXT_COL = \"texto\"\n",
    "LABEL_COL = \"etiqueta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27dc21",
   "metadata": {},
   "source": [
    "## 1) Carga y preprocesamiento **conservador** (preserva tildes/casing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a50745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splits: 2509 train, 646 val\n",
      "[INFO] Distribución train: {'depresion': 1745, 'ansiedad': 764}\n",
      "[INFO] Distribución val: {'depresion': 485, 'ansiedad': 161}\n",
      "[INFO] Distribución train: {'depresion': 1745, 'ansiedad': 764}\n",
      "[INFO] Distribución val: {'depresion': 485, 'ansiedad': 161}\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Carga de Datos y Preprocesamiento CONSERVADOR (BETO/Transformer)\n",
    "# ===============================================================\n",
    "#\n",
    "# ESTRATEGIA DE PREPROCESAMIENTO: CONSERVADORA (mínimo)\n",
    "#\n",
    "# ¿Por qué preprocesamiento conservador/mínimo?\n",
    "#\n",
    "# 1. **Los transformers están preentrenados con texto \"natural\"**:\n",
    "#    - BETO/RoBERTa se entrenaron con Wikipedia, noticias, web en español\n",
    "#    - Ese texto tiene mayúsculas, tildes, puntuación original\n",
    "#    - Normalizar agresivamente = salirse de la distribución de entrenamiento\n",
    "#\n",
    "# 2. **Tokenización BPE maneja variaciones**:\n",
    "#    - \"Depresión\" y \"depresión\" → mismo subtokens (el tokenizer lo normaliza)\n",
    "#    - El modelo aprende equivalencias durante el pretraining\n",
    "#    - No necesitamos lowercase manual\n",
    "#\n",
    "# 3. **Embeddings contextuales capturan semántica**:\n",
    "#    - BETO entiende \"no tengo apetito\" vs \"tengo apetito\" sin marcadores\n",
    "#    - La atención captura negación implícitamente\n",
    "#    - No necesitamos heurísticas como \"no_X\"\n",
    "#\n",
    "# Preprocesamiento conservador para transformers\n",
    "#\n",
    "# Estrategia: Mínimo (solo colapsa alargamientos, preserva todo lo demás)\n",
    "# - BETO se entrenó con texto natural (mayúsculas, tildes, puntuación)\n",
    "# - WordPiece tokenization maneja variaciones automáticamente\n",
    "# - Comparación: Rule-based conserva para patterns, TF-IDF normaliza, BETO preserva distribución original\n",
    "\n",
    "import pandas as pd, re, unicodedata\n",
    "\n",
    "# Cargar splits unificados desde 02_create_splits.ipynb\n",
    "dataset_base = pd.read_csv(SPLITS_PATH / 'dataset_base.csv')\n",
    "train_indices = pd.read_csv(SPLITS_PATH / 'train_indices.csv')['row_id'].values\n",
    "val_indices = pd.read_csv(SPLITS_PATH / 'val_indices.csv')['row_id'].values\n",
    "\n",
    "text_col = _guess_text_col(dataset_base)\n",
    "label_col = _guess_label_col(dataset_base)\n",
    "\n",
    "print(f\"[INFO] Splits: {len(train_indices)} train, {len(val_indices)} val\")\n",
    "\n",
    "# Definir función de limpieza conservadora\n",
    "RE_MULTI = re.compile(r'(.)\\1{2,}')  # Detecta 3+ letras repetidas\n",
    "\n",
    "def clean_text_trf(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpieza CONSERVADORA para transformers (mínimo preprocesamiento).\n",
    "    \n",
    "    Aplica ÚNICAMENTE:\n",
    "    - Normalización NFC (forma canónica de tildes)\n",
    "    - Colapso de alargamientos (holaaa → holaa)\n",
    "    - Normalización de espacios\n",
    "    \n",
    "    Preserva:\n",
    "    - Mayúsculas y minúsculas (BETO las usa)\n",
    "    - Tildes y acentos (parte del vocabulario)\n",
    "    - Puntuación (señal contextual)\n",
    "    - Estructura original del texto\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    \n",
    "    s = str(s).strip()\n",
    "    s = unicodedata.normalize(\"NFC\", s)  # Normaliza tildes (é = é, no e + ´)\n",
    "    s = RE_MULTI.sub(r'\\1\\1', s)         # holaaa → holaa (evita OOV)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()   # Colapsa espacios múltiples\n",
    "    \n",
    "    return s\n",
    "\n",
    "dataset_base['texto_trf'] = dataset_base[text_col].map(clean_text_trf)\n",
    "\n",
    "df_train = dataset_base[dataset_base['row_id'].isin(train_indices)].copy()\n",
    "df_val = dataset_base[dataset_base['row_id'].isin(val_indices)].copy()\n",
    "\n",
    "X_train, y_train = df_train['texto_trf'], df_train[label_col]\n",
    "X_val, y_val = df_val['texto_trf'], df_val[label_col]\n",
    "\n",
    "print(f\"[INFO] Distribución train: {dict(y_train.value_counts())}\")\n",
    "print(f\"[INFO] Distribución val: {dict(y_val.value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc44e3e",
   "metadata": {},
   "source": [
    "## 2) Tokenización y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6298d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Mapeo de etiquetas: {'depresion': 0, 'ansiedad': 1}\n",
      "[INFO] Distribución train (numérica): {0: 1745, 1: 764}\n",
      "[INFO] Distribución val (numérica): {0: 485, 1: 161}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dc2948f687447686ed17718764192c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2509 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77079c780494786964de8d1d6703a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Datasets creados:\n",
      "  Train: 2509 ejemplos\n",
      "  Val: 646 ejemplos\n",
      "  Columnas train_ds: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "  Columnas val_ds: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "MODEL = \"dccuchile/bert-base-spanish-wwm-cased\"  # Español cased\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Mapeo de etiquetas a IDs numéricos\n",
    "label2id = {'depresion': 0, 'ansiedad': 1}\n",
    "id2label = {0: 'depresion', 1: 'ansiedad'}\n",
    "\n",
    "# CRÍTICO: Agregar columna 'labels' numérica (el Trainer la requiere)\n",
    "df_train['labels'] = df_train[label_col].map(label2id)\n",
    "df_val['labels'] = df_val[label_col].map(label2id)\n",
    "\n",
    "print(f\"[INFO] Mapeo de etiquetas: {label2id}\")\n",
    "print(f\"[INFO] Distribución train (numérica): {dict(df_train['labels'].value_counts())}\")\n",
    "print(f\"[INFO] Distribución val (numérica): {dict(df_val['labels'].value_counts())}\")\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tok(batch[\"texto_trf\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "# Crear datasets con columnas: texto_trf y labels\n",
    "train_ds = Dataset.from_pandas(df_train[['texto_trf', 'labels']].reset_index(drop=True)).map(preprocess, batched=True, remove_columns=[\"texto_trf\"])\n",
    "val_ds = Dataset.from_pandas(df_val[['texto_trf', 'labels']].reset_index(drop=True)).map(preprocess, batched=True, remove_columns=[\"texto_trf\"])\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "print(f\"\\n[INFO] Datasets creados:\")\n",
    "print(f\"  Train: {len(train_ds)} ejemplos\")\n",
    "print(f\"  Val: {len(val_ds)} ejemplos\")\n",
    "print(f\"  Columnas train_ds: {train_ds.column_names}\")\n",
    "print(f\"  Columnas val_ds: {val_ds.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede33d46",
   "metadata": {},
   "source": [
    "## 3) Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330e270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/25/fy01l91x3gj63g090ghxj7000000gn/T/ipykernel_3805/719104514.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/var/folders/25/fy01l91x3gj63g090ghxj7000000gn/T/ipykernel_3805/719104514.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Iniciando entrenamiento...\n",
      "  Epochs: 3\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 06:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.481634</td>\n",
       "      <td>0.694681</td>\n",
       "      <td>0.714829</td>\n",
       "      <td>0.682417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.547733</td>\n",
       "      <td>0.704530</td>\n",
       "      <td>0.712809</td>\n",
       "      <td>0.697983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.587868</td>\n",
       "      <td>0.736864</td>\n",
       "      <td>0.743670</td>\n",
       "      <td>0.731088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entrenamiento completado\n",
      "  Macro F1: 0.7369\n",
      "  Macro Precision: 0.7437\n",
      "  Macro Recall: 0.7311\n",
      "[INFO] Eval guardada: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_eval.csv\n"
     ]
    }
   ],
   "source": [
    "import evaluate, numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Mapeo ya definido en celda anterior\n",
    "# label2id = {'depresion': 0, 'ansiedad': 1}\n",
    "# id2label = {0: 'depresion', 1: 'ansiedad'}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "metric_f1   = evaluate.load(\"f1\")\n",
    "metric_prec = evaluate.load(\"precision\")\n",
    "metric_rec  = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"macro_f1\":   metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"macro_precision\": metric_prec.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"macro_recall\":    metric_rec.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"runs/beto_ad\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"[INFO] Iniciando entrenamiento...\")\n",
    "print(f\"  Epochs: {args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {args.learning_rate}\")\n",
    "\n",
    "trainer.train()\n",
    "eval_res = trainer.evaluate()\n",
    "\n",
    "import pandas as pd\n",
    "(pd.DataFrame([eval_res]).to_csv(DATA_PATH/'beto_eval.csv', index=False, encoding='utf-8'))\n",
    "print(f\"\\n[INFO] Entrenamiento completado\")\n",
    "print(f\"  Macro F1: {eval_res['eval_macro_f1']:.4f}\")\n",
    "print(f\"  Macro Precision: {eval_res['eval_macro_precision']:.4f}\")\n",
    "print(f\"  Macro Recall: {eval_res['eval_macro_recall']:.4f}\")\n",
    "print(f\"[INFO] Eval guardada: {DATA_PATH/'beto_eval.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ae2c9",
   "metadata": {},
   "source": [
    "## 4) Reporte detallado y predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6178f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Exportados:\n",
      "  - Predicciones: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_predictions.csv\n",
      "  - Reporte: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_classification_report.csv\n",
      "  - Eval: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_eval.csv\n",
      "  - Matriz: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/beto_confusion_matrix.csv\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT (BETO)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   depresion       0.86      0.88      0.87       485\n",
      "    ansiedad       0.62      0.58      0.60       161\n",
      "\n",
      "    accuracy                           0.81       646\n",
      "   macro avg       0.74      0.73      0.74       646\n",
      "weighted avg       0.80      0.81      0.81       646\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "                pred_depresion  pred_ansiedad\n",
      "true_depresion             429             56\n",
      "true_ansiedad               68             93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred_logits = trainer.predict(val_ds).predictions\n",
    "pred_ids = pred_logits.argmax(axis=-1)\n",
    "y_true = df_val[\"labels\"].to_numpy()  # CORREGIDO: usar 'labels' en lugar de 'label'\n",
    "\n",
    "# Exportables\n",
    "beto_pred_csv   = DATA_PATH/'beto_predictions.csv'\n",
    "beto_report_csv = DATA_PATH/'beto_classification_report.csv'\n",
    "beto_eval_csv   = DATA_PATH/'beto_eval.csv'  # ya creado arriba\n",
    "beto_cm_csv     = DATA_PATH/'beto_confusion_matrix.csv'\n",
    "\n",
    "pd.DataFrame(classification_report(y_true, pred_ids, target_names=['depresion','ansiedad'], output_dict=True, zero_division=0)).transpose().to_csv(beto_report_csv, index=True, encoding='utf-8')\n",
    "\n",
    "cm = confusion_matrix(y_true, pred_ids, labels=[0,1])\n",
    "pd.DataFrame(cm, index=['true_depresion','true_ansiedad'], columns=['pred_depresion','pred_ansiedad']).to_csv(beto_cm_csv)\n",
    "\n",
    "# Con textos (útil para análisis de errores)\n",
    "val_out = df_val.copy()\n",
    "val_out[\"y_true\"] = val_out[\"labels\"].map({0:\"depresion\",1:\"ansiedad\"})\n",
    "val_out[\"y_pred\"] = [id2label[i] for i in pred_ids]\n",
    "val_out.to_csv(beto_pred_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"[INFO] Exportados:\")\n",
    "print(f\"  - Predicciones: {beto_pred_csv}\")\n",
    "print(f\"  - Reporte: {beto_report_csv}\")\n",
    "print(f\"  - Eval: {beto_eval_csv}\")\n",
    "print(f\"  - Matriz: {beto_cm_csv}\")\n",
    "\n",
    "# Mostrar reporte en consola\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (BETO)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, pred_ids, target_names=['depresion','ansiedad'], zero_division=0))\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(pd.DataFrame(cm, index=['true_depresion','true_ansiedad'], columns=['pred_depresion','pred_ansiedad']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a70fd",
   "metadata": {},
   "source": [
    "## 5) Análisis de Errores (FP/FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cdab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Análisis de errores exportado:\n",
      "  FP Depresión: 68 casos → beto_fp_depresion.csv\n",
      "  FN Depresión: 56 casos → beto_fn_depresion.csv\n",
      "  FP Ansiedad:  56 casos → beto_fp_ansiedad.csv\n",
      "  FN Ansiedad:  68 casos → beto_fn_ansiedad.csv\n"
     ]
    }
   ],
   "source": [
    "# Exportar errores para análisis cualitativo\n",
    "# Usar val_out que ya tiene y_true/y_pred en formato texto\n",
    "\n",
    "fp_depresion = val_out[(val_out['y_true'] == 'ansiedad') & (val_out['y_pred'] == 'depresion')].copy()\n",
    "fp_depresion['error_type'] = 'FP_depresion'\n",
    "\n",
    "fn_depresion = val_out[(val_out['y_true'] == 'depresion') & (val_out['y_pred'] == 'ansiedad')].copy()\n",
    "fn_depresion['error_type'] = 'FN_depresion'\n",
    "\n",
    "fp_ansiedad = val_out[(val_out['y_true'] == 'depresion') & (val_out['y_pred'] == 'ansiedad')].copy()\n",
    "fp_ansiedad['error_type'] = 'FP_ansiedad'\n",
    "\n",
    "fn_ansiedad = val_out[(val_out['y_true'] == 'ansiedad') & (val_out['y_pred'] == 'depresion')].copy()\n",
    "fn_ansiedad['error_type'] = 'FN_ansiedad'\n",
    "\n",
    "beto_fp_dep_csv = DATA_PATH / 'beto_fp_depresion.csv'\n",
    "beto_fn_dep_csv = DATA_PATH / 'beto_fn_depresion.csv'\n",
    "beto_fp_ans_csv = DATA_PATH / 'beto_fp_ansiedad.csv'\n",
    "beto_fn_ans_csv = DATA_PATH / 'beto_fn_ansiedad.csv'\n",
    "\n",
    "fp_depresion[['texto_trf', 'y_true', 'y_pred', 'error_type']].to_csv(beto_fp_dep_csv, index=False, encoding='utf-8')\n",
    "fn_depresion[['texto_trf', 'y_true', 'y_pred', 'error_type']].to_csv(beto_fn_dep_csv, index=False, encoding='utf-8')\n",
    "fp_ansiedad[['texto_trf', 'y_true', 'y_pred', 'error_type']].to_csv(beto_fp_ans_csv, index=False, encoding='utf-8')\n",
    "fn_ansiedad[['texto_trf', 'y_true', 'y_pred', 'error_type']].to_csv(beto_fn_ans_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"[INFO] Análisis de errores exportado:\")\n",
    "print(f\"  FP Depresión: {len(fp_depresion)} casos → {beto_fp_dep_csv.name}\")\n",
    "print(f\"  FN Depresión: {len(fn_depresion)} casos → {beto_fn_dep_csv.name}\")\n",
    "print(f\"  FP Ansiedad:  {len(fp_ansiedad)} casos → {beto_fp_ans_csv.name}\")\n",
    "print(f\"  FN Ansiedad:  {len(fn_ansiedad)} casos → {beto_fn_ans_csv.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
