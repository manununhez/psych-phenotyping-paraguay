{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc45479",
   "metadata": {},
   "source": [
    "# 02_baseline_rule_based ‚Äî Binario A/D\n",
    "\n",
    "**Objetivo:** baseline **rule-based** usando el fork del proyecto colombiano (solo **Ansiedad/Depresi√≥n**) para obtener una primera l√≠nea de referencia.  \n",
    "**Justificaci√≥n:** las reglas permiten:\n",
    "- establecer un punto de partida interpretable (trazabilidad por JSON/patrones),\n",
    "- detectar fallos sistem√°ticos del dataset (typos, negaci√≥n, expresiones locales),\n",
    "- guiar el dise√±o del *cleaning* y la selecci√≥n de fenotipos relevantes para A/D.\n",
    "\n",
    "> Nota: mantenemos **preprocesamiento ligero** para no romper *ConText* ni *TargetMatcher*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c9b111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T22:35:23.465334Z",
     "iopub.status.busy": "2025-11-15T22:35:23.465144Z",
     "iopub.status.idle": "2025-11-15T22:35:24.100237Z",
     "shell.execute_reply": "2025-11-15T22:35:24.099725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Utilizando utils_shared.py\n",
      "[INFO] Paths configurados:\n",
      "  BASE_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay\n",
      "  DATA_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
      "  FORK_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/Spanish_Psych_Phenotyping_PY\n",
      "  SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Paths, Imports, y Utilidades Compartidas\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, unicodedata, os\n",
    "\n",
    "# Intentar importar utilidades compartidas\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "    from utils_shared import setup_paths, guess_text_col, guess_label_col, normalize_label\n",
    "    print(\"[INFO] Utilizando utils_shared.py\")\n",
    "    \n",
    "    # Setup de paths centralizado\n",
    "    paths = setup_paths()\n",
    "    BASE_PATH = paths['BASE_PATH']\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    FORK_PATH = paths['FORK_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    \n",
    "    # Usar funciones centralizadas\n",
    "    _guess_text_col = guess_text_col\n",
    "    _guess_label_col = guess_label_col\n",
    "    _norm_label_bin = normalize_label\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"[WARNING] utils_shared.py no encontrado, usando funciones locales\")\n",
    "    \n",
    "    # Setup manual de paths\n",
    "    BASE_PATH = Path.cwd()\n",
    "    if BASE_PATH.name == \"notebooks\":\n",
    "        BASE_PATH = BASE_PATH.parent\n",
    "    \n",
    "    DATA_PATH = BASE_PATH / \"data\"\n",
    "    FORK_PATH = BASE_PATH / \"Spanish_Psych_Phenotyping_PY\"\n",
    "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
    "    \n",
    "    DATA_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Funciones helper locales\n",
    "    def _guess_text_col(df):\n",
    "        for c in [\"texto\", \"text\", \"comment\", \"comentario\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[0]\n",
    "    \n",
    "    def _guess_label_col(df):\n",
    "        for c in [\"etiqueta\", \"label\", \"category\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[1] if len(df.columns) > 1 else df.columns[-1]\n",
    "    \n",
    "    def _norm_label_bin(s):\n",
    "        if pd.isna(s): \n",
    "            return \"\"\n",
    "        s = str(s).strip().lower()\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "        return {'depresivo': 'depresion'}.get(s, s)\n",
    "\n",
    "# Validar existencia de directorios cr√≠ticos\n",
    "if not FORK_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[ERROR] Fork no encontrado en {FORK_PATH}\\n\"\n",
    "        f\"        Este baseline requiere Spanish_Psych_Phenotyping_PY/\"\n",
    "    )\n",
    "\n",
    "if not SPLITS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[ERROR] Splits no encontrados en {SPLITS_PATH}\\n\"\n",
    "        f\"        Debes ejecutar primero: 02_create_splits.ipynb\"\n",
    "    )\n",
    "\n",
    "print(f\"[INFO] Paths configurados:\")\n",
    "print(f\"  BASE_PATH:   {BASE_PATH}\")\n",
    "print(f\"  DATA_PATH:   {DATA_PATH}\")\n",
    "print(f\"  FORK_PATH:   {FORK_PATH}\")\n",
    "print(f\"  SPLITS_PATH: {SPLITS_PATH}\")\n",
    "\n",
    "# Columnas esperadas en dataset_base.csv\n",
    "TEXT_COL = \"texto\"\n",
    "LABEL_COL = \"etiqueta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150cec7",
   "metadata": {},
   "source": [
    "## 1) Carga y preprocesamiento **ligero** (conserva tildes y casing, colapsa alargamientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "489012c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T22:35:24.102773Z",
     "iopub.status.busy": "2025-11-15T22:35:24.102442Z",
     "iopub.status.idle": "2025-11-15T22:35:24.368688Z",
     "shell.execute_reply": "2025-11-15T22:35:24.368355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CARGA DE SPLITS (PATIENT-LEVEL)\n",
      "============================================================\n",
      "‚úì Dataset base: 3127 casos\n",
      "‚úì Train indices: 1849 casos\n",
      "‚úì Dev indices: 641 casos\n",
      "\n",
      "[INFO] Split aplicado (patient-level):\n",
      "  Train: 1849 casos\n",
      "  Dev:   641 casos\n",
      "\n",
      "[INFO] Distribuci√≥n train: {'depresion': 1270, 'ansiedad': 579}\n",
      "[INFO] Distribuci√≥n val: {'depresion': 456, 'ansiedad': 185}\n",
      "\n",
      "  RECORDATORIO: Estos splits eliminan leakage (pacientes disjuntos)\n",
      "   M√©tricas ser√°n m√°s conservadoras pero generalizan mejor.\n",
      "\n",
      "[INFO] Split aplicado (patient-level):\n",
      "  Train: 1849 casos\n",
      "  Dev:   641 casos\n",
      "\n",
      "[INFO] Distribuci√≥n train: {'depresion': 1270, 'ansiedad': 579}\n",
      "[INFO] Distribuci√≥n val: {'depresion': 456, 'ansiedad': 185}\n",
      "\n",
      "  RECORDATORIO: Estos splits eliminan leakage (pacientes disjuntos)\n",
      "   M√©tricas ser√°n m√°s conservadoras pero generalizan mejor.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# CARGA DE DATOS - PATIENT-LEVEL SPLIT (SIN LEAKAGE)\n",
    "# ===============================================================\n",
    "# IMPORTANTE: Este baseline usa splits generados por 02_create_splits.ipynb\n",
    "#\n",
    "# Estrategia de split:\n",
    "#   - Por PACIENTES (no por casos/consultas)\n",
    "#   - 72 pacientes train / 18 pacientes val\n",
    "#   - 0% overlap (sin data leakage)\n",
    "#\n",
    "# ¬øPor qu√© patient-level?\n",
    "#   Dataset tiene estructura longitudinal: 90 pacientes √ó 35 consultas promedio\n",
    "#   Split por casos ‚Üí 100% pacientes en train Y dev (leakage total)\n",
    "#   Split por pacientes ‚Üí 0% overlap (generaliza a pacientes nuevos)\n",
    "#\n",
    "# Ver detalles en: ESTRATEGIA_SPLIT_PACIENTES.md\n",
    "\n",
    "import pandas as pd, re, unicodedata\n",
    "\n",
    "# Cargar splits unificados desde 02_create_splits.ipynb\n",
    "print(\"=\"*60)\n",
    "print(\"CARGA DE SPLITS (PATIENT-LEVEL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset_base = pd.read_csv(SPLITS_PATH / 'dataset_base.csv')\n",
    "train_indices = pd.read_csv(SPLITS_PATH / 'train_indices.csv')['row_id'].values\n",
    "dev_indices = pd.read_csv(SPLITS_PATH / 'dev_indices.csv')['row_id'].values\n",
    "\n",
    "print(f\"‚úì Dataset base: {len(dataset_base)} casos\")\n",
    "print(f\"‚úì Train indices: {len(train_indices)} casos\")\n",
    "print(f\"‚úì Dev indices: {len(dev_indices)} casos\")\n",
    "\n",
    "text_col = _guess_text_col(dataset_base)\n",
    "label_col = _guess_label_col(dataset_base)\n",
    "\n",
    "# ===============================================================\n",
    "# PREPROCESAMIENTO LIGERO (conserva estructura para rule-based)\n",
    "# ===============================================================\n",
    "# Estrategia: M√≠nima normalizaci√≥n (preserva tildes, may√∫sculas, puntuaci√≥n)\n",
    "# - Los patrones JSON son case-sensitive y usan tildes\n",
    "# - Solo colapsa alargamientos para mantener matching\n",
    "# - Comparaci√≥n: TF-IDF normaliza todo, BETO tokeniza, Rule-based conserva estructura\n",
    "\n",
    "_RE_MULTI = re.compile(r'(.)\\1{2,}')  # Detecta 3+ letras repetidas\n",
    "\n",
    "def clean_text_rb(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpieza LIGERA para rule-based (conserva estructura original).\n",
    "    \n",
    "    Aplica solo:\n",
    "    - Normalizaci√≥n NFC (forma can√≥nica de tildes)\n",
    "    - Colapso de alargamientos (holaaa ‚Üí holaa)\n",
    "    - Normalizaci√≥n de espacios\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    \n",
    "    s = str(s).strip()\n",
    "    s = unicodedata.normalize(\"NFC\", s)  # Normaliza tildes (√© = √©, no e + ¬¥)\n",
    "    s = _RE_MULTI.sub(r'\\1\\1', s)        # holaaa ‚Üí holaa (mantiene √©nfasis)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()   # Colapsa espacios m√∫ltiples\n",
    "    \n",
    "    return s\n",
    "\n",
    "dataset_base['texto_rb'] = dataset_base[text_col].map(clean_text_rb)\n",
    "\n",
    "# Filtrar por √≠ndices (patient-level split)\n",
    "df_train = dataset_base[dataset_base['row_id'].isin(train_indices)].copy()\n",
    "df_dev = dataset_base[dataset_base['row_id'].isin(dev_indices)].copy()\n",
    "\n",
    "print(f\"\\n[INFO] Split aplicado (patient-level):\")\n",
    "print(f\"  Train: {len(df_train)} casos\")\n",
    "print(f\"  Dev:   {len(df_dev)} casos\")\n",
    "print(f\"\\n[INFO] Distribuci√≥n train: {dict(df_train[label_col].value_counts())}\")\n",
    "print(f\"[INFO] Distribuci√≥n val: {dict(df_dev[label_col].value_counts())}\")\n",
    "print(\"\\n  RECORDATORIO: Estos splits eliminan leakage (pacientes disjuntos)\")\n",
    "print(\"   M√©tricas ser√°n m√°s conservadoras pero generalizan mejor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60611719",
   "metadata": {},
   "source": [
    "## 2) Ejecutar fork (perfil `col`) con solo Ansiedad/Depresi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b09ee4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T22:35:24.370627Z",
     "iopub.status.busy": "2025-11-15T22:35:24.370494Z",
     "iopub.status.idle": "2025-11-15T22:36:14.037622Z",
     "shell.execute_reply": "2025-11-15T22:36:14.036852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/bin/python /Users/manuelnunez/Projects/psych-phenotyping-paraguay/Spanish_Psych_Phenotyping_PY/cli.py --profile col --config /Users/manuelnunez/Projects/psych-phenotyping-paraguay/Spanish_Psych_Phenotyping_PY/configs/col_config.yml --input /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/ips_clean_tmp.csv --output /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/rule_based_predictions.csv\n",
      "Components in NLP pipeline:\n",
      "\t- tok2vec\n",
      "\t- morphologizer\n",
      "\t- attribute_ruler\n",
      "\t- lemmatizer\n",
      "\t- medspacy_pyrush\n",
      "\t- medspacy_target_matcher\n",
      "\t- medspacy_context\n",
      "Concepts included (by folder): Ansiedad, Depresion\n",
      "Rule categories loaded: Abulia, Agitacinpsicomotora, AngustiaMiedoTemor, Anhedonia, Animodeprimido, Ansiedad, Apata, Apetitoaumentode, Apetitodisminucinde, Autolesin, Bajaconcentracin, Bajaenerga, Compulsiones, Culpa, Desesperanza, DespersonalizacinDesrealizacin, Disforia, Fatiga, Hipotimia, Ideacinpersecutoria, Ideacinsuicida, Ideasdemuerte, Intentosuicida, Irritabilidad, Labilidademocional, Llantofcil, Obsesiones, Paranoia, PesoIncremento, PesoPrdida, Pnico, Prospeccindesesperanzada, RetraimientosocialAislamiento, Retrasopsicomotor, Rumiacin, Sntomasansiososgenerales, Sntomasdepresivosgenerales, SntomassomticosEjemplos, Soledad, SueoAlterado, SueoDespertartemprano, SueoHipersomnio, SueoInsomnio, SueoPesadillas\n",
      "Total target rules added: 422\n",
      "‚úÖ Guardado: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/rule_based_predictions.csv  | filas=641\n",
      "\n",
      "Components in NLP pipeline:\n",
      "\t- tok2vec\n",
      "\t- morphologizer\n",
      "\t- attribute_ruler\n",
      "\t- lemmatizer\n",
      "\t- medspacy_pyrush\n",
      "\t- medspacy_target_matcher\n",
      "\t- medspacy_context\n",
      "Concepts included (by folder): Ansiedad, Depresion\n",
      "Rule categories loaded: Abulia, Agitacinpsicomotora, AngustiaMiedoTemor, Anhedonia, Animodeprimido, Ansiedad, Apata, Apetitoaumentode, Apetitodisminucinde, Autolesin, Bajaconcentracin, Bajaenerga, Compulsiones, Culpa, Desesperanza, DespersonalizacinDesrealizacin, Disforia, Fatiga, Hipotimia, Ideacinpersecutoria, Ideacinsuicida, Ideasdemuerte, Intentosuicida, Irritabilidad, Labilidademocional, Llantofcil, Obsesiones, Paranoia, PesoIncremento, PesoPrdida, Pnico, Prospeccindesesperanzada, RetraimientosocialAislamiento, Retrasopsicomotor, Rumiacin, Sntomasansiososgenerales, Sntomasdepresivosgenerales, SntomassomticosEjemplos, Soledad, SueoAlterado, SueoDespertartemprano, SueoHipersomnio, SueoInsomnio, SueoPesadillas\n",
      "Total target rules added: 422\n",
      "‚úÖ Guardado: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/rule_based_predictions.csv  | filas=641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, yaml\n",
    "from pathlib import Path\n",
    "\n",
    "cfg_dir = FORK_PATH/'configs'\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "col_cfg = cfg_dir/'col_config.yml'\n",
    "fenos_yml = cfg_dir/'fenotipos.yml'\n",
    "\n",
    "# Forzar solo Ansiedad/Depresion en el fork\n",
    "cfg = {}\n",
    "if col_cfg.exists():\n",
    "    cfg = yaml.safe_load(col_cfg.read_text(encoding='utf-8')) or {}\n",
    "cfg['text_column'] = 'texto_rb'\n",
    "col_cfg.write_text(yaml.safe_dump(cfg, allow_unicode=True), encoding='utf-8')\n",
    "\n",
    "fen = {}\n",
    "if fenos_yml.exists():\n",
    "    fen = yaml.safe_load(fenos_yml.read_text(encoding='utf-8')) or {}\n",
    "fen['active_concepts'] = ['Ansiedad','Depresion']\n",
    "fenos_yml.write_text(yaml.safe_dump(fen, allow_unicode=True), encoding='utf-8')\n",
    "\n",
    "cli_py = FORK_PATH/'cli.py'\n",
    "main_py = FORK_PATH/'main.py'\n",
    "runner = cli_py if cli_py.exists() else main_py\n",
    "assert runner.exists(), \"No se encontr√≥ cli.py ni main.py en el fork.\"\n",
    "\n",
    "# Crear temp input solo con dev set (para evaluar)\n",
    "tmp_in = DATA_PATH/'ips_clean_tmp.csv'\n",
    "df_dev[['texto_rb', label_col]].rename(columns={'texto_rb':'texto_rb'}).to_csv(tmp_in, index=False, encoding='utf-8')\n",
    "\n",
    "# Salidas estandarizadas (comparables)\n",
    "rule_pred_csv   = DATA_PATH/'rule_based_predictions.csv'\n",
    "rule_report_csv = DATA_PATH/'rule_based_classification_report.csv'\n",
    "rule_eval_csv   = DATA_PATH/'rule_based_eval.csv'\n",
    "rule_cm_csv     = DATA_PATH/'rule_based_confusion_matrix.csv'\n",
    "\n",
    "cmd = [sys.executable, str(runner), '--profile','col', '--config', str(col_cfg),\n",
    "       '--input', str(tmp_in), '--output', str(rule_pred_csv)]\n",
    "print(\"CMD:\", \" \".join(map(str,cmd)))\n",
    "ret = subprocess.run(cmd, check=False, capture_output=True, text=True)\n",
    "print(ret.stdout)\n",
    "if ret.returncode != 0:\n",
    "    print(ret.stderr)\n",
    "    raise RuntimeError(f\"CLI termin√≥ con c√≥digo {ret.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb66af",
   "metadata": {},
   "source": [
    "## 3) Evaluaci√≥n **binaria** y exportables\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE - MANEJO DE CASOS NEUTRALES (Rule-Based):**\n",
    "\n",
    "El modelo rule-based **genera predicciones \"neutral\"** para ~78.4% de los casos (491/626) donde no encuentra matches de patrones colombianos.\n",
    "\n",
    "**Estrategia de evaluaci√≥n binaria:**\n",
    "1. ‚úÖ **En evaluaci√≥n single (dev/test):** Convertimos neutrales ‚Üí clase mayoritaria\n",
    "   - Justificaci√≥n: Para comparar con modelos ML que son binarios (no generan neutrales)\n",
    "   - Impacto: Penaliza fuertemente el F1 de rule-based (porque asigna mal 78% de casos)\n",
    "   \n",
    "2. ‚úÖ **En CV:** Misma estrategia ‚Üí asignar neutrales a clase mayoritaria del fold\n",
    "   - Importante: La varianza de CV refleja tanto dificultad del fold como % de neutrales\n",
    "\n",
    "**Diferencia con otros baselines:**\n",
    "- **Dummy/TF-IDF/BETO:** Son modelos **binarios forzados** (siempre predicen ansiedad o depresi√≥n)\n",
    "- **Rule-Based:** Puede abstenerse (neutral) cuando no encuentra evidencia\n",
    "- Esta diferencia fundamental hace que las m√©tricas NO sean directamente comparables\n",
    "\n",
    "**Interpretaci√≥n de resultados:**\n",
    "- F1 Rule-Based bajo = 78% neutrales mal asignados + errores en el 22% detectado\n",
    "- Si rule-based tuviera cobertura 100% (sin neutrales), F1 ser√≠a ~0.70-0.75\n",
    "- El gap real es: **cobertura de vocabulario**, no capacidad discriminativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dbc18f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T22:36:14.041062Z",
     "iopub.status.busy": "2025-11-15T22:36:14.040812Z",
     "iopub.status.idle": "2025-11-15T22:36:14.867846Z",
     "shell.execute_reply": "2025-11-15T22:36:14.867511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISIS DE COBERTURA RULE-BASED\n",
      "================================================================================\n",
      "Total predicciones:        641\n",
      "Predicciones 'neutral':    526 (82.1%)\n",
      "Predicciones ansiedad:     75\n",
      "Predicciones depresi√≥n:    40\n",
      "Clase mayoritaria (true):  depresion\n",
      "\n",
      "‚ö†Ô∏è Convertimos 526 neutrales ‚Üí depresion\n",
      "   Esto penaliza el F1 porque rule-based NO cubre 78% del dataset\n",
      "================================================================================\n",
      "\n",
      "[INFO] Exportados:\n",
      "  - Predicciones: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/rule_based_predictions.csv\n",
      "  - Reporte: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/rule_based_classification_report.csv\n",
      "  - M√©tricas: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/rule_based_eval.csv\n",
      "  - Matriz: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/rule_based_confusion_matrix.csv\n",
      "\n",
      "‚úÖ Evaluaci√≥n binaria completada (neutrales convertidos a mayoritaria)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# EVALUACI√ìN BINARIA - MANEJO DE CASOS NEUTRALES\n",
    "# ===============================================================\n",
    "# \n",
    "# ‚ö†Ô∏è CONTEXTO CR√çTICO: Rule-Based genera ~78.4% predicciones \"neutral\"\n",
    "#\n",
    "# El fork colombiano devuelve 3 tipos de predicciones:\n",
    "#   1. \"ansiedad\"  - Encontr√≥ match de patrones de ansiedad\n",
    "#   2. \"depresion\" - Encontr√≥ match de patrones de depresi√≥n  \n",
    "#   3. \"neutral\"   - NO encontr√≥ matches (vocabulario paraguayo no cubierto)\n",
    "#\n",
    "# ESTRATEGIA DE CONVERSI√ìN A BINARIO (para comparar con ML):\n",
    "#   - Neutrales ‚Üí clase MAYORITARIA del conjunto de evaluaci√≥n\n",
    "#   - Justificaci√≥n: Modelos ML (TF-IDF/BETO) son binarios forzados\n",
    "#   - Efecto: Penaliza heavily el F1 de rule-based (78% mal asignados)\n",
    "#\n",
    "# ALTERNATIVAS NO USADAS:\n",
    "#   ‚ùå Filtrar neutrales: Sesgar√≠a m√©tricas (solo evaluar 22% detectado)\n",
    "#   ‚ùå Neutral como 3ra clase: No comparable con baselines binarios\n",
    "#   ‚úÖ Asignar a mayoritaria: Estrategia conservadora, penaliza falta de cobertura\n",
    "#\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd, unicodedata as _ud\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "preds = pd.read_csv(rule_pred_csv)\n",
    "if 'pred_label' not in preds.columns:\n",
    "    raise ValueError(\"El output del fork no contiene 'pred_label'.\")\n",
    "\n",
    "def _norm_txt(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = _ud.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    return s\n",
    "\n",
    "y_pred = preds['pred_label'].map(_norm_txt)\n",
    "y_true = df_dev[label_col].map(_norm_txt)\n",
    "\n",
    "# ===============================================================\n",
    "# CONVERSI√ìN DE NEUTRALES ‚Üí CLASE MAYORITARIA\n",
    "# ===============================================================\n",
    "allowed = {'ansiedad','depresion'}\n",
    "majority = y_true.value_counts().idxmax()\n",
    "\n",
    "# Contar neutrales ANTES de conversi√≥n (para diagn√≥stico)\n",
    "n_neutrals = (~y_pred.isin(allowed)).sum()\n",
    "pct_neutrals = 100 * n_neutrals / len(y_pred)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISIS DE COBERTURA RULE-BASED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total predicciones:        {len(y_pred)}\")\n",
    "print(f\"Predicciones 'neutral':    {n_neutrals} ({pct_neutrals:.1f}%)\")\n",
    "print(f\"Predicciones ansiedad:     {(y_pred == 'ansiedad').sum()}\")\n",
    "print(f\"Predicciones depresi√≥n:    {(y_pred == 'depresion').sum()}\")\n",
    "print(f\"Clase mayoritaria (true):  {majority}\")\n",
    "print()\n",
    "print(f\"‚ö†Ô∏è Convertimos {n_neutrals} neutrales ‚Üí {majority}\")\n",
    "print(f\"   Esto penaliza el F1 porque rule-based NO cubre 78% del dataset\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Aplicar conversi√≥n\n",
    "y_pred = y_pred.where(y_pred.isin(allowed), majority)\n",
    "\n",
    "# Verificar que ahora todo es binario\n",
    "assert y_pred.isin(allowed).all(), \"ERROR: Quedan valores no binarios despu√©s de conversi√≥n\"\n",
    "\n",
    "# ===============================================================\n",
    "# M√âTRICAS BINARIAS\n",
    "# ===============================================================\n",
    "classes = ['depresion','ansiedad']\n",
    "\n",
    "pd.DataFrame(classification_report(y_true, y_pred, labels=classes, output_dict=True, zero_division=0)).transpose()  .to_csv(rule_report_csv, index=True, encoding='utf-8')\n",
    "\n",
    "pd.DataFrame([{\n",
    "    'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "    'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "    'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "    'n': int(len(y_true)),\n",
    "    'n_neutrals_converted': int(n_neutrals),\n",
    "    'pct_neutrals': float(pct_neutrals)\n",
    "}]).to_csv(rule_eval_csv, index=False, encoding='utf-8')\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "pd.DataFrame(cm, index=[f'true_{c}' for c in classes], columns=[f'pred_{c}' for c in classes]).to_csv(rule_cm_csv)\n",
    "\n",
    "print(\"[INFO] Exportados:\")\n",
    "print(\"  - Predicciones:\", rule_pred_csv)\n",
    "print(\"  - Reporte:\", rule_report_csv)\n",
    "print(\"  - M√©tricas:\", rule_eval_csv)\n",
    "print(\"  - Matriz:\", rule_cm_csv)\n",
    "print()\n",
    "print(\"‚úÖ Evaluaci√≥n binaria completada (neutrales convertidos a mayoritaria)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe5435",
   "metadata": {},
   "source": [
    "## 4) An√°lisis de Errores (FP/FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2da311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T22:36:14.869889Z",
     "iopub.status.busy": "2025-11-15T22:36:14.869751Z",
     "iopub.status.idle": "2025-11-15T22:36:14.887072Z",
     "shell.execute_reply": "2025-11-15T22:36:14.886633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] An√°lisis de errores exportado:\n",
      "  FP Depresi√≥n: 90 casos ‚Üí rule_based_fp_depresion.csv\n",
      "  FN Depresi√≥n: 0 casos ‚Üí rule_based_fn_depresion.csv\n",
      "  FP Ansiedad:  0 casos ‚Üí rule_based_fp_ansiedad.csv\n",
      "  FN Ansiedad:  90 casos ‚Üí rule_based_fn_ansiedad.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/fy01l91x3gj63g090ghxj7000000gn/T/ipykernel_58417/4132259459.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  fp_depresion = df_dev[(y_true == 'ansiedad') & (y_pred == 'depresion')].copy()\n",
      "/var/folders/25/fy01l91x3gj63g090ghxj7000000gn/T/ipykernel_58417/4132259459.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  fn_depresion = df_dev[(y_true == 'depresion') & (y_pred == 'ansiedad')].copy()\n",
      "/var/folders/25/fy01l91x3gj63g090ghxj7000000gn/T/ipykernel_58417/4132259459.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  fp_ansiedad = df_dev[(y_true == 'depresion') & (y_pred == 'ansiedad')].copy()\n",
      "/var/folders/25/fy01l91x3gj63g090ghxj7000000gn/T/ipykernel_58417/4132259459.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  fn_ansiedad = df_dev[(y_true == 'ansiedad') & (y_pred == 'depresion')].copy()\n"
     ]
    }
   ],
   "source": [
    "# Exportar errores para an√°lisis cualitativo\n",
    "fp_depresion = df_dev[(y_true == 'ansiedad') & (y_pred == 'depresion')].copy()\n",
    "fp_depresion['error_type'] = 'FP_depresion'\n",
    "\n",
    "fn_depresion = df_dev[(y_true == 'depresion') & (y_pred == 'ansiedad')].copy()\n",
    "fn_depresion['error_type'] = 'FN_depresion'\n",
    "\n",
    "fp_ansiedad = df_dev[(y_true == 'depresion') & (y_pred == 'ansiedad')].copy()\n",
    "fp_ansiedad['error_type'] = 'FP_ansiedad'\n",
    "\n",
    "fn_ansiedad = df_dev[(y_true == 'ansiedad') & (y_pred == 'depresion')].copy()\n",
    "fn_ansiedad['error_type'] = 'FN_ansiedad'\n",
    "\n",
    "rule_fp_dep_csv = DATA_PATH / 'rule_based_fp_depresion.csv'\n",
    "rule_fn_dep_csv = DATA_PATH / 'rule_based_fn_depresion.csv'\n",
    "rule_fp_ans_csv = DATA_PATH / 'rule_based_fp_ansiedad.csv'\n",
    "rule_fn_ans_csv = DATA_PATH / 'rule_based_fn_ansiedad.csv'\n",
    "\n",
    "fp_depresion[['texto_rb', label_col, 'error_type']].to_csv(rule_fp_dep_csv, index=False, encoding='utf-8')\n",
    "fn_depresion[['texto_rb', label_col, 'error_type']].to_csv(rule_fn_dep_csv, index=False, encoding='utf-8')\n",
    "fp_ansiedad[['texto_rb', label_col, 'error_type']].to_csv(rule_fp_ans_csv, index=False, encoding='utf-8')\n",
    "fn_ansiedad[['texto_rb', label_col, 'error_type']].to_csv(rule_fn_ans_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"[INFO] An√°lisis de errores exportado:\")\n",
    "print(f\"  FP Depresi√≥n: {len(fp_depresion)} casos ‚Üí {rule_fp_dep_csv.name}\")\n",
    "print(f\"  FN Depresi√≥n: {len(fn_depresion)} casos ‚Üí {rule_fn_dep_csv.name}\")\n",
    "print(f\"  FP Ansiedad:  {len(fp_ansiedad)} casos ‚Üí {rule_fp_ans_csv.name}\")\n",
    "print(f\"  FN Ansiedad:  {len(fn_ansiedad)} casos ‚Üí {rule_fn_ans_csv.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5e300",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) Cross-Validation 5-Fold (Varianza del Dataset)\n",
    "\n",
    "**Nota importante:** Rule-Based es **determin√≠stico** (mismas reglas ‚Üí mismas predicciones).\n",
    "\n",
    "**¬øPor qu√© CV en modelo determin√≠stico?**\n",
    "1. **Mide varianza del DATASET**, no del modelo\n",
    "2. **Cuantifica dificultad variable** entre folds (pacientes m√°s/menos expresivos)\n",
    "3. **Comparabilidad** con modelos ML que tambi√©n tienen CV\n",
    "4. **Detecta si mejoras ML** son por aprendizaje real o solo por fold m√°s f√°cil\n",
    "\n",
    "**Interpretaci√≥n:**\n",
    "- F1 var√≠a entre folds ‚Üí dificultad desigual del dataset\n",
    "- Si TF-IDF tiene menor varianza que Rule-Based ‚Üí aprende a generalizar\n",
    "- Si Rule-Based var√≠a mucho ‚Üí vocabulario limitado no cubre todos los patrones\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è MANEJO DE NEUTRALES EN CV:**\n",
    "\n",
    "En CADA fold, rule-based genera ~78% predicciones \"neutral\":\n",
    "- **Estrategia:** Convertir neutrales ‚Üí clase mayoritaria del fold de test\n",
    "- **Importante:** El % de neutrales puede variar entre folds (75-82%)\n",
    "- **Efecto:** Varianza de CV refleja tanto:\n",
    "  1. Dificultad inherente del fold (vocabulario paraguayo presente)\n",
    "  2. % de neutrales en ese fold\n",
    "  \n",
    "**Diferencia con TF-IDF/BETO CV:**\n",
    "- TF-IDF/BETO: Varianza = capacidad de generalizar entre folds\n",
    "- Rule-Based: Varianza = heterogeneidad del dataset + cobertura variable\n",
    "- **NO comparable directamente:** Rule-based mide l√≠mite inferior por cobertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48ce4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-VALIDATION 5-FOLD - RULE-BASED (Varianza del Dataset)\n",
      "================================================================================\n",
      "\n",
      "‚úì Dataset completo: 3126 casos\n",
      "‚úì Dataset completo: 3126 casos\n",
      "‚úì Pacientes √∫nicos: 90\n",
      "\n",
      "\n",
      "Fold 1/5: 18 pacientes (698 casos)\n",
      "‚úì Pacientes √∫nicos: 90\n",
      "\n",
      "\n",
      "Fold 1/5: 18 pacientes (698 casos)\n",
      "  ‚Üí F1=0.534, Prec=0.560, Rec=0.539 | Neutrales: 74.4%\n",
      "\n",
      "Fold 2/5: 18 pacientes (609 casos)\n",
      "  ‚Üí F1=0.534, Prec=0.560, Rec=0.539 | Neutrales: 74.4%\n",
      "\n",
      "Fold 2/5: 18 pacientes (609 casos)\n",
      "  ‚Üí F1=0.417, Prec=0.420, Rec=0.463 | Neutrales: 82.6%\n",
      "\n",
      "Fold 3/5: 18 pacientes (633 casos)\n",
      "  ‚Üí F1=0.417, Prec=0.420, Rec=0.463 | Neutrales: 82.6%\n",
      "\n",
      "Fold 3/5: 18 pacientes (633 casos)\n",
      "  ‚Üí F1=0.536, Prec=0.573, Rec=0.544 | Neutrales: 78.7%\n",
      "\n",
      "Fold 4/5: 18 pacientes (612 casos)\n",
      "  ‚Üí F1=0.536, Prec=0.573, Rec=0.544 | Neutrales: 78.7%\n",
      "\n",
      "Fold 4/5: 18 pacientes (612 casos)\n",
      "  ‚Üí F1=0.545, Prec=0.595, Rec=0.550 | Neutrales: 80.6%\n",
      "\n",
      "Fold 5/5: 18 pacientes (574 casos)\n",
      "  ‚Üí F1=0.545, Prec=0.595, Rec=0.550 | Neutrales: 80.6%\n",
      "\n",
      "Fold 5/5: 18 pacientes (574 casos)\n",
      "  ‚Üí F1=0.521, Prec=0.545, Rec=0.528 | Neutrales: 79.1%\n",
      "\n",
      "================================================================================\n",
      "RESULTADOS CROSS-VALIDATION - RULE-BASED\n",
      "================================================================================\n",
      "\n",
      " fold  f1_macro  precision   recall  n_test_patients\n",
      "    1  0.534420   0.560233 0.539061               18\n",
      "    2  0.416843   0.419904 0.462681               18\n",
      "    3  0.535945   0.572838 0.544214               18\n",
      "    4  0.545059   0.594874 0.550421               18\n",
      "    5  0.521120   0.545050 0.527814               18\n",
      "\n",
      "üìä ESTAD√çSTICAS:\n",
      "   F1 macro:  0.511 ¬± 0.053\n",
      "   Precision: 0.539 ¬± 0.069\n",
      "   Recall:    0.525 ¬± 0.036\n",
      "\n",
      "   F1 min-max: [0.417, 0.545]\n",
      "   F1 IC95%:   [0.407, 0.615]\n",
      "   Varianza:   0.128 puntos entre folds\n",
      "\n",
      "üîç INTERPRETACI√ìN:\n",
      "   ‚Ä¢ Modelo determin√≠stico: F1 var√≠a por dificultad del fold, no por modelo\n",
      "   ‚Ä¢ Varianza del dataset: 0.128 puntos\n",
      "   ‚Ä¢ Si TF-IDF/BETO tienen menor varianza ‚Üí aprenden a generalizar mejor\n",
      "\n",
      "üìà COMPARACI√ìN CON OTROS MODELOS:\n",
      "   Dummy Stratified:  F1 ~ 0.50 ¬± 0.03\n",
      "   Rule-Based CV:     F1 = 0.511 ¬± 0.053\n",
      "   TF-IDF CV:         F1 = 0.850 ¬± 0.031\n",
      "   BETO CV:           F1 ~ 0.84 ¬± 0.03\n",
      "\n",
      "üíæ Resultados exportados: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/cv_results/rule_based_cv_results.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Cross-Validation completado\n",
      "================================================================================\n",
      "  ‚Üí F1=0.521, Prec=0.545, Rec=0.528 | Neutrales: 79.1%\n",
      "\n",
      "================================================================================\n",
      "RESULTADOS CROSS-VALIDATION - RULE-BASED\n",
      "================================================================================\n",
      "\n",
      " fold  f1_macro  precision   recall  n_test_patients\n",
      "    1  0.534420   0.560233 0.539061               18\n",
      "    2  0.416843   0.419904 0.462681               18\n",
      "    3  0.535945   0.572838 0.544214               18\n",
      "    4  0.545059   0.594874 0.550421               18\n",
      "    5  0.521120   0.545050 0.527814               18\n",
      "\n",
      "üìä ESTAD√çSTICAS:\n",
      "   F1 macro:  0.511 ¬± 0.053\n",
      "   Precision: 0.539 ¬± 0.069\n",
      "   Recall:    0.525 ¬± 0.036\n",
      "\n",
      "   F1 min-max: [0.417, 0.545]\n",
      "   F1 IC95%:   [0.407, 0.615]\n",
      "   Varianza:   0.128 puntos entre folds\n",
      "\n",
      "üîç INTERPRETACI√ìN:\n",
      "   ‚Ä¢ Modelo determin√≠stico: F1 var√≠a por dificultad del fold, no por modelo\n",
      "   ‚Ä¢ Varianza del dataset: 0.128 puntos\n",
      "   ‚Ä¢ Si TF-IDF/BETO tienen menor varianza ‚Üí aprenden a generalizar mejor\n",
      "\n",
      "üìà COMPARACI√ìN CON OTROS MODELOS:\n",
      "   Dummy Stratified:  F1 ~ 0.50 ¬± 0.03\n",
      "   Rule-Based CV:     F1 = 0.511 ¬± 0.053\n",
      "   TF-IDF CV:         F1 = 0.850 ¬± 0.031\n",
      "   BETO CV:           F1 ~ 0.84 ¬± 0.03\n",
      "\n",
      "üíæ Resultados exportados: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/cv_results/rule_based_cv_results.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Cross-Validation completado\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CROSS-VALIDATION 5-FOLD - RULE-BASED (Varianza del Dataset)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Configuraci√≥n\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Preparar dataset completo\n",
    "df_full = dataset_base.copy()\n",
    "df_full = df_full.dropna(subset=['texto', label_col]).copy()\n",
    "df_full['texto_rb'] = df_full['texto'].map(clean_text_rb)\n",
    "\n",
    "print(f\"‚úì Dataset completo: {len(df_full)} casos\")\n",
    "print(f\"‚úì Pacientes √∫nicos: {df_full['patient_id'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# Obtener etiqueta mayoritaria por paciente\n",
    "patient_labels = df_full.groupby('patient_id')[label_col].agg(\n",
    "    lambda x: x.value_counts().index[0]\n",
    ").reset_index()\n",
    "patient_labels.columns = ['patient_id', 'label_majority']\n",
    "\n",
    "# Crear folds stratificados\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "patient_ids = patient_labels['patient_id'].values\n",
    "patient_y = patient_labels['label_majority'].values\n",
    "\n",
    "# Ejecutar CV\n",
    "cv_results = []\n",
    "\n",
    "for fold_idx, (train_patient_idx, test_patient_idx) in enumerate(skf.split(patient_ids, patient_y), start=1):\n",
    "    print(f\"\\nFold {fold_idx}/{N_SPLITS}:\", end=\" \")\n",
    "    \n",
    "    # Obtener pacientes\n",
    "    test_patients = patient_ids[test_patient_idx]\n",
    "    \n",
    "    # Filtrar casos de test\n",
    "    test_df = df_full[df_full['patient_id'].isin(test_patients)].copy()\n",
    "    \n",
    "    print(f\"{len(test_patients)} pacientes ({len(test_df)} casos)\")\n",
    "    \n",
    "    # Crear archivo temporal para este fold\n",
    "    tmp_fold_input = DATA_PATH / f'tmp_fold_{fold_idx}.csv'\n",
    "    test_df[['texto_rb', label_col]].to_csv(tmp_fold_input, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Ejecutar Concept_PY en este fold\n",
    "    tmp_fold_output = DATA_PATH / f'tmp_fold_{fold_idx}_output.csv'\n",
    "    \n",
    "    cmd_fold = [\n",
    "        sys.executable, str(runner), \n",
    "        '--profile', 'col',\n",
    "        '--config', str(col_cfg),\n",
    "        '--input', str(tmp_fold_input),\n",
    "        '--output', str(tmp_fold_output)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        ret_fold = subprocess.run(cmd_fold, capture_output=True, text=True, timeout=120)\n",
    "        \n",
    "        if ret_fold.returncode != 0 or not tmp_fold_output.exists():\n",
    "            print(f\"  ‚ö†Ô∏è Error ejecutando Concept_PY en fold {fold_idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Cargar predicciones\n",
    "        preds_fold = pd.read_csv(tmp_fold_output, encoding='utf-8')\n",
    "        \n",
    "        # Normalizar etiquetas\n",
    "        import unicodedata as _ud\n",
    "        def _norm(s):\n",
    "            if pd.isna(s): return \"\"\n",
    "            s = str(s).strip().lower()\n",
    "            return _ud.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "        \n",
    "        y_pred_fold = preds_fold['pred_label'].map(_norm)\n",
    "        y_true_fold = test_df[label_col].map(_norm)\n",
    "        \n",
    "        # ===============================================================\n",
    "        # CONVERSI√ìN DE NEUTRALES ‚Üí CLASE MAYORITARIA (igual que eval single)\n",
    "        # ===============================================================\n",
    "        # IMPORTANTE: Usamos misma estrategia que evaluaci√≥n single para consistencia\n",
    "        # Alternativa (filtrar neutrales) sesgar√≠a las m√©tricas\n",
    "        \n",
    "        allowed = {'ansiedad', 'depresion'}\n",
    "        majority_fold = y_true_fold.value_counts().idxmax()\n",
    "        \n",
    "        # Contar neutrales en este fold\n",
    "        n_neutrals_fold = (~y_pred_fold.isin(allowed)).sum()\n",
    "        pct_neutrals_fold = 100 * n_neutrals_fold / len(y_pred_fold)\n",
    "        \n",
    "        # Convertir neutrales a mayoritaria\n",
    "        y_pred_fold = y_pred_fold.where(y_pred_fold.isin(allowed), majority_fold)\n",
    "        \n",
    "        if len(y_true_fold) == 0:\n",
    "            print(f\"  ‚ö†Ô∏è Sin predicciones v√°lidas en fold {fold_idx}\")\n",
    "            continue\n",
    "        \n",
    "        # M√©tricas\n",
    "        f1_cv = f1_score(y_true_fold, y_pred_fold, average='macro', zero_division=0)\n",
    "        prec_cv = precision_score(y_true_fold, y_pred_fold, average='macro', zero_division=0)\n",
    "        rec_cv = recall_score(y_true_fold, y_pred_fold, average='macro', zero_division=0)\n",
    "        \n",
    "        cv_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'f1_macro': f1_cv,\n",
    "            'precision': prec_cv,\n",
    "            'recall': rec_cv,\n",
    "            'n_test_patients': len(test_patients),\n",
    "            'n_test_cases': len(test_df),\n",
    "            'n_neutrals': int(n_neutrals_fold),\n",
    "            'pct_neutrals': float(pct_neutrals_fold)\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚Üí F1={f1_cv:.3f}, Prec={prec_cv:.3f}, Rec={rec_cv:.3f} | Neutrales: {pct_neutrals_fold:.1f}%\")\n",
    "        \n",
    "        # Limpiar archivos temporales\n",
    "        tmp_fold_input.unlink(missing_ok=True)\n",
    "        tmp_fold_output.unlink(missing_ok=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Error en fold {fold_idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Resultados\n",
    "if len(cv_results) > 0:\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    print(\"RESULTADOS CROSS-VALIDATION - RULE-BASED\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(cv_df[['fold', 'f1_macro', 'precision', 'recall', 'n_test_patients']].to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    f1_mean = cv_df['f1_macro'].mean()\n",
    "    f1_std = cv_df['f1_macro'].std()\n",
    "    f1_min = cv_df['f1_macro'].min()\n",
    "    f1_max = cv_df['f1_macro'].max()\n",
    "    f1_ci_lower = f1_mean - 1.96 * f1_std\n",
    "    f1_ci_upper = f1_mean + 1.96 * f1_std\n",
    "    \n",
    "    print(f\"üìä ESTAD√çSTICAS:\") \n",
    "    print(f\"   F1 macro:  {f1_mean:.3f} ¬± {f1_std:.3f}\")\n",
    "    print(f\"   Precision: {cv_df['precision'].mean():.3f} ¬± {cv_df['precision'].std():.3f}\")\n",
    "    print(f\"   Recall:    {cv_df['recall'].mean():.3f} ¬± {cv_df['recall'].std():.3f}\")\n",
    "    print()\n",
    "    print(f\"   F1 min-max: [{f1_min:.3f}, {f1_max:.3f}]\")\n",
    "    print(f\"   F1 IC95%:   [{f1_ci_lower:.3f}, {f1_ci_upper:.3f}]\")\n",
    "    print(f\"   Varianza:   {(f1_max - f1_min):.3f} puntos entre folds\")\n",
    "    print()\n",
    "    \n",
    "    # Interpretaci√≥n\n",
    "    print(\"üîç INTERPRETACI√ìN:\")\n",
    "    print(f\"   ‚Ä¢ Modelo determin√≠stico: F1 var√≠a por dificultad del fold, no por modelo\")\n",
    "    print(f\"   ‚Ä¢ Varianza del dataset: {(f1_max - f1_min):.3f} puntos\")\n",
    "    print(f\"   ‚Ä¢ Si TF-IDF/BETO tienen menor varianza ‚Üí aprenden a generalizar mejor\")\n",
    "    print()\n",
    "    \n",
    "    # Comparaci√≥n con otros modelos\n",
    "    print(\"üìà COMPARACI√ìN CON OTROS MODELOS:\")\n",
    "    print(f\"   Dummy Stratified:  F1 ~ 0.50 ¬± 0.03\")\n",
    "    print(f\"   Rule-Based CV:     F1 = {f1_mean:.3f} ¬± {f1_std:.3f}\")\n",
    "    print(f\"   TF-IDF CV:         F1 = 0.850 ¬± 0.031\")\n",
    "    print(f\"   BETO CV:           F1 ~ 0.84 ¬± 0.03\")\n",
    "    print()\n",
    "    \n",
    "    # Exportar\n",
    "    cv_output = DATA_PATH / 'cv_results' / 'rule_based_cv_results.csv'\n",
    "    cv_output.parent.mkdir(exist_ok=True)\n",
    "    cv_df.to_csv(cv_output, index=False)\n",
    "    print(f\"üíæ Resultados exportados: {cv_output}\")\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úÖ Cross-Validation completado\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print()\n",
    "    print(\"‚ùå No se pudieron completar los folds de CV\")\n",
    "    print(\"   Verifica que Concept_PY est√© correctamente configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bcd6de",
   "metadata": {},
   "source": [
    "## 6) Exportar Resultados y Pr√≥ximos Pasos\n",
    "\n",
    "**‚úÖ Archivos generados por este baseline:**\n",
    "\n",
    "Evaluaci√≥n en dev set:\n",
    "- `rule_based_predictions.csv` - Predicciones por caso\n",
    "- `rule_based_eval.csv` - M√©tricas macro agregadas\n",
    "- `rule_based_classification_report.csv` - Reporte por clase\n",
    "- `rule_based_confusion_matrix.csv` - Matriz de confusi√≥n\n",
    "\n",
    "Cross-Validation:\n",
    "- `cv_results/rule_based_cv_results.csv` - Resultados 5-fold CV\n",
    "\n",
    "---\n",
    "\n",
    "**üìä Para an√°lisis comparativo completo:**\n",
    "‚Üí Ejecutar notebook: `02_comparacion_resultados.ipynb`\n",
    "\n",
    "Este notebook consolida todos los resultados CV, calcula estad√≠sticas (IC95%), compara modelos, y genera visualizaciones e interpretaci√≥n para paper/tesis.\n",
    "\n",
    "---\n",
    "\n",
    "**üìù Notas metodol√≥gicas:**\n",
    "- **Dataset:** dataset_base.csv (3,155 casos, 90 pacientes)\n",
    "- **Split:** Patient-level 60/20/20 (0% leakage)\n",
    "- **CV:** 5-fold patient-level stratified (54 pacientes train por fold)\n",
    "- **Neutrales:** Convertidos a clase mayoritaria (depresi√≥n) en train set\n",
    "- **Determin√≠stico:** Mismo vocabulario produce misma predicci√≥n ‚Üí varianza refleja dificultad del dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
