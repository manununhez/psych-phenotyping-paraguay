{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864cb57e",
   "metadata": {},
   "source": [
    "# 02_baseline_tfidf ‚Äî Binario A/D\n",
    "\n",
    "**Objetivo:** baseline cl√°sico **robusto a ruido** (typos/transcripci√≥n) con **char TF‚ÄëIDF (3‚Äì5)** + **SVM (LinearSVC)**.  \n",
    "**Justificaci√≥n:** los n‚Äëgramas de caracteres capturan patrones ortogr√°ficos aun con errores; es un buen contrapunto al enfoque rule‚Äëbased.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361947b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Utilizando utils_shared.py\n",
      "‚úÖ Paths configurados:\n",
      "   BASE_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay\n",
      "   DATA_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
      "   SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Paths, Imports, y Utilidades Compartidas\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, unicodedata, os\n",
    "\n",
    "# Intentar importar utilidades compartidas\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, str(Path.cwd()))\n",
    "    from utils_shared import setup_paths, guess_text_col, guess_label_col, normalize_label\n",
    "    print(\"‚úÖ Utilizando utils_shared.py\")\n",
    "    \n",
    "    # Setup de paths centralizado\n",
    "    paths = setup_paths()\n",
    "    BASE_PATH = paths['BASE_PATH']\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    \n",
    "    # Usar funciones centralizadas\n",
    "    _guess_text_col = guess_text_col\n",
    "    _guess_label_col = guess_label_col\n",
    "    _norm_label_bin = normalize_label\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è utils_shared.py no encontrado, usando funciones locales\")\n",
    "    \n",
    "    # Setup manual de paths\n",
    "    BASE_PATH = Path.cwd()\n",
    "    if BASE_PATH.name == \"notebooks\":\n",
    "        BASE_PATH = BASE_PATH.parent\n",
    "    \n",
    "    DATA_PATH = BASE_PATH / \"data\"\n",
    "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
    "    \n",
    "    DATA_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Funciones helper locales\n",
    "    def _guess_text_col(df):\n",
    "        for c in [\"texto\", \"text\", \"comment\", \"comentario\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[0]\n",
    "    \n",
    "    def _guess_label_col(df):\n",
    "        for c in [\"etiqueta\", \"label\", \"category\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[1] if len(df.columns) > 1 else df.columns[-1]\n",
    "    \n",
    "    def _norm_label_bin(s):\n",
    "        if pd.isna(s): \n",
    "            return \"\"\n",
    "        s = str(s).strip().lower()\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "        return {'depresivo': 'depresion'}.get(s, s)\n",
    "\n",
    "# Validar existencia de splits\n",
    "if not SPLITS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Splits no encontrados en {SPLITS_PATH}\\n\"\n",
    "        f\"   Debes ejecutar primero: 02_create_splits.ipynb\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Paths configurados:\")\n",
    "print(f\"   BASE_PATH:   {BASE_PATH}\")\n",
    "print(f\"   DATA_PATH:   {DATA_PATH}\")\n",
    "print(f\"   SPLITS_PATH: {SPLITS_PATH}\")\n",
    "\n",
    "# Columnas esperadas en dataset_base.csv\n",
    "TEXT_COL = \"texto\"\n",
    "LABEL_COL = \"etiqueta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39de3f3",
   "metadata": {},
   "source": [
    "## 1) Carga y preprocesamiento **agresivo** (pensado para ML cl√°sico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56fb4499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Splits cargados desde /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits/:\n",
      "   Train: 2500 ejemplos\n",
      "   Val:   626 ejemplos\n",
      "   Total: 3126 ejemplos en dataset_base.csv\n",
      "\n",
      "üìã Columnas detectadas: texto='texto', label='etiqueta'\n",
      "\n",
      "üßπ Aplicado preprocesamiento agresivo (lowercase + sin tildes + negaciones)\n",
      "   Ejemplo antes: Reposicion de medicacion 2) EXAMEN FISICO GRAL. Y GINECOLOGICO PESO ( ) TALLA ( ...\n",
      "   Ejemplo despu√©s: reposicion de medicacion 2 examen fisico gral. y ginecologico peso talla presion...\n",
      "\n",
      "üìä Splits creados:\n",
      "\n",
      "Train (2500 ejemplos):\n",
      "   depresion: 1760\n",
      "   ansiedad: 740\n",
      "\n",
      "Val (626 ejemplos):\n",
      "   depresion: 441\n",
      "   ansiedad: 185\n",
      "\n",
      "üßπ Aplicado preprocesamiento agresivo (lowercase + sin tildes + negaciones)\n",
      "   Ejemplo antes: Reposicion de medicacion 2) EXAMEN FISICO GRAL. Y GINECOLOGICO PESO ( ) TALLA ( ...\n",
      "   Ejemplo despu√©s: reposicion de medicacion 2 examen fisico gral. y ginecologico peso talla presion...\n",
      "\n",
      "üìä Splits creados:\n",
      "\n",
      "Train (2500 ejemplos):\n",
      "   depresion: 1760\n",
      "   ansiedad: 740\n",
      "\n",
      "Val (626 ejemplos):\n",
      "   depresion: 441\n",
      "   ansiedad: 185\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Carga de Datos y Preprocesamiento AGRESIVO (TF-IDF + SVM)\n",
    "# ===============================================================\n",
    "#\n",
    "# ESTRATEGIA DE PREPROCESAMIENTO: AGRESIVA (m√°xima normalizaci√≥n)\n",
    "#\n",
    "# ¬øPor qu√© preprocesamiento agresivo?\n",
    "#\n",
    "# 1. **Convierte a min√∫sculas**:\n",
    "#    - \"Depresi√≥n\" y \"depresi√≥n\" son la misma palabra para el modelo\n",
    "#    - TF-IDF trata cada variaci√≥n como token diferente sin lowercase\n",
    "#    - Reduce vocabulario (~40%) mejorando generalizaci√≥n\n",
    "#\n",
    "# 2. **Elimina acentos/tildes**:\n",
    "#    - \"depresi√≥n\" ‚Üí \"depresion\"\n",
    "#    - Robustez ante errores de transcripci√≥n (com√∫n en notas cl√≠nicas)\n",
    "#    - Los datos tienen muchos typos: \"anciedad\", \"deprecion\", etc.\n",
    "#\n",
    "# 3. **Elimina s√≠mbolos especiales**:\n",
    "#    - Preserva solo: letras, n√∫meros, espacios, puntuaci√≥n b√°sica\n",
    "#    - Elimina ruido de transcripci√≥n: emojis, s√≠mbolos raros, etc.\n",
    "#\n",
    "# 4. **Colapsa alargamientos**:\n",
    "#    - \"holaaaaa\" ‚Üí \"holaa\" (igual que rule-based)\n",
    "#    - Mantiene √©nfasis sin explotar el vocabulario\n",
    "#\n",
    "# 5. **Marca negaciones** (innovaci√≥n clave):\n",
    "#    - \"no tengo apetito\" ‚Üí \"no_tengo apetito\"\n",
    "#    - Permite a TF-IDF capturar \"no_X\" como feature diferente de \"X\"\n",
    "#    - Proxy simple para manejar negaci√≥n sin ConText\n",
    "#\n",
    "# ¬øQu√© NO hace este preprocesamiento?\n",
    "# - ‚ùå No hace stemming/lemmatizaci√≥n (puede romper char n-grams)\n",
    "# - ‚ùå No elimina stopwords (√∫tiles en contexto cl√≠nico)\n",
    "# - ‚ùå No tokeniza (char-level TF-IDF lo hace autom√°ticamente)\n",
    "#\n",
    "# Comparaci√≥n con otros baselines:\n",
    "# - Rule-Based: Conservador (preserva tildes/may√∫sculas para patterns)\n",
    "# - TF-IDF: Agresivo (m√°xima normalizaci√≥n para robustez)\n",
    "# - BETO: M√≠nimo (solo tokenizaci√≥n, el modelo maneja el resto)\n",
    "#\n",
    "# Justificaci√≥n del char TF-IDF (3-5):\n",
    "# - **Robusto a typos**: \"deprecion\" captura \"pre\", \"rec\", \"epr\", \"ecion\" ‚Üí overlap con \"depresion\"\n",
    "# - **No requiere tokenizaci√≥n perfecta**: funciona character-by-character\n",
    "# - **Captura patrones morfol√≥gicos**: sufijos -ci√≥n, -dad, -oso, etc.\n",
    "# - **Est√°ndar de la industria** para texto con ruido (OCR, transcripci√≥n)\n",
    "#\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd, re, unicodedata\n",
    "\n",
    "# Cargar splits unificados desde 02_create_splits.ipynb\n",
    "dataset_base = pd.read_csv(SPLITS_PATH / 'dataset_base.csv')\n",
    "train_indices = pd.read_csv(SPLITS_PATH / 'train_indices.csv')['row_id'].values\n",
    "val_indices = pd.read_csv(SPLITS_PATH / 'val_indices.csv')['row_id'].values\n",
    "\n",
    "print(f\"‚úÖ Splits cargados desde {SPLITS_PATH}/:\")\n",
    "print(f\"   Train: {len(train_indices)} ejemplos\")\n",
    "print(f\"   Val:   {len(val_indices)} ejemplos\")\n",
    "print(f\"   Total: {len(dataset_base)} ejemplos en dataset_base.csv\")\n",
    "\n",
    "# Detectar columnas autom√°ticamente\n",
    "text_col = _guess_text_col(dataset_base)\n",
    "label_col = _guess_label_col(dataset_base)\n",
    "print(f\"\\nüìã Columnas detectadas: texto='{text_col}', label='{label_col}'\")\n",
    "\n",
    "# Definir funci√≥n de limpieza agresiva\n",
    "RE_MULTI = re.compile(r'(.)\\1{2,}')  # Detecta 3+ letras repetidas\n",
    "\n",
    "def clean_text_ml(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpieza AGRESIVA para TF-IDF (m√°xima normalizaci√≥n para robustez).\n",
    "    \n",
    "    Aplica:\n",
    "    - Lowercase completo\n",
    "    - Normalizaci√≥n NFC + eliminaci√≥n de tildes\n",
    "    - Colapso de alargamientos\n",
    "    - Eliminaci√≥n de s√≠mbolos especiales (preserva puntuaci√≥n b√°sica)\n",
    "    - Marca negaciones simples (\"no X\" ‚Üí \"no_X\")\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    \n",
    "    s = str(s).lower().strip()           # Lowercase total\n",
    "    s = unicodedata.normalize(\"NFC\", s)  # Normaliza tildes\n",
    "    s = RE_MULTI.sub(r'\\1\\1', s)         # Colapsa alargamientos\n",
    "    \n",
    "    # Elimina s√≠mbolos especiales (preserva letras, n√∫meros, puntuaci√≥n b√°sica)\n",
    "    s = re.sub(r\"[^a-z0-9√°√©√≠√≥√∫√º√±\\s.,!?:/\\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()   # Normaliza espacios\n",
    "    \n",
    "    # Marca negaciones simples: \"no tengo\" ‚Üí \"no_tengo\"\n",
    "    s = re.sub(r\"\\bno\\s+([a-z√°√©√≠√≥√∫√º√±]{2,})\", r\"no_\\1\", s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Aplicar limpieza agresiva\n",
    "dataset_base['texto_ml'] = dataset_base[text_col].map(clean_text_ml)\n",
    "\n",
    "print(f\"\\nüßπ Aplicado preprocesamiento agresivo (lowercase + sin tildes + negaciones)\")\n",
    "print(f\"   Ejemplo antes: {dataset_base[text_col].iloc[0][:80]}...\")\n",
    "print(f\"   Ejemplo despu√©s: {dataset_base['texto_ml'].iloc[0][:80]}...\")\n",
    "\n",
    "# Separar train y val usando √≠ndices guardados\n",
    "df_train = dataset_base[dataset_base['row_id'].isin(train_indices)].copy()\n",
    "df_val = dataset_base[dataset_base['row_id'].isin(val_indices)].copy()\n",
    "\n",
    "X_train, y_train = df_train['texto_ml'], df_train[label_col]\n",
    "X_val, y_val = df_val['texto_ml'], df_val[label_col]\n",
    "\n",
    "print(f\"\\nüìä Splits creados:\")\n",
    "print(f\"\\nTrain ({len(df_train)} ejemplos):\")\n",
    "train_dist = y_train.value_counts()\n",
    "for label, count in train_dist.items():\n",
    "    print(f\"   {label}: {count}\")\n",
    "\n",
    "print(f\"\\nVal ({len(df_val)} ejemplos):\")\n",
    "val_dist = y_val.value_counts()\n",
    "for label, count in val_dist.items():\n",
    "    print(f\"   {label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b1573",
   "metadata": {},
   "source": [
    "## 2) Split estratificado y entrenamiento (char TF‚ÄëIDF + LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53508f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('tfidf', tfidf_char),\n",
    "    ('svm', LinearSVC(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"‚úÖ Entrenamiento completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff481a1c",
   "metadata": {},
   "source": [
    "## 3) M√©tricas y exportables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "921cc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exportados:\n",
      " - Predicciones: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_predictions.csv\n",
      " - Reporte: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_classification_report.csv\n",
      " - M√©tricas: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_eval.csv\n",
      " - Matriz: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_confusion_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "tfidf_pred_csv   = DATA_PATH/'tfidf_predictions.csv'\n",
    "tfidf_report_csv = DATA_PATH/'tfidf_classification_report.csv'\n",
    "tfidf_eval_csv   = DATA_PATH/'tfidf_eval.csv'\n",
    "tfidf_cm_csv     = DATA_PATH/'tfidf_confusion_matrix.csv'\n",
    "\n",
    "classes = ['depresion','ansiedad']\n",
    "\n",
    "pd.DataFrame(classification_report(y_val, y_pred, labels=classes, output_dict=True, zero_division=0))  .transpose().to_csv(tfidf_report_csv, index=True, encoding='utf-8')\n",
    "\n",
    "pd.DataFrame([{\n",
    "    'macro_f1': f1_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'macro_precision': precision_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'macro_recall': recall_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'n': int(len(y_val))\n",
    "}]).to_csv(tfidf_eval_csv, index=False, encoding='utf-8')\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred, labels=classes)\n",
    "pd.DataFrame(cm, index=[f'true_{c}' for c in classes], columns=[f'pred_{c}' for c in classes]).to_csv(tfidf_cm_csv)\n",
    "\n",
    "pd.DataFrame({'texto': X_val, 'y_true': y_val, 'y_pred': y_pred}).to_csv(tfidf_pred_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"‚úÖ Exportados:\")\n",
    "print(\" - Predicciones:\", tfidf_pred_csv)\n",
    "print(\" - Reporte:\", tfidf_report_csv)\n",
    "print(\" - M√©tricas:\", tfidf_eval_csv)\n",
    "print(\" - Matriz:\", tfidf_cm_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
