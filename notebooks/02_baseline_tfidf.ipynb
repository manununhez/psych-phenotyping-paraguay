{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864cb57e",
   "metadata": {},
   "source": [
    "# 02_baseline_tfidf — Binario A/D\n",
    "\n",
    "**Objetivo:** baseline clásico **robusto a ruido** (typos/transcripción) con **char TF‑IDF (3–5)** + **SVM (LinearSVC)**.  \n",
    "**Justificación:** los n‑gramas de caracteres capturan patrones ortográficos aun con errores; es un buen contrapunto al enfoque rule‑based.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "361947b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Utilizando utils_shared.py\n",
      "Paths configurados:\n",
      "  BASE_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay\n",
      "  DATA_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
      "  SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Paths, Imports, y Utilidades Compartidas\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, unicodedata, os\n",
    "\n",
    "# Intentar importar utilidades compartidas\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, str(Path.cwd()))\n",
    "    from utils_shared import setup_paths, guess_text_col, guess_label_col, normalize_label\n",
    "    print(\"[INFO] Utilizando utils_shared.py\")\n",
    "    \n",
    "    # Setup de paths centralizado\n",
    "    paths = setup_paths()\n",
    "    BASE_PATH = paths['BASE_PATH']\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    \n",
    "    # Usar funciones centralizadas\n",
    "    _guess_text_col = guess_text_col\n",
    "    _guess_label_col = guess_label_col\n",
    "    _norm_label_bin = normalize_label\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"[WARNING] utils_shared.py no encontrado, usando funciones locales\")\n",
    "    \n",
    "    # Setup manual de paths\n",
    "    BASE_PATH = Path.cwd()\n",
    "    if BASE_PATH.name == \"notebooks\":\n",
    "        BASE_PATH = BASE_PATH.parent\n",
    "    \n",
    "    DATA_PATH = BASE_PATH / \"data\"\n",
    "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
    "    \n",
    "    DATA_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Funciones helper locales\n",
    "    def _guess_text_col(df):\n",
    "        for c in [\"texto\", \"text\", \"comment\", \"comentario\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[0]\n",
    "    \n",
    "    def _guess_label_col(df):\n",
    "        for c in [\"etiqueta\", \"label\", \"category\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[1] if len(df.columns) > 1 else df.columns[-1]\n",
    "    \n",
    "    def _norm_label_bin(s):\n",
    "        if pd.isna(s): \n",
    "            return \"\"\n",
    "        s = str(s).strip().lower()\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "        return {'depresivo': 'depresion'}.get(s, s)\n",
    "\n",
    "# Validar existencia de splits\n",
    "if not SPLITS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[ERROR] Splits no encontrados en {SPLITS_PATH}\\n\"\n",
    "        f\"        Debes ejecutar primero: 02_create_splits.ipynb\"\n",
    "    )\n",
    "\n",
    "print(\"Paths configurados:\")\n",
    "print(f\"  BASE_PATH:   {BASE_PATH}\")\n",
    "print(f\"  DATA_PATH:   {DATA_PATH}\")\n",
    "print(f\"  SPLITS_PATH: {SPLITS_PATH}\")\n",
    "\n",
    "# Columnas esperadas en dataset_base.csv\n",
    "TEXT_COL = \"texto\"\n",
    "LABEL_COL = \"etiqueta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39de3f3",
   "metadata": {},
   "source": [
    "## 1) Carga y preprocesamiento **agresivo** (pensado para ML clásico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56fb4499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splits: 2500 train, 626 val\n",
      "[INFO] Distribución train: {'depresion': 1760, 'ansiedad': 740}\n",
      "[INFO] Distribución val: {'depresion': 441, 'ansiedad': 185}\n",
      "[INFO] Distribución train: {'depresion': 1760, 'ansiedad': 740}\n",
      "[INFO] Distribución val: {'depresion': 441, 'ansiedad': 185}\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos y preprocesamiento agresivo para TF-IDF\n",
    "# \n",
    "# Estrategia: Máxima normalización (lowercase, sin tildes, sin símbolos)\n",
    "# - Reduce vocabulario y mejora robustez ante typos\n",
    "# - Marca negaciones: \"no tengo\" → \"no_tengo\" (feature diferenciado)\n",
    "# - Char n-grams (3-5): robustos a variantes ortográficas\n",
    "#\n",
    "# Comparación: Rule-based conserva tildes, BETO solo tokeniza, TF-IDF normaliza todo\n",
    "\n",
    "import pandas as pd, re, unicodedata\n",
    "\n",
    "# Cargar splits unificados desde 02_create_splits.ipynb\n",
    "dataset_base = pd.read_csv(SPLITS_PATH / 'dataset_base.csv')\n",
    "train_indices = pd.read_csv(SPLITS_PATH / 'train_indices.csv')['row_id'].values\n",
    "val_indices = pd.read_csv(SPLITS_PATH / 'val_indices.csv')['row_id'].values\n",
    "\n",
    "text_col = _guess_text_col(dataset_base)\n",
    "label_col = _guess_label_col(dataset_base)\n",
    "\n",
    "print(f\"[INFO] Splits: {len(train_indices)} train, {len(val_indices)} val\")\n",
    "\n",
    "# Definir función de limpieza agresiva\n",
    "RE_MULTI = re.compile(r'(.)\\1{2,}')  # Detecta 3+ letras repetidas\n",
    "\n",
    "def clean_text_ml(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpieza AGRESIVA para TF-IDF (máxima normalización para robustez).\n",
    "    \n",
    "    Aplica:\n",
    "    - Lowercase completo\n",
    "    - Normalización NFC + eliminación de tildes\n",
    "    - Colapso de alargamientos\n",
    "    - Eliminación de símbolos especiales (preserva puntuación básica)\n",
    "    - Marca negaciones simples (\"no X\" → \"no_X\")\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    \n",
    "    s = str(s).lower().strip()           # Lowercase total\n",
    "    s = unicodedata.normalize(\"NFC\", s)  # Normaliza tildes\n",
    "    s = RE_MULTI.sub(r'\\1\\1', s)         # Colapsa alargamientos\n",
    "    \n",
    "    # Elimina símbolos especiales (preserva letras, números, puntuación básica)\n",
    "    s = re.sub(r\"[^a-z0-9áéíóúüñ\\s.,!?:/\\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()   # Normaliza espacios\n",
    "    \n",
    "    # Marca negaciones simples: \"no tengo\" → \"no_tengo\"\n",
    "    s = re.sub(r\"\\bno\\s+([a-záéíóúüñ]{2,})\", r\"no_\\1\", s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "dataset_base['texto_ml'] = dataset_base[text_col].map(clean_text_ml)\n",
    "\n",
    "df_train = dataset_base[dataset_base['row_id'].isin(train_indices)].copy()\n",
    "df_val = dataset_base[dataset_base['row_id'].isin(val_indices)].copy()\n",
    "\n",
    "X_train, y_train = df_train['texto_ml'], df_train[label_col]\n",
    "X_val, y_val = df_val['texto_ml'], df_val[label_col]\n",
    "\n",
    "print(f\"[INFO] Distribución train: {dict(y_train.value_counts())}\")\n",
    "print(f\"[INFO] Distribución val: {dict(y_val.value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b1573",
   "metadata": {},
   "source": [
    "## 2) Split estratificado y entrenamiento (char TF‑IDF + LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53508f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('tfidf', tfidf_char),\n",
    "    ('svm', LinearSVC(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"[INFO] Entrenamiento completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff481a1c",
   "metadata": {},
   "source": [
    "## 3) Métricas y exportables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "921cc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Exportados:\n",
      "  - Predicciones: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_predictions.csv\n",
      "  - Reporte: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_classification_report.csv\n",
      "  - Métricas: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_eval.csv\n",
      "  - Matriz: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_confusion_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "tfidf_pred_csv   = DATA_PATH/'tfidf_predictions.csv'\n",
    "tfidf_report_csv = DATA_PATH/'tfidf_classification_report.csv'\n",
    "tfidf_eval_csv   = DATA_PATH/'tfidf_eval.csv'\n",
    "tfidf_cm_csv     = DATA_PATH/'tfidf_confusion_matrix.csv'\n",
    "\n",
    "classes = ['depresion','ansiedad']\n",
    "\n",
    "pd.DataFrame(classification_report(y_val, y_pred, labels=classes, output_dict=True, zero_division=0))  .transpose().to_csv(tfidf_report_csv, index=True, encoding='utf-8')\n",
    "\n",
    "pd.DataFrame([{\n",
    "    'macro_f1': f1_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'macro_precision': precision_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'macro_recall': recall_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'n': int(len(y_val))\n",
    "}]).to_csv(tfidf_eval_csv, index=False, encoding='utf-8')\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred, labels=classes)\n",
    "pd.DataFrame(cm, index=[f'true_{c}' for c in classes], columns=[f'pred_{c}' for c in classes]).to_csv(tfidf_cm_csv)\n",
    "\n",
    "pd.DataFrame({'texto': X_val, 'y_true': y_val, 'y_pred': y_pred}).to_csv(tfidf_pred_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"[INFO] Exportados:\")\n",
    "print(\"  - Predicciones:\", tfidf_pred_csv)\n",
    "print(\"  - Reporte:\", tfidf_report_csv)\n",
    "print(\"  - Métricas:\", tfidf_eval_csv)\n",
    "print(\"  - Matriz:\", tfidf_cm_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4eb729",
   "metadata": {},
   "source": [
    "## 4) Análisis de Errores (FP/FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c8f2b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Análisis de errores exportado:\n",
      "  FP Depresión: 24 casos → tfidf_fp_depresion.csv\n",
      "  FN Depresión: 27 casos → tfidf_fn_depresion.csv\n",
      "  FP Ansiedad:  27 casos → tfidf_fp_ansiedad.csv\n",
      "  FN Ansiedad:  24 casos → tfidf_fn_ansiedad.csv\n"
     ]
    }
   ],
   "source": [
    "# Exportar errores para análisis cualitativo\n",
    "# FP = predijo X cuando era Y (falsos positivos de cada clase)\n",
    "# FN = predijo Y cuando era X (falsos negativos de cada clase)\n",
    "\n",
    "# False Positives: casos predichos como depresión cuando eran ansiedad\n",
    "fp_depresion = df_val[(y_val == 'ansiedad') & (y_pred == 'depresion')].copy()\n",
    "fp_depresion['error_type'] = 'FP_depresion'\n",
    "\n",
    "# False Negatives: casos que eran depresión pero se predijeron como ansiedad\n",
    "fn_depresion = df_val[(y_val == 'depresion') & (y_pred == 'ansiedad')].copy()\n",
    "fn_depresion['error_type'] = 'FN_depresion'\n",
    "\n",
    "# False Positives: casos predichos como ansiedad cuando eran depresión\n",
    "fp_ansiedad = df_val[(y_val == 'depresion') & (y_pred == 'ansiedad')].copy()\n",
    "fp_ansiedad['error_type'] = 'FP_ansiedad'\n",
    "\n",
    "# False Negatives: casos que eran ansiedad pero se predijeron como depresión  \n",
    "fn_ansiedad = df_val[(y_val == 'ansiedad') & (y_pred == 'depresion')].copy()\n",
    "fn_ansiedad['error_type'] = 'FN_ansiedad'\n",
    "\n",
    "# Guardar análisis de errores\n",
    "tfidf_fp_dep_csv = DATA_PATH / 'tfidf_fp_depresion.csv'\n",
    "tfidf_fn_dep_csv = DATA_PATH / 'tfidf_fn_depresion.csv'\n",
    "tfidf_fp_ans_csv = DATA_PATH / 'tfidf_fp_ansiedad.csv'\n",
    "tfidf_fn_ans_csv = DATA_PATH / 'tfidf_fn_ansiedad.csv'\n",
    "\n",
    "fp_depresion[[text_col, label_col, 'error_type']].to_csv(tfidf_fp_dep_csv, index=False, encoding='utf-8')\n",
    "fn_depresion[[text_col, label_col, 'error_type']].to_csv(tfidf_fn_dep_csv, index=False, encoding='utf-8')\n",
    "fp_ansiedad[[text_col, label_col, 'error_type']].to_csv(tfidf_fp_ans_csv, index=False, encoding='utf-8')\n",
    "fn_ansiedad[[text_col, label_col, 'error_type']].to_csv(tfidf_fn_ans_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"[INFO] Análisis de errores exportado:\")\n",
    "print(f\"  FP Depresión: {len(fp_depresion)} casos → {tfidf_fp_dep_csv.name}\")\n",
    "print(f\"  FN Depresión: {len(fn_depresion)} casos → {tfidf_fn_dep_csv.name}\")\n",
    "print(f\"  FP Ansiedad:  {len(fp_ansiedad)} casos → {tfidf_fp_ans_csv.name}\")\n",
    "print(f\"  FN Ansiedad:  {len(fn_ansiedad)} casos → {tfidf_fn_ans_csv.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
