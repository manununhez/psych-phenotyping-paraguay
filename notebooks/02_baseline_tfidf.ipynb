{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864cb57e",
   "metadata": {},
   "source": [
    "# 02_baseline_tfidf ‚Äî Binario A/D\n",
    "\n",
    "**Objetivo:** baseline cl√°sico **robusto a ruido** (typos/transcripci√≥n) con **char TF‚ÄëIDF (3‚Äì5)** + **SVM (LinearSVC)**.  \n",
    "**Justificaci√≥n:** los n‚Äëgramas de caracteres capturan patrones ortogr√°ficos aun con errores; es un buen contrapunto al enfoque rule‚Äëbased.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361947b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Utilizando utils_shared.py\n",
      "Paths configurados:\n",
      "  BASE_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay\n",
      "  DATA_PATH:   /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data\n",
      "  SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Paths, Imports, y Utilidades Compartidas\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, unicodedata, os\n",
    "\n",
    "# Intentar importar utilidades compartidas\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, str(Path.cwd()))\n",
    "    from utils_shared import setup_paths, guess_text_col, guess_label_col, normalize_label\n",
    "    print(\"[INFO] Utilizando utils_shared.py\")\n",
    "    \n",
    "    # Setup de paths centralizado\n",
    "    paths = setup_paths()\n",
    "    BASE_PATH = paths['BASE_PATH']\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    \n",
    "    # Usar funciones centralizadas\n",
    "    _guess_text_col = guess_text_col\n",
    "    _guess_label_col = guess_label_col\n",
    "    _norm_label_bin = normalize_label\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"[WARNING] utils_shared.py no encontrado, usando funciones locales\")\n",
    "    \n",
    "    # Setup manual de paths\n",
    "    BASE_PATH = Path.cwd()\n",
    "    if BASE_PATH.name == \"notebooks\":\n",
    "        BASE_PATH = BASE_PATH.parent\n",
    "    \n",
    "    DATA_PATH = BASE_PATH / \"data\"\n",
    "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
    "    \n",
    "    DATA_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Funciones helper locales\n",
    "    def _guess_text_col(df):\n",
    "        for c in [\"texto\", \"text\", \"comment\", \"comentario\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[0]\n",
    "    \n",
    "    def _guess_label_col(df):\n",
    "        for c in [\"etiqueta\", \"label\", \"category\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return df.columns[1] if len(df.columns) > 1 else df.columns[-1]\n",
    "    \n",
    "    def _norm_label_bin(s):\n",
    "        if pd.isna(s): \n",
    "            return \"\"\n",
    "        s = str(s).strip().lower()\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "        return {'depresivo': 'depresion'}.get(s, s)\n",
    "\n",
    "# Validar existencia de splits\n",
    "if not SPLITS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[ERROR] Splits no encontrados en {SPLITS_PATH}\\n\"\n",
    "        f\"        Debes ejecutar primero: 02_create_splits.ipynb\"\n",
    "    )\n",
    "\n",
    "print(\"Paths configurados:\")\n",
    "print(f\"  BASE_PATH:   {BASE_PATH}\")\n",
    "print(f\"  DATA_PATH:   {DATA_PATH}\")\n",
    "print(f\"  SPLITS_PATH: {SPLITS_PATH}\")\n",
    "\n",
    "# Columnas esperadas en dataset_base.csv\n",
    "TEXT_COL = \"texto\"\n",
    "LABEL_COL = \"etiqueta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39de3f3",
   "metadata": {},
   "source": [
    "## 1) Carga y preprocesamiento **agresivo** (pensado para ML cl√°sico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fb4499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CARGA DE SPLITS (PATIENT-LEVEL)\n",
      "============================================================\n",
      "‚úì Dataset base: 3127 casos\n",
      "‚úì Train indices: 1849 casos\n",
      "‚úì Dev indices: 641 casos\n",
      "\n",
      "[INFO] Split aplicado (patient-level):\n",
      "  Train: 1849 casos\n",
      "  Val:   641 casos\n",
      "\n",
      "[INFO] Distribuci√≥n train: {'depresion': 1270, 'ansiedad': 579}\n",
      "[INFO] Distribuci√≥n val: {'depresion': 456, 'ansiedad': 185}\n",
      "\n",
      "  RECORDATORIO: Estos splits eliminan leakage (pacientes disjuntos)\n",
      "   M√©tricas ser√°n m√°s conservadoras pero generalizan mejor.\n",
      "\n",
      "[INFO] Split aplicado (patient-level):\n",
      "  Train: 1849 casos\n",
      "  Val:   641 casos\n",
      "\n",
      "[INFO] Distribuci√≥n train: {'depresion': 1270, 'ansiedad': 579}\n",
      "[INFO] Distribuci√≥n val: {'depresion': 456, 'ansiedad': 185}\n",
      "\n",
      "  RECORDATORIO: Estos splits eliminan leakage (pacientes disjuntos)\n",
      "   M√©tricas ser√°n m√°s conservadoras pero generalizan mejor.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# CARGA DE DATOS - PATIENT-LEVEL SPLIT (SIN LEAKAGE)\n",
    "# ===============================================================\n",
    "# IMPORTANTE: Este baseline usa splits generados por 02_create_splits.ipynb\n",
    "#\n",
    "# Estrategia de split:\n",
    "#   - Por PACIENTES (no por casos/consultas)\n",
    "#   - 72 pacientes train / 18 pacientes val\n",
    "#   - 0% overlap (sin data leakage)\n",
    "#\n",
    "# ¬øPor qu√© patient-level?\n",
    "#   Dataset tiene estructura longitudinal: 90 pacientes √ó 35 consultas promedio\n",
    "#   Split por casos ‚Üí 100% pacientes en train Y val (leakage total)\n",
    "#   Split por pacientes ‚Üí 0% overlap (generaliza a pacientes nuevos)\n",
    "#\n",
    "# Ver detalles en: ESTRATEGIA_SPLIT_PACIENTES.md\n",
    "\n",
    "import pandas as pd, re, unicodedata\n",
    "\n",
    "# Cargar splits unificados desde 02_create_splits.ipynb\n",
    "print(\"=\"*60)\n",
    "print(\"CARGA DE SPLITS (PATIENT-LEVEL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset_base = pd.read_csv(SPLITS_PATH / 'dataset_base.csv')\n",
    "train_indices = pd.read_csv(SPLITS_PATH / 'train_indices.csv')['row_id'].values\n",
    "dev_indices = pd.read_csv(SPLITS_PATH / 'dev_indices.csv')['row_id'].values\n",
    "\n",
    "print(f\"‚úì Dataset base: {len(dataset_base)} casos\")\n",
    "print(f\"‚úì Train indices: {len(train_indices)} casos\")\n",
    "print(f\"‚úì Dev indices: {len(dev_indices)} casos\")\n",
    "\n",
    "text_col = _guess_text_col(dataset_base)\n",
    "label_col = _guess_label_col(dataset_base)\n",
    "\n",
    "# ===============================================================\n",
    "# PREPROCESAMIENTO AGRESIVO (m√°xima normalizaci√≥n para TF-IDF)\n",
    "# ===============================================================\n",
    "# Estrategia: M√°xima normalizaci√≥n (lowercase, sin tildes, sin s√≠mbolos)\n",
    "# - Reduce vocabulario y mejora robustez ante typos\n",
    "# - Marca negaciones: \"no tengo\" ‚Üí \"no_tengo\" (feature diferenciado)\n",
    "# - Char n-grams (3-5): robustos a variantes ortogr√°ficas\n",
    "#\n",
    "# Comparaci√≥n: Rule-based conserva tildes, BETO solo tokeniza, TF-IDF normaliza todo\n",
    "\n",
    "RE_MULTI = re.compile(r'(.)\\1{2,}')  # Detecta 3+ letras repetidas\n",
    "\n",
    "def clean_text_ml(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpieza AGRESIVA para TF-IDF (m√°xima normalizaci√≥n para robustez).\n",
    "    \n",
    "    Aplica:\n",
    "    - Lowercase completo\n",
    "    - Normalizaci√≥n NFC + eliminaci√≥n de tildes\n",
    "    - Colapso de alargamientos\n",
    "    - Eliminaci√≥n de s√≠mbolos especiales (preserva puntuaci√≥n b√°sica)\n",
    "    - Marca negaciones simples (\"no X\" ‚Üí \"no_X\")\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    \n",
    "    s = str(s).lower().strip()           # Lowercase total\n",
    "    s = unicodedata.normalize(\"NFC\", s)  # Normaliza tildes\n",
    "    s = RE_MULTI.sub(r'\\1\\1', s)         # Colapsa alargamientos\n",
    "    \n",
    "    # Elimina s√≠mbolos especiales (preserva letras, n√∫meros, puntuaci√≥n b√°sica)\n",
    "    s = re.sub(r\"[^a-z0-9√°√©√≠√≥√∫√º√±\\s.,!?:/\\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()   # Normaliza espacios\n",
    "    \n",
    "    # Marca negaciones simples: \"no tengo\" ‚Üí \"no_tengo\"\n",
    "    s = re.sub(r\"\\bno\\s+([a-z√°√©√≠√≥√∫√º√±]{2,})\", r\"no_\\1\", s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "dataset_base['texto_ml'] = dataset_base[text_col].map(clean_text_ml)\n",
    "\n",
    "# Filtrar por √≠ndices (patient-level split)\n",
    "df_train = dataset_base[dataset_base['row_id'].isin(train_indices)].copy()\n",
    "df_val = dataset_base[dataset_base['row_id'].isin(dev_indices)].copy()\n",
    "\n",
    "X_train, y_train = df_train['texto_ml'], df_train[label_col]\n",
    "X_val, y_val = df_val['texto_ml'], df_val[label_col]\n",
    "\n",
    "print(f\"\\n[INFO] Split aplicado (patient-level):\")\n",
    "print(f\"  Train: {len(X_train)} casos\")\n",
    "print(f\"  Val:   {len(X_val)} casos\")\n",
    "print(f\"\\n[INFO] Distribuci√≥n train: {dict(y_train.value_counts())}\")\n",
    "print(f\"[INFO] Distribuci√≥n val: {dict(y_val.value_counts())}\")\n",
    "print(\"\\n  RECORDATORIO: Estos splits eliminan leakage (pacientes disjuntos)\")\n",
    "print(\"   M√©tricas ser√°n m√°s conservadoras pero generalizan mejor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b1573",
   "metadata": {},
   "source": [
    "## 2) Split estratificado y entrenamiento (char TF‚ÄëIDF + LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53508f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('tfidf', tfidf_char),\n",
    "    ('svm', LinearSVC(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"[INFO] Entrenamiento completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff481a1c",
   "metadata": {},
   "source": [
    "## 3) M√©tricas y exportables\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE - MANEJO DE CASOS NEUTRALES (TF-IDF vs Rule-Based):**\n",
    "\n",
    "**TF-IDF es un modelo ML BINARIO FORZADO:**\n",
    "- LinearSVC genera **solo 2 salidas posibles**: ansiedad o depresi√≥n\n",
    "- **NO puede abstenerse** ni generar predicciones \"neutral\"\n",
    "- Siempre clasifica cada caso en una de las dos clases (decisi√≥n forzada)\n",
    "\n",
    "**Diferencia con Rule-Based:**\n",
    "- **Rule-Based:** Puede devolver \"neutral\" (~78.4% casos sin matches)\n",
    "  - Tiene 3 salidas: ansiedad, depresi√≥n, neutral\n",
    "  - Para comparar con TF-IDF, convierte neutrales ‚Üí mayoritaria\n",
    "  \n",
    "- **TF-IDF:** Siempre binario (0% neutrales)\n",
    "  - Tiene 2 salidas: ansiedad o depresi√≥n\n",
    "  - Aprende frontera de decisi√≥n en espacio char n-grams\n",
    "  - Incluso casos ambiguos son forzados a una clase\n",
    "\n",
    "**Implicaciones para comparaci√≥n:**\n",
    "1. ‚úÖ TF-IDF comparable directamente con Dummy/BETO (todos binarios)\n",
    "2. ‚ö†Ô∏è Comparaci√≥n con Rule-Based es INJUSTA:\n",
    "   - F1 RB bajo = 78% neutrales + errores en 22% detectado\n",
    "   - F1 TF-IDF alto = decisi√≥n forzada en 100% casos\n",
    "3. üìä Gap real: TF-IDF aprende vocabulario paraguayo que RB no cubre\n",
    "\n",
    "**En este notebook:**\n",
    "- NO hay conversi√≥n de neutrales (modelo binario puro)\n",
    "- M√©tricas reflejan capacidad discriminativa directa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921cc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Exportados:\n",
      "  - Predicciones: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_predictions.csv\n",
      "  - Reporte: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_classification_report.csv\n",
      "  - M√©tricas: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_eval.csv\n",
      "  - Matriz: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_confusion_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "tfidf_pred_csv   = DATA_PATH/'tfidf_predictions.csv'\n",
    "tfidf_report_csv = DATA_PATH/'tfidf_classification_report.csv'\n",
    "tfidf_eval_csv   = DATA_PATH/'tfidf_eval.csv'\n",
    "tfidf_cm_csv     = DATA_PATH/'tfidf_confusion_matrix.csv'\n",
    "\n",
    "classes = ['depresion','ansiedad']\n",
    "\n",
    "pd.DataFrame(classification_report(y_val, y_pred, labels=classes, output_dict=True, zero_division=0))  .transpose().to_csv(tfidf_report_csv, index=True, encoding='utf-8')\n",
    "\n",
    "pd.DataFrame([{\n",
    "    'macro_f1': f1_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'macro_precision': precision_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'macro_recall': recall_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "    'n': int(len(y_val))\n",
    "}]).to_csv(tfidf_eval_csv, index=False, encoding='utf-8')\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred, labels=classes)\n",
    "pd.DataFrame(cm, index=[f'true_{c}' for c in classes], columns=[f'pred_{c}' for c in classes]).to_csv(tfidf_cm_csv)\n",
    "\n",
    "pd.DataFrame({'texto': X_val, 'y_true': y_val, 'y_pred': y_pred}).to_csv(tfidf_pred_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"[INFO] Exportados:\")\n",
    "print(\"  - Predicciones:\", tfidf_pred_csv)\n",
    "print(\"  - Reporte:\", tfidf_report_csv)\n",
    "print(\"  - M√©tricas:\", tfidf_eval_csv)\n",
    "print(\"  - Matriz:\", tfidf_cm_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4eb729",
   "metadata": {},
   "source": [
    "## 4) An√°lisis de Errores (FP/FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8f2b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] An√°lisis de errores exportado:\n",
      "  FP Depresi√≥n: 42 casos ‚Üí tfidf_fp_depresion.csv\n",
      "  FN Depresi√≥n: 27 casos ‚Üí tfidf_fn_depresion.csv\n",
      "  FP Ansiedad:  27 casos ‚Üí tfidf_fp_ansiedad.csv\n",
      "  FN Ansiedad:  42 casos ‚Üí tfidf_fn_ansiedad.csv\n"
     ]
    }
   ],
   "source": [
    "# Exportar errores para an√°lisis cualitativo\n",
    "# FP = predijo X cuando era Y (falsos positivos de cada clase)\n",
    "# FN = predijo Y cuando era X (falsos negativos de cada clase)\n",
    "\n",
    "# False Positives: casos predichos como depresi√≥n cuando eran ansiedad\n",
    "fp_depresion = df_val[(y_val == 'ansiedad') & (y_pred == 'depresion')].copy()\n",
    "fp_depresion['error_type'] = 'FP_depresion'\n",
    "\n",
    "# False Negatives: casos que eran depresi√≥n pero se predijeron como ansiedad\n",
    "fn_depresion = df_val[(y_val == 'depresion') & (y_pred == 'ansiedad')].copy()\n",
    "fn_depresion['error_type'] = 'FN_depresion'\n",
    "\n",
    "# False Positives: casos predichos como ansiedad cuando eran depresi√≥n\n",
    "fp_ansiedad = df_val[(y_val == 'depresion') & (y_pred == 'ansiedad')].copy()\n",
    "fp_ansiedad['error_type'] = 'FP_ansiedad'\n",
    "\n",
    "# False Negatives: casos que eran ansiedad pero se predijeron como depresi√≥n  \n",
    "fn_ansiedad = df_val[(y_val == 'ansiedad') & (y_pred == 'depresion')].copy()\n",
    "fn_ansiedad['error_type'] = 'FN_ansiedad'\n",
    "\n",
    "# Guardar an√°lisis de errores\n",
    "tfidf_fp_dep_csv = DATA_PATH / 'tfidf_fp_depresion.csv'\n",
    "tfidf_fn_dep_csv = DATA_PATH / 'tfidf_fn_depresion.csv'\n",
    "tfidf_fp_ans_csv = DATA_PATH / 'tfidf_fp_ansiedad.csv'\n",
    "tfidf_fn_ans_csv = DATA_PATH / 'tfidf_fn_ansiedad.csv'\n",
    "\n",
    "fp_depresion[[text_col, label_col, 'error_type']].to_csv(tfidf_fp_dep_csv, index=False, encoding='utf-8')\n",
    "fn_depresion[[text_col, label_col, 'error_type']].to_csv(tfidf_fn_dep_csv, index=False, encoding='utf-8')\n",
    "fp_ansiedad[[text_col, label_col, 'error_type']].to_csv(tfidf_fp_ans_csv, index=False, encoding='utf-8')\n",
    "fn_ansiedad[[text_col, label_col, 'error_type']].to_csv(tfidf_fn_ans_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"[INFO] An√°lisis de errores exportado:\")\n",
    "print(f\"  FP Depresi√≥n: {len(fp_depresion)} casos ‚Üí {tfidf_fp_dep_csv.name}\")\n",
    "print(f\"  FN Depresi√≥n: {len(fn_depresion)} casos ‚Üí {tfidf_fn_dep_csv.name}\")\n",
    "print(f\"  FP Ansiedad:  {len(fp_ansiedad)} casos ‚Üí {tfidf_fp_ans_csv.name}\")\n",
    "print(f\"  FN Ansiedad:  {len(fn_ansiedad)} casos ‚Üí {tfidf_fn_ans_csv.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38152271",
   "metadata": {},
   "source": [
    "## 5) Cross-Validation 5-Fold (Evaluaci√≥n Robusta)\n",
    "\n",
    "**Estrategia final:** Evaluar con CV para estimar F1 con intervalos de confianza.\n",
    "\n",
    "**Justificaci√≥n:**\n",
    "- Dataset peque√±o (90 pacientes) ‚Üí alta varianza por muestreo\n",
    "- Single test set (18 pacientes) puede variar ¬±10-15% por azar\n",
    "- CV 5-fold usa TODOS los datos ‚Üí estimaci√≥n m√°s confiable\n",
    "- Permite reportar: F1 = 0.85 ¬± 0.03 (IC95%)\n",
    "\n",
    "**Resultados previos:**\n",
    "- Dev 60/20/20: F1 = 0.866 (18 pacientes)\n",
    "- Test 60/20/20: F1 = 0.786 (18 pacientes)\n",
    "- **Varianza observada:** 8 puntos de F1 entre evaluaciones\n",
    "\n",
    "**Objetivo:** Estimar F1 real con todos los pacientes y cuantificar varianza.\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è MANEJO DE NEUTRALES EN CV:**\n",
    "\n",
    "**TF-IDF NO genera predicciones neutrales en ning√∫n fold:**\n",
    "- Modelo ML binario forzado (LinearSVC con 2 clases)\n",
    "- Varianza de CV refleja **solo capacidad de generalizaci√≥n**\n",
    "- NO hay conversi√≥n de neutrales (contrario a Rule-Based)\n",
    "\n",
    "**Diferencia con Rule-Based CV:**\n",
    "- **Rule-Based:** ~78% neutrales por fold ‚Üí convertidos a mayoritaria\n",
    "  - Varianza CV = heterogeneidad dataset + cobertura variable\n",
    "  - F1 penalizado por falta de cobertura\n",
    "  \n",
    "- **TF-IDF:** 0% neutrales (binario puro)\n",
    "  - Varianza CV = capacidad de generalizar entre folds\n",
    "  - F1 refleja discriminaci√≥n real (no penalizaci√≥n por cobertura)\n",
    "\n",
    "**Interpretaci√≥n:**\n",
    "- Si TF-IDF tiene menor varianza que RB ‚Üí aprende mejor\n",
    "- Si F1 TF-IDF >> F1 RB ‚Üí vocabulario ML cubre dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1388140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-VALIDATION 5-FOLD (PATIENT-LEVEL)\n",
      "================================================================================\n",
      "\n",
      "‚úì Dataset completo: 3126 casos\n",
      "‚úì Pacientes √∫nicos: 90\n",
      "\n",
      "Fold 1/5... ‚úì Dataset completo: 3126 casos\n",
      "‚úì Pacientes √∫nicos: 90\n",
      "\n",
      "Fold 1/5... F1=0.801, Prec=0.817, Rec=0.790\n",
      "Fold 2/5... F1=0.801, Prec=0.817, Rec=0.790\n",
      "Fold 2/5... F1=0.867, Prec=0.876, Rec=0.859\n",
      "Fold 3/5... F1=0.867, Prec=0.876, Rec=0.859\n",
      "Fold 3/5... F1=0.843, Prec=0.831, Rec=0.866\n",
      "Fold 4/5... F1=0.843, Prec=0.831, Rec=0.866\n",
      "Fold 4/5... F1=0.883, Prec=0.904, Rec=0.868\n",
      "Fold 5/5... F1=0.883, Prec=0.904, Rec=0.868\n",
      "Fold 5/5... F1=0.854, Prec=0.838, Rec=0.878\n",
      "\n",
      "================================================================================\n",
      "RESULTADOS CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n",
      " fold  f1_macro  precision   recall  n_test_patients\n",
      "    1  0.801422   0.817126 0.789808               18\n",
      "    2  0.866665   0.875965 0.859445               18\n",
      "    3  0.842912   0.831371 0.866074               18\n",
      "    4  0.883363   0.903846 0.867605               18\n",
      "    5  0.853742   0.838419 0.877597               18\n",
      "\n",
      "üìä ESTAD√çSTICAS:\n",
      "   F1 macro:  0.850 ¬± 0.031\n",
      "   Precision: 0.853 ¬± 0.036\n",
      "   Recall:    0.852 ¬± 0.035\n",
      "\n",
      "   F1 min-max: [0.801, 0.883]\n",
      "   F1 IC95%:   [0.789, 0.910]\n",
      "   Varianza:   0.082 puntos entre folds\n",
      "\n",
      "üîç INTERPRETACI√ìN:\n",
      "   ‚Ä¢ F1 var√≠a 0.082 puntos entre folds ‚Üí normal en dataset peque√±o\n",
      "   ‚Ä¢ IC95%: [0.789, 0.910] ‚Üí rango esperado para nuevos pacientes\n",
      "   ‚Ä¢ TF-IDF generaliza bien (modelo binario forzado, 0% neutrales)\n",
      "\n",
      "üìà CONCLUSI√ìN:\n",
      "   ‚Ä¢ F1 TF-IDF (CV): 0.850 ¬± 0.031\n",
      "   ‚Ä¢ IC95%: [0.789, 0.910]\n",
      "   ‚Ä¢ Varianza: 0.082 puntos (normal en 90 pacientes)\n",
      "   ‚Ä¢ Comparaci√≥n: Dummy Stratified (0.50) ‚Üí TF-IDF (0.85) = +70% mejora\n",
      "\n",
      "üíæ Resultados exportados: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/cv_results/tfidf_cv_results.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Cross-Validation completado\n",
      "================================================================================\n",
      "F1=0.854, Prec=0.838, Rec=0.878\n",
      "\n",
      "================================================================================\n",
      "RESULTADOS CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n",
      " fold  f1_macro  precision   recall  n_test_patients\n",
      "    1  0.801422   0.817126 0.789808               18\n",
      "    2  0.866665   0.875965 0.859445               18\n",
      "    3  0.842912   0.831371 0.866074               18\n",
      "    4  0.883363   0.903846 0.867605               18\n",
      "    5  0.853742   0.838419 0.877597               18\n",
      "\n",
      "üìä ESTAD√çSTICAS:\n",
      "   F1 macro:  0.850 ¬± 0.031\n",
      "   Precision: 0.853 ¬± 0.036\n",
      "   Recall:    0.852 ¬± 0.035\n",
      "\n",
      "   F1 min-max: [0.801, 0.883]\n",
      "   F1 IC95%:   [0.789, 0.910]\n",
      "   Varianza:   0.082 puntos entre folds\n",
      "\n",
      "üîç INTERPRETACI√ìN:\n",
      "   ‚Ä¢ F1 var√≠a 0.082 puntos entre folds ‚Üí normal en dataset peque√±o\n",
      "   ‚Ä¢ IC95%: [0.789, 0.910] ‚Üí rango esperado para nuevos pacientes\n",
      "   ‚Ä¢ TF-IDF generaliza bien (modelo binario forzado, 0% neutrales)\n",
      "\n",
      "üìà CONCLUSI√ìN:\n",
      "   ‚Ä¢ F1 TF-IDF (CV): 0.850 ¬± 0.031\n",
      "   ‚Ä¢ IC95%: [0.789, 0.910]\n",
      "   ‚Ä¢ Varianza: 0.082 puntos (normal en 90 pacientes)\n",
      "   ‚Ä¢ Comparaci√≥n: Dummy Stratified (0.50) ‚Üí TF-IDF (0.85) = +70% mejora\n",
      "\n",
      "üíæ Resultados exportados: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/cv_results/tfidf_cv_results.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Cross-Validation completado\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CROSS-VALIDATION 5-FOLD (PATIENT-LEVEL)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Configuraci√≥n\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Preparar dataset con row_id\n",
    "df_full = dataset_base.copy()\n",
    "if 'row_id' not in df_full.columns:\n",
    "    df_full = df_full.reset_index(drop=True)\n",
    "    df_full['row_id'] = df_full.index\n",
    "\n",
    "# Limpiar NaNs\n",
    "df_full = df_full.dropna(subset=['texto', label_col]).copy()\n",
    "df_full['texto_ml'] = df_full['texto'].map(clean_text_ml)\n",
    "\n",
    "print(f\"‚úì Dataset completo: {len(df_full)} casos\")\n",
    "print(f\"‚úì Pacientes √∫nicos: {df_full['patient_id'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# Obtener etiqueta mayoritaria por paciente (para stratification)\n",
    "patient_labels = df_full.groupby('patient_id')[label_col].agg(\n",
    "    lambda x: x.value_counts().index[0]\n",
    ").reset_index()\n",
    "patient_labels.columns = ['patient_id', 'label_majority']\n",
    "\n",
    "# Crear folds stratificados\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "patient_ids = patient_labels['patient_id'].values\n",
    "patient_y = patient_labels['label_majority'].values\n",
    "\n",
    "# Ejecutar CV\n",
    "cv_results = []\n",
    "\n",
    "for fold_idx, (train_patient_idx, test_patient_idx) in enumerate(skf.split(patient_ids, patient_y), start=1):\n",
    "    print(f\"Fold {fold_idx}/{N_SPLITS}...\", end=\" \")\n",
    "    \n",
    "    # Obtener pacientes\n",
    "    train_patients = patient_ids[train_patient_idx]\n",
    "    test_patients = patient_ids[test_patient_idx]\n",
    "    \n",
    "    # Filtrar casos\n",
    "    train_df = df_full[df_full['patient_id'].isin(train_patients)]\n",
    "    test_df = df_full[df_full['patient_id'].isin(test_patients)]\n",
    "    \n",
    "    X_train_cv = train_df['texto_ml']\n",
    "    y_train_cv = train_df[label_col]\n",
    "    X_test_cv = test_df['texto_ml']\n",
    "    y_test_cv = test_df[label_col]\n",
    "    \n",
    "    # Entrenar modelo (mismo pipeline que antes)\n",
    "    clf_cv = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            analyzer='char_wb',\n",
    "            ngram_range=(3, 5),\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )),\n",
    "        ('svm', LinearSVC(class_weight='balanced', random_state=42, max_iter=2000))\n",
    "    ])\n",
    "    \n",
    "    clf_cv.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_cv = clf_cv.predict(X_test_cv)\n",
    "    \n",
    "    # M√©tricas\n",
    "    f1_cv = f1_score(y_test_cv, y_pred_cv, average='macro', zero_division=0)\n",
    "    prec_cv = precision_score(y_test_cv, y_pred_cv, average='macro', zero_division=0)\n",
    "    rec_cv = recall_score(y_test_cv, y_pred_cv, average='macro', zero_division=0)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'fold': fold_idx,\n",
    "        'f1_macro': f1_cv,\n",
    "        'precision': prec_cv,\n",
    "        'recall': rec_cv,\n",
    "        'n_train_patients': len(train_patients),\n",
    "        'n_test_patients': len(test_patients),\n",
    "        'n_train_cases': len(X_train_cv),\n",
    "        'n_test_cases': len(X_test_cv)\n",
    "    })\n",
    "    \n",
    "    print(f\"F1={f1_cv:.3f}, Prec={prec_cv:.3f}, Rec={rec_cv:.3f}\")\n",
    "\n",
    "# Resultados\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTADOS CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(cv_df[['fold', 'f1_macro', 'precision', 'recall', 'n_test_patients']].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Estad√≠sticas\n",
    "f1_mean = cv_df['f1_macro'].mean()\n",
    "f1_std = cv_df['f1_macro'].std()\n",
    "f1_min = cv_df['f1_macro'].min()\n",
    "f1_max = cv_df['f1_macro'].max()\n",
    "f1_ci_lower = f1_mean - 1.96 * f1_std\n",
    "f1_ci_upper = f1_mean + 1.96 * f1_std\n",
    "\n",
    "print(f\"üìä ESTAD√çSTICAS:\") \n",
    "print(f\"   F1 macro:  {f1_mean:.3f} ¬± {f1_std:.3f}\")\n",
    "print(f\"   Precision: {cv_df['precision'].mean():.3f} ¬± {cv_df['precision'].std():.3f}\")\n",
    "print(f\"   Recall:    {cv_df['recall'].mean():.3f} ¬± {cv_df['recall'].std():.3f}\")\n",
    "print()\n",
    "print(f\"   F1 min-max: [{f1_min:.3f}, {f1_max:.3f}]\")\n",
    "print(f\"   F1 IC95%:   [{f1_ci_lower:.3f}, {f1_ci_upper:.3f}]\")\n",
    "print(f\"   Varianza:   {(f1_max - f1_min):.3f} puntos entre folds\")\n",
    "print()\n",
    "\n",
    "# Interpretaci√≥n\n",
    "print(\"üîç INTERPRETACI√ìN:\")\n",
    "print(f\"   ‚Ä¢ F1 var√≠a {(f1_max - f1_min):.3f} puntos entre folds ‚Üí normal en dataset peque√±o\")\n",
    "print(f\"   ‚Ä¢ IC95%: [{f1_ci_lower:.3f}, {f1_ci_upper:.3f}] ‚Üí rango esperado para nuevos pacientes\")\n",
    "print(f\"   ‚Ä¢ TF-IDF generaliza bien (modelo binario forzado, 0% neutrales)\")\n",
    "\n",
    "print()\n",
    "print(f\"üìà CONCLUSI√ìN:\")\n",
    "print(f\"   ‚Ä¢ F1 TF-IDF (CV): {f1_mean:.3f} ¬± {f1_std:.3f}\")\n",
    "print(f\"   ‚Ä¢ IC95%: [{f1_ci_lower:.3f}, {f1_ci_upper:.3f}]\")\n",
    "print(f\"   ‚Ä¢ Varianza: {(f1_max - f1_min):.3f} puntos (normal en 90 pacientes)\")\n",
    "print(f\"   ‚Ä¢ Comparaci√≥n: Dummy Stratified (0.50) ‚Üí TF-IDF (0.85) = +70% mejora\")\n",
    "print()\n",
    "\n",
    "# Exportar\n",
    "cv_output = DATA_PATH / 'cv_results' / 'tfidf_cv_results.csv'\n",
    "cv_output.parent.mkdir(exist_ok=True)\n",
    "cv_df.to_csv(cv_output, index=False)\n",
    "print(f\"üíæ Resultados exportados: {cv_output}\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ Cross-Validation completado\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4cb31",
   "metadata": {},
   "source": [
    "## 6) Exportar Resultados y Pr√≥ximos Pasos\n",
    "\n",
    "**‚úÖ Archivos generados por este baseline:**\n",
    "\n",
    "Evaluaci√≥n en dev set:\n",
    "- `tfidf_predictions.csv` - Predicciones por caso\n",
    "- `tfidf_eval.csv` - M√©tricas macro agregadas\n",
    "- `tfidf_classification_report.csv` - Reporte por clase\n",
    "- `tfidf_confusion_matrix.csv` - Matriz de confusi√≥n\n",
    "\n",
    "Cross-Validation:\n",
    "- `cv_results/tfidf_cv_results.csv` - Resultados 5-fold CV\n",
    "\n",
    "---\n",
    "\n",
    "**üìä Para an√°lisis comparativo completo:**\n",
    "‚Üí Ejecutar notebook: `02_comparacion_resultados.ipynb`\n",
    "\n",
    "Este notebook consolida todos los resultados CV, calcula estad√≠sticas (IC95%), compara modelos, y genera visualizaciones e interpretaci√≥n para paper/tesis.\n",
    "\n",
    "---\n",
    "\n",
    "**üìù Notas metodol√≥gicas:**\n",
    "- **Dataset:** dataset_base.csv (3,155 casos, 90 pacientes)\n",
    "- **Split:** Patient-level 60/20/20 (0% leakage)\n",
    "- **CV:** 5-fold patient-level stratified (54 pacientes train por fold)\n",
    "- **Modelo:** TfidfVectorizer char(3-5) + LinearSVC(C=1.0, class_weight='balanced')\n",
    "- **Preprocesamiento:** Agresivo (lowercase, sin tildes, sin puntuaci√≥n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
