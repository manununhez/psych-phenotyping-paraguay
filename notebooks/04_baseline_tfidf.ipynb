{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864cb57e",
   "metadata": {},
   "source": [
    "# 04_baseline_tfidf — TF-IDF + LinearSVC\n",
    "\n",
    "**Objetivo:** Implementar un baseline robusto usando **Character TF-IDF** (n-grams 3-5) y **LinearSVC**.\n",
    "\n",
    "**Estrategia:**\n",
    "- **Train:** Usar `train_denoised.csv` (solo casos con señal clínica) para evitar aprender ruido.\n",
    "- **Dev:** Usar `dev_full.csv` (dataset completo) para evaluar en un escenario realista.\n",
    "- **Features:** Char n-grams son robustos a errores ortográficos y variantes morfológicas.\n",
    "\n",
    "**Exportables:**\n",
    "- `data/tfidf_eval.csv`\n",
    "- `data/tfidf_classification_report.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2afe241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Usando utils_shared.py\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Imports y configuración de paths\n",
    "# ===============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Importar utilidades compartidas\n",
    "try:\n",
    "    from utils_shared import setup_paths, load_splits, calculate_metrics, get_cv_splitter\n",
    "    paths = setup_paths()\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    print(\"[OK] Usando utils_shared.py\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] No se encontró utils_shared.py. Verifica que estás en el directorio correcto.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39de3f3",
   "metadata": {},
   "source": [
    "## 1) Carga de Datos y Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56fb4499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T17:32:28.234037Z",
     "iopub.status.busy": "2025-11-26T17:32:28.233785Z",
     "iopub.status.idle": "2025-11-26T17:32:28.462322Z",
     "shell.execute_reply": "2025-11-26T17:32:28.461992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (Denoised): 993 casos\n",
      "Dev (Full): 627 casos\n",
      "Preprocesando textos...\n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets procesados\n",
    "try:\n",
    "    # Train: Usar train_denoised (señal clínica) desde SPLITS_PATH\n",
    "    df_train = pd.read_csv(SPLITS_PATH / 'train_denoised.csv')\n",
    "    \n",
    "    # Dev: Construir desde splits (dataset completo)\n",
    "    df_base, _, dev_idx, _ = load_splits(SPLITS_PATH)\n",
    "    df_dev = df_base.set_index('row_id').loc[dev_idx].reset_index()\n",
    "    \n",
    "    print(f\"Train (Denoised): {len(df_train)} casos\")\n",
    "    print(f\"Dev (Full): {len(df_dev)} casos\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] No se encontraron los datasets. Ejecuta 03_rule_based_denoising.ipynb primero.\")\n",
    "    raise\n",
    "\n",
    "# Preprocesamiento Agresivo para TF-IDF\n",
    "RE_MULTI = re.compile(r'(.)\\1{2,}')\n",
    "\n",
    "def clean_text_ml(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = RE_MULTI.sub(r'\\1\\1', s)\n",
    "    s = re.sub(r\"[^a-z0-9áéíóúüñ\\s.,!?:/\\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # Marca negaciones simples: \"no tengo\" -> \"no_tengo\"\n",
    "    s = re.sub(r\"\\bno\\s+([a-záéíóúüñ]{2,})\", r\"no_\\1\", s)\n",
    "    return s\n",
    "\n",
    "print(\"Preprocesando textos...\")\n",
    "df_train['texto_ml'] = df_train['texto'].map(clean_text_ml)\n",
    "df_dev['texto_ml'] = df_dev['texto'].map(clean_text_ml)\n",
    "\n",
    "X_train = df_train['texto_ml']\n",
    "y_train = df_train['etiqueta']\n",
    "X_dev = df_dev['texto_ml']\n",
    "y_dev = df_dev['etiqueta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_model",
   "metadata": {},
   "source": [
    "## 2) Entrenamiento y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "train_code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T17:32:28.464378Z",
     "iopub.status.busy": "2025-11-26T17:32:28.464185Z",
     "iopub.status.idle": "2025-11-26T17:32:29.899921Z",
     "shell.execute_reply": "2025-11-26T17:32:29.899604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo...\n",
      "Evaluando en Dev...\n",
      "============================================================\n",
      "TF-IDF + LinearSVC Results\n",
      "============================================================\n",
      "Macro F1: 0.8386\n",
      "Macro Precision: 0.8679\n",
      "Macro Recall: 0.8336\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    ansiedad       0.94      0.71      0.81       285\n",
      "   depresion       0.80      0.96      0.87       342\n",
      "\n",
      "    accuracy                           0.85       627\n",
      "   macro avg       0.87      0.83      0.84       627\n",
      "weighted avg       0.86      0.85      0.84       627\n",
      "\n",
      "✓ Exportado: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_eval.csv\n",
      "✓ Exportado: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_classification_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Pipeline: TF-IDF (Chars) + LinearSVC\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        analyzer='char_wb',\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        max_features=10000\n",
    "    )),\n",
    "    ('clf', LinearSVC(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Entrenando modelo...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Evaluando en Dev...\")\n",
    "y_pred = pipeline.predict(X_dev)\n",
    "\n",
    "# Calcular métricas\n",
    "metrics = calculate_metrics(y_dev, y_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TF-IDF + LinearSVC Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Macro F1: {metrics['f1_macro']:.4f}\")\n",
    "print(f\"Macro Precision: {metrics['precision_macro']:.4f}\")\n",
    "print(f\"Macro Recall: {metrics['recall_macro']:.4f}\")\n",
    "print()\n",
    "print(metrics['report'])\n",
    "\n",
    "# Exportar resultados\n",
    "eval_df = pd.DataFrame([{\n",
    "    'modelo': 'tfidf_svm',\n",
    "    'f1_macro': metrics['f1_macro'],\n",
    "    'precision_macro': metrics['precision_macro'],\n",
    "    'recall_macro': metrics['recall_macro'],\n",
    "    'n_train': len(X_train),\n",
    "    'n_dev': len(X_dev)\n",
    "}])\n",
    "eval_df.to_csv(DATA_PATH / 'tfidf_eval.csv', index=False)\n",
    "print(f\"✓ Exportado: {DATA_PATH / 'tfidf_eval.csv'}\")\n",
    "\n",
    "# Exportar reporte completo\n",
    "report_df = pd.DataFrame(metrics['report_dict']).transpose()\n",
    "report_df.to_csv(DATA_PATH / 'tfidf_classification_report.csv')\n",
    "print(f\"✓ Exportado: {DATA_PATH / 'tfidf_classification_report.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fe4bb",
   "metadata": {},
   "source": [
    "## 3) Cross-Validation (5-Fold)\n",
    "\n",
    "Evaluación robusta usando Stratified Group K-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce8d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Cross-Validation (TF-IDF)...\n",
      "Fold 1: F1=0.8420\n",
      "Fold 2: F1=0.7966\n",
      "Fold 3: F1=0.8772\n",
      "Fold 4: F1=0.8032\n",
      "Fold 5: F1=0.8022\n",
      "\n",
      "Promedio CV:\n",
      "fold               3.000000\n",
      "f1_macro           0.824234\n",
      "precision_macro    0.826822\n",
      "recall_macro       0.841238\n",
      "dtype: float64\n",
      "✓ Exportado: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/tfidf_cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "from utils_shared import get_cv_splitter\n",
    "\n",
    "# Combinar Train + Dev\n",
    "df_full = pd.concat([df_train, df_dev]).reset_index(drop=True)\n",
    "# Recalcular limpieza para asegurar consistencia\n",
    "df_full['texto_ml'] = df_full['texto'].map(clean_text_ml)\n",
    "\n",
    "X_full = df_full['texto_ml']\n",
    "y_full = df_full['etiqueta']\n",
    "groups_full = df_full['patient_id']  # Usar patient_id directamente\n",
    "\n",
    "cv = get_cv_splitter(n_splits=5)\n",
    "cv_results = []\n",
    "\n",
    "print(\"Iniciando Cross-Validation (TF-IDF)...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_full, y_full, groups_full)):\n",
    "    X_tr, y_tr = X_full.iloc[train_idx], y_full.iloc[train_idx]\n",
    "    X_val, y_val = X_full.iloc[val_idx], y_full.iloc[val_idx]\n",
    "    \n",
    "    # Re-inicializar pipeline para no contaminar\n",
    "    pipeline_cv = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            analyzer='char_wb',\n",
    "            ngram_range=(3, 5),\n",
    "            min_df=2,\n",
    "            max_features=10000\n",
    "        )),\n",
    "        ('clf', LinearSVC(class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "    \n",
    "    pipeline_cv.fit(X_tr, y_tr)\n",
    "    y_pred = pipeline_cv.predict(X_val)\n",
    "    \n",
    "    metrics = calculate_metrics(y_val, y_pred)\n",
    "    cv_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'model': 'TF-IDF + LinearSVC',\n",
    "        'f1_macro': metrics['f1_macro'],\n",
    "        'precision_macro': metrics['precision_macro'],\n",
    "        'recall_macro': metrics['recall_macro']\n",
    "    })\n",
    "    print(f\"Fold {fold+1}: F1={metrics['f1_macro']:.4f}\")\n",
    "\n",
    "df_cv = pd.DataFrame(cv_results)\n",
    "print(\"\\nPromedio CV:\")\n",
    "print(df_cv.mean(numeric_only=True))\n",
    "\n",
    "out_path = DATA_PATH / 'tfidf_cv_results.csv'\n",
    "df_cv.to_csv(out_path, index=False)\n",
    "print(f\"✓ Exportado: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
