{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15388f5",
   "metadata": {},
   "source": [
    "# 02_baseline_dummy — Sanity Check Baselines\n",
    "\n",
    "**Objetivo:** Implementar baselines triviales (majority class, random estratificado) para validar que los modelos ML capturan patrones reales.\n",
    "\n",
    "**Exportables:**\n",
    "- `data/dummy_majority_eval.csv` + classification_report\n",
    "- `data/dummy_stratified_eval.csv` + classification_report\n",
    "\n",
    "**Para comparación completa:** Ver `05_comparacion_resultados.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452649d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T13:30:50.161977Z",
     "iopub.status.busy": "2025-11-26T13:30:50.161825Z",
     "iopub.status.idle": "2025-11-26T13:30:52.016800Z",
     "shell.execute_reply": "2025-11-26T13:30:52.016422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Usando utils_shared.py\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Imports y configuración de paths\n",
    "# ===============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Importar utilidades compartidas\n",
    "try:\n",
    "    from utils_shared import setup_paths, load_splits, calculate_metrics, get_cv_splitter\n",
    "    paths = setup_paths()\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    print(\"[OK] Usando utils_shared.py\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] No se encontró utils_shared.py. Verifica que estás en el directorio correcto.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac6a8a",
   "metadata": {},
   "source": [
    "## 1) Cargar datos y splits\n",
    "\n",
    "Usamos los mismos splits pacientes que los otros baselines (patient-level, 0% leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85f1e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T13:30:52.019104Z",
     "iopub.status.busy": "2025-11-26T13:30:52.018794Z",
     "iopub.status.idle": "2025-11-26T13:30:52.076826Z",
     "shell.execute_reply": "2025-11-26T13:30:52.076346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset base: 3131 casos\n",
      "Train: 1867 casos\n",
      "Dev: 627 casos\n",
      "\n",
      "Distribución train:\n",
      "depresion    1388\n",
      "ansiedad      479\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución dev:\n",
      "depresion    342\n",
      "ansiedad     285\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets alineados con estrategia Denoised\n",
    "try:\n",
    "    # Train: Usar train_denoised (señal clínica)\n",
    "    df_train = pd.read_csv(SPLITS_PATH / 'train_denoised.csv')\n",
    "    \n",
    "    # Dev: Construir desde splits (dataset completo)\n",
    "    df_base, _, dev_idx, _ = load_splits(SPLITS_PATH)\n",
    "    df_dev = df_base.set_index('row_id').loc[dev_idx].reset_index()\n",
    "    \n",
    "    print(f\"Train (Denoised): {len(df_train)} casos\")\n",
    "    print(f\"Dev (Full): {len(df_dev)} casos\")\n",
    "\n",
    "    # Preparar X, y\n",
    "    X_train = df_train['texto'].values\n",
    "    y_train = df_train['etiqueta'].values\n",
    "\n",
    "    X_dev = df_dev['texto'].values\n",
    "    y_dev = df_dev['etiqueta'].values\n",
    "\n",
    "    print(f\"\\nDistribución train:\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print(f\"\\nDistribución dev:\")\n",
    "    print(pd.Series(y_dev).value_counts())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] No se encontraron los datasets. Ejecuta 03_rule_based_denoising.ipynb primero.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1771302",
   "metadata": {},
   "source": [
    "## 2) Baseline 1: Majority Class\n",
    "\n",
    "Predice **siempre** la clase mayoritaria (Depresión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073f019e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T13:30:52.078973Z",
     "iopub.status.busy": "2025-11-26T13:30:52.078792Z",
     "iopub.status.idle": "2025-11-26T13:30:52.102776Z",
     "shell.execute_reply": "2025-11-26T13:30:52.102467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUMMY BASELINE: MAJORITY CLASS\n",
      "============================================================\n",
      "Macro F1: 0.3529\n",
      "Macro Precision: 0.2727\n",
      "Macro Recall: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    ansiedad       0.00      0.00      0.00       285\n",
      "   depresion       0.55      1.00      0.71       342\n",
      "\n",
      "    accuracy                           0.55       627\n",
      "   macro avg       0.27      0.50      0.35       627\n",
      "weighted avg       0.30      0.55      0.39       627\n",
      "\n",
      "✓ Exportado: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/dummy_majority_eval.csv\n"
     ]
    }
   ],
   "source": [
    "# Entrenar (solo aprende la clase mayoritaria)\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "dummy_majority.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en validación\n",
    "y_pred_majority = dummy_majority.predict(X_dev)\n",
    "\n",
    "# Calcular métricas\n",
    "metrics_majority = calculate_metrics(y_dev, y_pred_majority)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DUMMY BASELINE: MAJORITY CLASS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Macro F1: {metrics_majority['f1_macro']:.4f}\")\n",
    "print(f\"Macro Precision: {metrics_majority['precision_macro']:.4f}\")\n",
    "print(f\"Macro Recall: {metrics_majority['recall_macro']:.4f}\")\n",
    "print()\n",
    "print(metrics_majority['report'])\n",
    "\n",
    "# Exportar resultados\n",
    "eval_majority = pd.DataFrame([{\n",
    "    'modelo': 'dummy_majority',\n",
    "    'f1_macro': metrics_majority['f1_macro'],\n",
    "    'precision_macro': metrics_majority['precision_macro'],\n",
    "    'recall_macro': metrics_majority['recall_macro'],\n",
    "    'n_dev': len(y_dev)\n",
    "}])\n",
    "eval_majority.to_csv(DATA_PATH / 'dummy_majority_eval.csv', index=False)\n",
    "print(f\"✓ Exportado: {DATA_PATH / 'dummy_majority_eval.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a937eef",
   "metadata": {},
   "source": [
    "## 3) Baseline 2: Stratified Random\n",
    "\n",
    "Predice aleatoriamente respetando la distribución de clases del train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39862bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T13:30:52.104618Z",
     "iopub.status.busy": "2025-11-26T13:30:52.104475Z",
     "iopub.status.idle": "2025-11-26T13:30:52.130221Z",
     "shell.execute_reply": "2025-11-26T13:30:52.129722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUMMY BASELINE: STRATIFIED RANDOM\n",
      "============================================================\n",
      "Macro F1: 0.4764\n",
      "Macro Precision: 0.4945\n",
      "Macro Recall: 0.4956\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    ansiedad       0.45      0.26      0.33       285\n",
      "   depresion       0.54      0.73      0.62       342\n",
      "\n",
      "    accuracy                           0.52       627\n",
      "   macro avg       0.49      0.50      0.48       627\n",
      "weighted avg       0.50      0.52      0.49       627\n",
      "\n",
      "✓ Exportado: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/dummy_stratified_eval.csv\n"
     ]
    }
   ],
   "source": [
    "# Entrenar (aprende solo la distribución de clases)\n",
    "dummy_stratified = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy_stratified.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en validación\n",
    "y_pred_stratified = dummy_stratified.predict(X_dev)\n",
    "\n",
    "# Calcular métricas\n",
    "metrics_stratified = calculate_metrics(y_dev, y_pred_stratified)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DUMMY BASELINE: STRATIFIED RANDOM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Macro F1: {metrics_stratified['f1_macro']:.4f}\")\n",
    "print(f\"Macro Precision: {metrics_stratified['precision_macro']:.4f}\")\n",
    "print(f\"Macro Recall: {metrics_stratified['recall_macro']:.4f}\")\n",
    "print()\n",
    "print(metrics_stratified['report'])\n",
    "\n",
    "# Exportar resultados\n",
    "eval_stratified = pd.DataFrame([{\n",
    "    'modelo': 'dummy_stratified',\n",
    "    'f1_macro': metrics_stratified['f1_macro'],\n",
    "    'precision_macro': metrics_stratified['precision_macro'],\n",
    "    'recall_macro': metrics_stratified['recall_macro'],\n",
    "    'n_dev': len(y_dev)\n",
    "}])\n",
    "eval_stratified.to_csv(DATA_PATH / 'dummy_stratified_eval.csv', index=False)\n",
    "print(f\"✓ Exportado: {DATA_PATH / 'dummy_stratified_eval.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5084d9f",
   "metadata": {},
   "source": [
    "## 3) Cross-Validation (5-Fold)\n",
    "\n",
    "Para una evaluación robusta, combinamos Train + Dev y realizamos validación cruzada estratificada por paciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c922aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Cross-Validation (Dummy Stratified)...\n",
      "Fold 1: F1=0.4632\n",
      "Fold 2: F1=0.5085\n",
      "Fold 3: F1=0.5131\n",
      "Fold 4: F1=0.4461\n",
      "Fold 5: F1=0.4760\n",
      "\n",
      "Promedio CV:\n",
      "fold               3.000000\n",
      "f1_macro           0.481367\n",
      "precision_macro    0.492582\n",
      "recall_macro       0.491253\n",
      "dtype: float64\n",
      "✓ Exportado: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/dummy_cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "from utils_shared import get_cv_splitter, calculate_metrics\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Combinar Train + Dev para CV\n",
    "# reset_index() para mantener row_id como columna\n",
    "df_full = pd.concat([df_train, df_dev]).reset_index()\n",
    "X_full = df_full['texto']\n",
    "y_full = df_full['etiqueta']\n",
    "groups_full = df_full['patient_id'] # Usar patient_id directamente\n",
    "\n",
    "cv = get_cv_splitter(n_splits=5)\n",
    "cv_results = []\n",
    "\n",
    "print(\"Iniciando Cross-Validation (Dummy Stratified)...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_full, y_full, groups_full)):\n",
    "    X_tr, y_tr = X_full.iloc[train_idx], y_full.iloc[train_idx]\n",
    "    X_val, y_val = X_full.iloc[val_idx], y_full.iloc[val_idx]\n",
    "    \n",
    "    # Dummy Stratified\n",
    "    clf = DummyClassifier(strategy='stratified', random_state=42 + fold)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    \n",
    "    metrics = calculate_metrics(y_val, y_pred)\n",
    "    cv_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'model': 'Dummy Stratified',\n",
    "        'f1_macro': metrics['f1_macro'],\n",
    "        'precision_macro': metrics['precision_macro'],\n",
    "        'recall_macro': metrics['recall_macro']\n",
    "    })\n",
    "    print(f\"Fold {fold+1}: F1={metrics['f1_macro']:.4f}\")\n",
    "\n",
    "df_cv = pd.DataFrame(cv_results)\n",
    "print(\"\\nPromedio CV:\")\n",
    "print(df_cv.mean(numeric_only=True))\n",
    "\n",
    "out_path = DATA_PATH / 'dummy_cv_results.csv'\n",
    "df_cv.to_csv(out_path, index=False)\n",
    "print(f\"✓ Exportado: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
