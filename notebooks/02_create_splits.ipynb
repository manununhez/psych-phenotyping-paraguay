{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee5c248",
   "metadata": {},
   "source": [
    "# 02_create_splits ‚Äî Split √∫nico reproducible para todos los baselines\n",
    "\n",
    "**Objetivo:** crear **una sola vez** los splits train/val estratificados y guardarlos en `data/splits/` para que **todos los baselines** (rule-based, TF-IDF, transformer) usen **exactamente los mismos ejemplos** y sean **comparables**.\n",
    "\n",
    "**Outputs:**\n",
    "- `data/splits/train_indices.csv` (√≠ndices para train)\n",
    "- `data/splits/val_indices.csv` (√≠ndices para val)\n",
    "- `data/splits/dataset_base.csv` (dataset normalizado base con √≠ndices)\n",
    "\n",
    "**Reproducibilidad:** `random_state=42` fijo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0786bf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Usando utils_shared.py para configuraci√≥n\n",
      "üì• INPUT_FILE: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/ips_clean.csv\n",
      "üìÅ SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Paths y configuraci√≥n\n",
    "# ===============================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importar utilidades compartidas\n",
    "try:\n",
    "    from utils_shared import (\n",
    "        setup_paths, \n",
    "        guess_text_col, \n",
    "        guess_label_col, \n",
    "        normalize_label,\n",
    "        validate_file_exists\n",
    "    )\n",
    "    paths = setup_paths()\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    print(\"‚úÖ Usando utils_shared.py para configuraci√≥n\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è utils_shared.py no encontrado, usando configuraci√≥n manual\")\n",
    "    BASE_PATH = Path.cwd()\n",
    "    if BASE_PATH.name == \"notebooks\":\n",
    "        BASE_PATH = BASE_PATH.parent\n",
    "    DATA_PATH = BASE_PATH / \"data\"\n",
    "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
    "    SPLITS_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Definir funciones helper manualmente si no hay utils\n",
    "    def guess_text_col(df):\n",
    "        for c in ['texto', 'Motivo Consulta', 'text']:\n",
    "            if c in df.columns: return c\n",
    "        raise ValueError(\"No se encontr√≥ columna de texto\")\n",
    "    \n",
    "    def guess_label_col(df):\n",
    "        for c in ['etiqueta', 'Tipo', 'label']:\n",
    "            if c in df.columns: return c\n",
    "        return None\n",
    "    \n",
    "    def normalize_label(s):\n",
    "        import unicodedata\n",
    "        if pd.isna(s): return \"\"\n",
    "        s = str(s).strip().lower()\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "        return {'depresivo': 'depresion'}.get(s, s)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Detectar archivo de entrada (priorizar ips_clean.csv)\n",
    "# ---------------------------------------------------------------\n",
    "INPUT_FILE = DATA_PATH / 'ips_clean.csv'\n",
    "if not INPUT_FILE.exists():\n",
    "    INPUT_FILE = DATA_PATH / 'ips_raw.csv'\n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ ips_clean.csv, usando ips_raw.csv\")\n",
    "    print(\"   Recomendaci√≥n: ejecutar 01_eda_understanding.ipynb primero\")\n",
    "    if not INPUT_FILE.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå No se encontr√≥ ni ips_clean.csv ni ips_raw.csv en {DATA_PATH}\\n\"\n",
    "            f\"   Ejecuta 01_eda_understanding.ipynb para generar ips_clean.csv\"\n",
    "        )\n",
    "\n",
    "print(f\"üì• INPUT_FILE: {INPUT_FILE}\")\n",
    "print(f\"üìÅ SPLITS_PATH: {SPLITS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495930a",
   "metadata": {},
   "source": [
    "## 1) Carga y normalizaci√≥n base (sin preprocesamiento de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa153f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset cargado: 3127 filas\n",
      "üìä Columnas detectadas:\n",
      "   Texto: 'texto'\n",
      "   Etiqueta: 'etiqueta'\n",
      "   Despu√©s de remover nulos: 3126 filas\n",
      "   Despu√©s de filtrar A/D: 3126 filas\n",
      "\n",
      "‚úÖ Dataset final preparado: 3126 ejemplos √∫nicos\n",
      "\n",
      "üìä Distribuci√≥n de clases:\n",
      "etiqueta\n",
      "depresion    2201\n",
      "ansiedad      925\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Carga y normalizaci√≥n del dataset base\n",
    "# ===============================================================\n",
    "# Cargar dataset (ya deber√≠a estar limpio si viene de ips_clean.csv)\n",
    "df_raw = pd.read_csv(INPUT_FILE)\n",
    "print(f\"‚úÖ Dataset cargado: {len(df_raw)} filas\")\n",
    "\n",
    "# Detectar columnas autom√°ticamente usando utils\n",
    "text_col = guess_text_col(df_raw)\n",
    "label_col = guess_label_col(df_raw)\n",
    "\n",
    "if label_col is None:\n",
    "    raise ValueError(\n",
    "        \"‚ùå No se encontr√≥ columna de etiquetas en el dataset.\\n\"\n",
    "        f\"   Columnas disponibles: {list(df_raw.columns)}\"\n",
    "    )\n",
    "\n",
    "print(f\"üìä Columnas detectadas:\")\n",
    "print(f\"   Texto: '{text_col}'\")\n",
    "print(f\"   Etiqueta: '{label_col}'\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Filtrar y normalizar etiquetas\n",
    "# ---------------------------------------------------------------\n",
    "# Eliminar filas sin texto o etiqueta\n",
    "df = df_raw.dropna(subset=[text_col, label_col]).copy()\n",
    "print(f\"   Despu√©s de remover nulos: {len(df)} filas\")\n",
    "\n",
    "# Normalizar etiquetas (ansiedad/depresion)\n",
    "df[label_col] = df[label_col].map(normalize_label)\n",
    "\n",
    "# Filtrar solo ansiedad y depresi√≥n (clasificaci√≥n binaria)\n",
    "df = df[df[label_col].isin(['ansiedad','depresion'])].copy()\n",
    "print(f\"   Despu√©s de filtrar A/D: {len(df)} filas\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Eliminar duplicados (por si viene de ips_raw.csv)\n",
    "# ---------------------------------------------------------------\n",
    "n_before = len(df)\n",
    "df = df.drop_duplicates(subset=[text_col]).copy()\n",
    "n_after = len(df)\n",
    "\n",
    "if n_before > n_after:\n",
    "    print(f\"‚ö†Ô∏è Eliminados {n_before - n_after} duplicados\")\n",
    "    print(f\"   Recomendaci√≥n: Usar ips_clean.csv para evitar esto en el futuro\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Resetear √≠ndice y crear row_id √∫nico\n",
    "# ---------------------------------------------------------------\n",
    "df = df.reset_index(drop=True)\n",
    "df['row_id'] = df.index\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset final preparado: {len(df)} ejemplos √∫nicos\")\n",
    "print(f\"\\nüìä Distribuci√≥n de clases:\")\n",
    "print(df[label_col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2db3d",
   "metadata": {},
   "source": [
    "## 2) Split estratificado (80/20) con semilla fija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe464a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split creado exitosamente:\n",
      "   Train: 2500 ejemplos (80.0%)\n",
      "   Val:   626 ejemplos (20.0%)\n",
      "   Total: 3126 ejemplos\n",
      "\n",
      "üìä Distribuci√≥n en Train:\n",
      "   depresion: 1760 (70.4%)\n",
      "   ansiedad: 740 (29.6%)\n",
      "\n",
      "üìä Distribuci√≥n en Val:\n",
      "   depresion: 441 (70.4%)\n",
      "   ansiedad: 185 (29.6%)\n",
      "\n",
      "‚úÖ Estratificaci√≥n exitosa (diferencia m√°xima: 0.05%)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Estrategia de Split: 80/20 estratificado con semilla fija\n",
    "# ===============================================================\n",
    "# \n",
    "# ¬øPor qu√© estos par√°metros?\n",
    "# \n",
    "# 1) **RANDOM_STATE = 42**: \n",
    "#    - Fija la semilla aleatoria para reproducibilidad total\n",
    "#    - Todos los baselines ver√°n exactamente los mismos ejemplos\n",
    "#    - Permite comparaciones justas entre modelos\n",
    "#\n",
    "# 2) **TEST_SIZE = 0.2** (80/20):\n",
    "#    - Est√°ndar de la industria para datasets peque√±os/medianos\n",
    "#    - 80% train proporciona suficientes ejemplos para aprender patrones\n",
    "#    - 20% val proporciona validaci√≥n confiable sin desperdiciar datos\n",
    "#\n",
    "# 3) **stratify=df[label_col]**:\n",
    "#    - Mantiene la proporci√≥n de clases en train y val\n",
    "#    - Evita desbalances accidentales (ej: val con 90% ansiedad)\n",
    "#    - Cr√≠tico para evaluar correctamente modelos en datos desbalanceados\n",
    "#\n",
    "# ===============================================================\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    df.index,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[label_col]\n",
    ")\n",
    "\n",
    "# Reportar resultados del split\n",
    "print(f\"‚úÖ Split creado exitosamente:\")\n",
    "print(f\"   Train: {len(train_idx)} ejemplos ({len(train_idx)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Val:   {len(val_idx)} ejemplos ({len(val_idx)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Total: {len(df)} ejemplos\")\n",
    "\n",
    "print(f\"\\nüìä Distribuci√≥n en Train:\")\n",
    "train_dist = df.loc[train_idx, label_col].value_counts()\n",
    "for label, count in train_dist.items():\n",
    "    print(f\"   {label}: {count} ({count/len(train_idx)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Distribuci√≥n en Val:\")\n",
    "val_dist = df.loc[val_idx, label_col].value_counts()\n",
    "for label, count in val_dist.items():\n",
    "    print(f\"   {label}: {count} ({count/len(val_idx)*100:.1f}%)\")\n",
    "\n",
    "# Validaci√≥n: verificar que estratificaci√≥n funcion√≥ correctamente\n",
    "train_prop = train_dist / len(train_idx)\n",
    "val_prop = val_dist / len(val_idx)\n",
    "max_diff = (train_prop - val_prop).abs().max()\n",
    "\n",
    "if max_diff < 0.05:  # Diferencia < 5%\n",
    "    print(f\"\\n‚úÖ Estratificaci√≥n exitosa (diferencia m√°xima: {max_diff*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Estratificaci√≥n con diferencia de {max_diff*100:.2f}% (verificar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebec7ba",
   "metadata": {},
   "source": [
    "## 3) Guardar splits y dataset base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc033898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ Archivos de splits creados exitosamente\n",
      "============================================================\n",
      "\n",
      "üìÅ Ubicaci√≥n: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits/\n",
      "\n",
      "1Ô∏è‚É£  train_indices.csv\n",
      "    - 2500 √≠ndices de entrenamiento\n",
      "\n",
      "2Ô∏è‚É£  val_indices.csv\n",
      "    - 626 √≠ndices de validaci√≥n\n",
      "\n",
      "3Ô∏è‚É£  dataset_base.csv\n",
      "    - 3126 ejemplos con texto original\n",
      "    - Columnas: ['row_id', 'texto', 'etiqueta']\n",
      "    - Origen: ips_clean.csv\n",
      "\n",
      "============================================================\n",
      "üîí IMPORTANTE: Todos los baselines DEBEN usar estos archivos\n",
      "============================================================\n",
      "üìù Los baselines deben:\n",
      "   1. Cargar dataset_base.csv\n",
      "   2. Cargar train_indices.csv y val_indices.csv\n",
      "   3. Filtrar ejemplos usando row_id\n",
      "   4. Aplicar su propia estrategia de preprocesamiento\n",
      "\n",
      "üí° Ver notebooks 02_baseline_*.ipynb para ejemplos de uso\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Guardar splits unificados para todos los baselines\n",
    "# ===============================================================\n",
    "#\n",
    "# Estructura de archivos generada:\n",
    "#\n",
    "# 1) **train_indices.csv / val_indices.csv**:\n",
    "#    - Contienen √∫nicamente los row_id de cada split\n",
    "#    - Permite a cada baseline cargar exactamente los mismos ejemplos\n",
    "#    - Ligero (solo IDs), no duplica datos de texto\n",
    "#\n",
    "# 2) **dataset_base.csv**:\n",
    "#    - Contiene: row_id + texto original + etiqueta normalizada\n",
    "#    - Es el \"dataset maestro\" que usan TODOS los baselines\n",
    "#    - Cada baseline carga este archivo y filtra con los √≠ndices\n",
    "#\n",
    "# ¬øPor qu√© esta estrategia?\n",
    "# - **Reproducibilidad**: Un √∫nico punto de verdad para los datos\n",
    "# - **Comparabilidad**: Todos los modelos ven exactamente lo mismo\n",
    "# - **Flexibilidad**: Cada baseline puede preprocesar el texto a su manera\n",
    "#   (ej: rule-based no lo limpia, TF-IDF s√≠ lo limpia)\n",
    "# - **Mantenibilidad**: Cambios en splits se propagan autom√°ticamente\n",
    "#\n",
    "# ===============================================================\n",
    "\n",
    "# Guardar √≠ndices de train y val\n",
    "train_indices_path = SPLITS_PATH / 'train_indices.csv'\n",
    "val_indices_path = SPLITS_PATH / 'val_indices.csv'\n",
    "\n",
    "pd.DataFrame({'row_id': train_idx}).to_csv(train_indices_path, index=False)\n",
    "pd.DataFrame({'row_id': val_idx}).to_csv(val_indices_path, index=False)\n",
    "\n",
    "# Guardar dataset base (con texto original y etiqueta normalizada)\n",
    "# IMPORTANTE: No aplicamos limpieza aqu√≠, cada baseline decide su estrategia\n",
    "df_base = df[['row_id', text_col, label_col]].copy()\n",
    "dataset_base_path = SPLITS_PATH / 'dataset_base.csv'\n",
    "df_base.to_csv(dataset_base_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Reportar archivos creados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Archivos de splits creados exitosamente\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ Ubicaci√≥n: {SPLITS_PATH}/\")\n",
    "print(f\"\\n1Ô∏è‚É£  train_indices.csv\")\n",
    "print(f\"    - {len(train_idx)} √≠ndices de entrenamiento\")\n",
    "print(f\"\\n2Ô∏è‚É£  val_indices.csv\")\n",
    "print(f\"    - {len(val_idx)} √≠ndices de validaci√≥n\")\n",
    "print(f\"\\n3Ô∏è‚É£  dataset_base.csv\")\n",
    "print(f\"    - {len(df_base)} ejemplos con texto original\")\n",
    "print(f\"    - Columnas: {list(df_base.columns)}\")\n",
    "print(f\"    - Origen: {INPUT_FILE.name}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üîí IMPORTANTE: Todos los baselines DEBEN usar estos archivos\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìù Los baselines deben:\")\n",
    "print(f\"   1. Cargar dataset_base.csv\")\n",
    "print(f\"   2. Cargar train_indices.csv y val_indices.csv\")\n",
    "print(f\"   3. Filtrar ejemplos usando row_id\")\n",
    "print(f\"   4. Aplicar su propia estrategia de preprocesamiento\")\n",
    "print(f\"\\nüí° Ver notebooks 02_baseline_*.ipynb para ejemplos de uso\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
