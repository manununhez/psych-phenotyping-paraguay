{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee5c248",
   "metadata": {},
   "source": [
    "# 02_create_splits — Split único reproducible para todos los baselines\n",
    "\n",
    "**Objetivo:** crear **una sola vez** los splits train/val estratificados y guardarlos en `data/splits/` para que **todos los baselines** (rule-based, TF-IDF, transformer) usen **exactamente los mismos ejemplos** y sean **comparables**.\n",
    "\n",
    "**Outputs:**\n",
    "- `data/splits/train_indices.csv` (índices para train)\n",
    "- `data/splits/val_indices.csv` (índices para val)\n",
    "- `data/splits/dataset_base.csv` (dataset normalizado base con índices)\n",
    "\n",
    "**Reproducibilidad:** `random_state=42` fijo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0786bf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Usando utils_shared.py para configuración\n",
      "[INFO] INPUT_FILE: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/ips_clean.csv\n",
      "[INFO] SPLITS_PATH: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Setup: Paths y configuración\n",
    "# ===============================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importar utilidades compartidas\n",
    "try:\n",
    "    from utils_shared import (\n",
    "        setup_paths, \n",
    "        guess_text_col, \n",
    "        guess_label_col, \n",
    "        normalize_label,\n",
    "        validate_file_exists\n",
    "    )\n",
    "    paths = setup_paths()\n",
    "    DATA_PATH = paths['DATA_PATH']\n",
    "    SPLITS_PATH = paths['SPLITS_PATH']\n",
    "    print(\"[INFO] Usando utils_shared.py para configuración\")\n",
    "except ImportError:\n",
    "    print(\"[WARNING] utils_shared.py no encontrado, usando configuración manual\")\n",
    "    BASE_PATH = Path.cwd()\n",
    "    if BASE_PATH.name == \"notebooks\":\n",
    "        BASE_PATH = BASE_PATH.parent\n",
    "    DATA_PATH = BASE_PATH / \"data\"\n",
    "    SPLITS_PATH = DATA_PATH / \"splits\"\n",
    "    SPLITS_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Definir funciones helper manualmente si no hay utils\n",
    "    def guess_text_col(df):\n",
    "        for c in ['texto', 'Motivo Consulta', 'text']:\n",
    "            if c in df.columns: return c\n",
    "        raise ValueError(\"No se encontró columna de texto\")\n",
    "    \n",
    "    def guess_label_col(df):\n",
    "        for c in ['etiqueta', 'Tipo', 'label']:\n",
    "            if c in df.columns: return c\n",
    "        return None\n",
    "    \n",
    "    def normalize_label(s):\n",
    "        import unicodedata\n",
    "        if pd.isna(s): return \"\"\n",
    "        s = str(s).strip().lower()\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "        return {'depresivo': 'depresion'}.get(s, s)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Detectar archivo de entrada (priorizar ips_clean.csv)\n",
    "# ---------------------------------------------------------------\n",
    "INPUT_FILE = DATA_PATH / 'ips_clean.csv'\n",
    "if not INPUT_FILE.exists():\n",
    "    INPUT_FILE = DATA_PATH / 'ips_raw.csv'\n",
    "    print(\"[WARNING] No se encontró ips_clean.csv, usando ips_raw.csv\")\n",
    "    print(\"          Recomendación: ejecutar 01_eda_understanding.ipynb primero\")\n",
    "    if not INPUT_FILE.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[ERROR] No se encontró ni ips_clean.csv ni ips_raw.csv en {DATA_PATH}\\n\"\n",
    "            f\"        Ejecuta 01_eda_understanding.ipynb para generar ips_clean.csv\"\n",
    "        )\n",
    "\n",
    "print(f\"[INFO] INPUT_FILE: {INPUT_FILE}\")\n",
    "print(f\"[INFO] SPLITS_PATH: {SPLITS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495930a",
   "metadata": {},
   "source": [
    "## 1) Carga y normalización base (sin preprocesamiento de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa153f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset cargado: 3127 filas\n",
      "[INFO] Columnas detectadas:\n",
      "  Texto: 'texto'\n",
      "  Etiqueta: 'etiqueta'\n",
      "  Después de remover nulos: 3126 filas\n",
      "  Después de filtrar A/D: 3126 filas\n",
      "\n",
      "[INFO] Dataset final preparado: 3126 ejemplos únicos\n",
      "\n",
      "Distribución de clases:\n",
      "etiqueta\n",
      "depresion    2201\n",
      "ansiedad      925\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Carga y normalización del dataset base\n",
    "# ===============================================================\n",
    "# Cargar dataset (ya debería estar limpio si viene de ips_clean.csv)\n",
    "df_raw = pd.read_csv(INPUT_FILE)\n",
    "print(f\"[INFO] Dataset cargado: {len(df_raw)} filas\")\n",
    "\n",
    "# Detectar columnas automáticamente usando utils\n",
    "text_col = guess_text_col(df_raw)\n",
    "label_col = guess_label_col(df_raw)\n",
    "\n",
    "if label_col is None:\n",
    "    raise ValueError(\n",
    "        \"[ERROR] No se encontró columna de etiquetas en el dataset.\\n\"\n",
    "        f\"        Columnas disponibles: {list(df_raw.columns)}\"\n",
    "    )\n",
    "\n",
    "print(f\"[INFO] Columnas detectadas:\")\n",
    "print(f\"  Texto: '{text_col}'\")\n",
    "print(f\"  Etiqueta: '{label_col}'\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Filtrar y normalizar etiquetas\n",
    "# ---------------------------------------------------------------\n",
    "# Eliminar filas sin texto o etiqueta\n",
    "df = df_raw.dropna(subset=[text_col, label_col]).copy()\n",
    "print(f\"  Después de remover nulos: {len(df)} filas\")\n",
    "\n",
    "# Normalizar etiquetas (ansiedad/depresion)\n",
    "df[label_col] = df[label_col].map(normalize_label)\n",
    "\n",
    "# Filtrar solo ansiedad y depresión (clasificación binaria)\n",
    "df = df[df[label_col].isin(['ansiedad','depresion'])].copy()\n",
    "print(f\"  Después de filtrar A/D: {len(df)} filas\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Eliminar duplicados (por si viene de ips_raw.csv)\n",
    "# ---------------------------------------------------------------\n",
    "n_before = len(df)\n",
    "df = df.drop_duplicates(subset=[text_col]).copy()\n",
    "n_after = len(df)\n",
    "\n",
    "if n_before > n_after:\n",
    "    print(f\"[WARNING] Eliminados {n_before - n_after} duplicados\")\n",
    "    print(f\"          Recomendación: Usar ips_clean.csv para evitar esto en el futuro\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Resetear índice y crear row_id único\n",
    "# ---------------------------------------------------------------\n",
    "df = df.reset_index(drop=True)\n",
    "df['row_id'] = df.index\n",
    "\n",
    "print(f\"\\n[INFO] Dataset final preparado: {len(df)} ejemplos únicos\")\n",
    "print(f\"\\nDistribución de clases:\")\n",
    "print(df[label_col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2db3d",
   "metadata": {},
   "source": [
    "## 2) Split estratificado (80/20) con semilla fija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe464a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Split creado exitosamente:\n",
      "  Train: 2500 ejemplos (80.0%)\n",
      "  Val:   626 ejemplos (20.0%)\n",
      "  Total: 3126 ejemplos\n",
      "\n",
      "Distribución en Train:\n",
      "  depresion: 1760 (70.4%)\n",
      "  ansiedad: 740 (29.6%)\n",
      "\n",
      "Distribución en Val:\n",
      "  depresion: 441 (70.4%)\n",
      "  ansiedad: 185 (29.6%)\n",
      "\n",
      "[INFO] Estratificación exitosa (diferencia máxima: 0.05%)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Estrategia de Split: 80/20 estratificado con semilla fija\n",
    "# ===============================================================\n",
    "# \n",
    "# ¿Por qué estos parámetros?\n",
    "# \n",
    "# 1) **RANDOM_STATE = 42**: \n",
    "#    - Fija la semilla aleatoria para reproducibilidad total\n",
    "#    - Todos los baselines verán exactamente los mismos ejemplos\n",
    "#    - Permite comparaciones justas entre modelos\n",
    "#\n",
    "# 2) **TEST_SIZE = 0.2** (80/20):\n",
    "#    - Estándar de la industria para datasets pequeños/medianos\n",
    "#    - 80% train proporciona suficientes ejemplos para aprender patrones\n",
    "#    - 20% val proporciona validación confiable sin desperdiciar datos\n",
    "#\n",
    "# 3) **stratify=df[label_col]**:\n",
    "#    - Mantiene la proporción de clases en train y val\n",
    "#    - Evita desbalances accidentales (ej: val con 90% ansiedad)\n",
    "#    - Crítico para evaluar correctamente modelos en datos desbalanceados\n",
    "#\n",
    "# ===============================================================\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    df.index,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[label_col]\n",
    ")\n",
    "\n",
    "# Reportar resultados del split\n",
    "print(f\"[INFO] Split creado exitosamente:\")\n",
    "print(f\"  Train: {len(train_idx)} ejemplos ({len(train_idx)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_idx)} ejemplos ({len(val_idx)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Total: {len(df)} ejemplos\")\n",
    "\n",
    "print(f\"\\nDistribución en Train:\")\n",
    "train_dist = df.loc[train_idx, label_col].value_counts()\n",
    "for label, count in train_dist.items():\n",
    "    print(f\"  {label}: {count} ({count/len(train_idx)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribución en Val:\")\n",
    "val_dist = df.loc[val_idx, label_col].value_counts()\n",
    "for label, count in val_dist.items():\n",
    "    print(f\"  {label}: {count} ({count/len(val_idx)*100:.1f}%)\")\n",
    "\n",
    "# Validación: verificar que estratificación funcionó correctamente\n",
    "train_prop = train_dist / len(train_idx)\n",
    "val_prop = val_dist / len(val_idx)\n",
    "max_diff = (train_prop - val_prop).abs().max()\n",
    "\n",
    "if max_diff < 0.05:  # Diferencia < 5%\n",
    "    print(f\"\\n[INFO] Estratificación exitosa (diferencia máxima: {max_diff*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\n[WARNING] Estratificación con diferencia de {max_diff*100:.2f}% (verificar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebec7ba",
   "metadata": {},
   "source": [
    "## 3) Guardar splits y dataset base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc033898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[INFO] Archivos de splits creados exitosamente\n",
      "============================================================\n",
      "\n",
      "Ubicación: /Users/manuelnunez/Projects/psych-phenotyping-paraguay/data/splits/\n",
      "\n",
      "1. train_indices.csv\n",
      "   - 2500 índices de entrenamiento\n",
      "\n",
      "2. val_indices.csv\n",
      "   - 626 índices de validación\n",
      "\n",
      "3. dataset_base.csv\n",
      "   - 3126 ejemplos con texto original\n",
      "   - Columnas: ['row_id', 'texto', 'etiqueta']\n",
      "   - Origen: ips_clean.csv\n",
      "\n",
      "============================================================\n",
      "IMPORTANTE: Todos los baselines DEBEN usar estos archivos\n",
      "============================================================\n",
      "Los baselines deben:\n",
      "  1. Cargar dataset_base.csv\n",
      "  2. Cargar train_indices.csv y val_indices.csv\n",
      "  3. Filtrar ejemplos usando row_id\n",
      "  4. Aplicar su propia estrategia de preprocesamiento\n",
      "\n",
      "Ver notebooks 02_baseline_*.ipynb para ejemplos de uso\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Guardar splits unificados para todos los baselines\n",
    "# ===============================================================\n",
    "#\n",
    "# Estructura de archivos generada:\n",
    "#\n",
    "# 1) **train_indices.csv / val_indices.csv**:\n",
    "#    - Contienen únicamente los row_id de cada split\n",
    "#    - Permite a cada baseline cargar exactamente los mismos ejemplos\n",
    "#    - Ligero (solo IDs), no duplica datos de texto\n",
    "#\n",
    "# 2) **dataset_base.csv**:\n",
    "#    - Contiene: row_id + texto original + etiqueta normalizada\n",
    "#    - Es el \"dataset maestro\" que usan TODOS los baselines\n",
    "#    - Cada baseline carga este archivo y filtra con los índices\n",
    "#\n",
    "# ¿Por qué esta estrategia?\n",
    "# - **Reproducibilidad**: Un único punto de verdad para los datos\n",
    "# - **Comparabilidad**: Todos los modelos ven exactamente lo mismo\n",
    "# - **Flexibilidad**: Cada baseline puede preprocesar el texto a su manera\n",
    "#   (ej: rule-based no lo limpia, TF-IDF sí lo limpia)\n",
    "# - **Mantenibilidad**: Cambios en splits se propagan automáticamente\n",
    "#\n",
    "# ===============================================================\n",
    "\n",
    "# Guardar índices de train y val\n",
    "train_indices_path = SPLITS_PATH / 'train_indices.csv'\n",
    "val_indices_path = SPLITS_PATH / 'val_indices.csv'\n",
    "\n",
    "pd.DataFrame({'row_id': train_idx}).to_csv(train_indices_path, index=False)\n",
    "pd.DataFrame({'row_id': val_idx}).to_csv(val_indices_path, index=False)\n",
    "\n",
    "# Guardar dataset base (con texto original y etiqueta normalizada)\n",
    "# IMPORTANTE: No aplicamos limpieza aquí, cada baseline decide su estrategia\n",
    "df_base = df[['row_id', text_col, label_col]].copy()\n",
    "dataset_base_path = SPLITS_PATH / 'dataset_base.csv'\n",
    "df_base.to_csv(dataset_base_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Reportar archivos creados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[INFO] Archivos de splits creados exitosamente\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nUbicación: {SPLITS_PATH}/\")\n",
    "print(f\"\\n1. train_indices.csv\")\n",
    "print(f\"   - {len(train_idx)} índices de entrenamiento\")\n",
    "print(f\"\\n2. val_indices.csv\")\n",
    "print(f\"   - {len(val_idx)} índices de validación\")\n",
    "print(f\"\\n3. dataset_base.csv\")\n",
    "print(f\"   - {len(df_base)} ejemplos con texto original\")\n",
    "print(f\"   - Columnas: {list(df_base.columns)}\")\n",
    "print(f\"   - Origen: {INPUT_FILE.name}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTANTE: Todos los baselines DEBEN usar estos archivos\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Los baselines deben:\")\n",
    "print(f\"  1. Cargar dataset_base.csv\")\n",
    "print(f\"  2. Cargar train_indices.csv y val_indices.csv\")\n",
    "print(f\"  3. Filtrar ejemplos usando row_id\")\n",
    "print(f\"  4. Aplicar su propia estrategia de preprocesamiento\")\n",
    "print(f\"\\nVer notebooks 02_baseline_*.ipynb para ejemplos de uso\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
